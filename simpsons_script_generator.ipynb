{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpsons Project\n",
    "In this project, we will create neural network to train on [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  we'll be using [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons. The Neural Network will generate a new TV script.\n",
    "## Get the Data\n",
    "First of all we read the data and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8084: expected 13 fields, saw 20\\nSkipping line 52607: expected 13 fields, saw 21\\nSkipping line 59910: expected 13 fields, saw 21\\n'\n",
      "b'Skipping line 71801: expected 13 fields, saw 20\\nSkipping line 73539: expected 13 fields, saw 21\\nSkipping line 77230: expected 13 fields, saw 21\\nSkipping line 78953: expected 13 fields, saw 21\\nSkipping line 81138: expected 13 fields, saw 20\\nSkipping line 86746: expected 13 fields, saw 22\\nSkipping line 101154: expected 13 fields, saw 21\\nSkipping line 115438: expected 13 fields, saw 20\\nSkipping line 117573: expected 13 fields, saw 22\\nSkipping line 130610: expected 13 fields, saw 22\\n'\n",
      "b'Skipping line 152970: expected 13 fields, saw 22\\nSkipping line 153017: expected 13 fields, saw 20\\nSkipping line 153018: expected 13 fields, saw 30\\nSkipping line 154080: expected 13 fields, saw 20\\nSkipping line 154082: expected 13 fields, saw 20\\nSkipping line 154084: expected 13 fields, saw 20\\nSkipping line 154086: expected 13 fields, saw 20\\nSkipping line 154089: expected 13 fields, saw 23\\nSkipping line 154165: expected 13 fields, saw 21\\nSkipping line 156872: expected 13 fields, saw 20\\n'\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2821: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>timestamp_in_ms</th>\n",
       "      <th>speaking_line</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>8000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Street</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>8000</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Car</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>8000</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "      <td>ooo careful homer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>10000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "      <td>theres no time to be careful</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>10000</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>We're late.</td>\n",
       "      <td>were late</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number                                       raw_text  \\\n",
       "id                                                                      \n",
       "1            1       0   (Street: ext. street - establishing - night)   \n",
       "2            1       1                        (Car: int. car - night)   \n",
       "3            1       2            Marge Simpson: Ooo, careful, Homer.   \n",
       "4            1       3  Homer Simpson: There's no time to be careful.   \n",
       "5            1       4                     Homer Simpson: We're late.   \n",
       "\n",
       "    timestamp_in_ms  speaking_line  character_id  location_id  \\\n",
       "id                                                              \n",
       "1              8000           True             0            1   \n",
       "2              8000           True             0            2   \n",
       "3              8000           True             1            2   \n",
       "4             10000           True             2            2   \n",
       "5             10000           True             2            2   \n",
       "\n",
       "   raw_character_text raw_location_text                    spoken_words  \\\n",
       "id                                                                        \n",
       "1                                Street                                   \n",
       "2                                   Car                                   \n",
       "3       Marge Simpson               Car            Ooo, careful, Homer.   \n",
       "4       Homer Simpson               Car  There's no time to be careful.   \n",
       "5       Homer Simpson               Car                     We're late.   \n",
       "\n",
       "                 normalized_text  word_count  \n",
       "id                                            \n",
       "1                                          0  \n",
       "2                                          0  \n",
       "3              ooo careful homer           3  \n",
       "4   theres no time to be careful           6  \n",
       "5                      were late           2  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "#import helper\n",
    "import helper\n",
    "import pandas as pd\n",
    "data_dir = './data/simpsons/simpsons_script_lines.csv'\n",
    "\n",
    "\n",
    "df = helper.load_data(data_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>number</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>character_id</th>\n",
       "      <th>location_id</th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>raw_location_text</th>\n",
       "      <th>spoken_words</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>(Street: ext. street - establishing - night)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Street</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Car: int. car - night)</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Car</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson: Ooo, careful, Homer.</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>Ooo, careful, Homer.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Homer Simpson: There's no time to be careful.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>There's no time to be careful.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Homer Simpson: We're late.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>Car</td>\n",
       "      <td>We're late.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    episode_id  number                                       raw_text  \\\n",
       "id                                                                      \n",
       "1            1       0   (Street: ext. street - establishing - night)   \n",
       "2            1       1                        (Car: int. car - night)   \n",
       "3            1       2            Marge Simpson: Ooo, careful, Homer.   \n",
       "4            1       3  Homer Simpson: There's no time to be careful.   \n",
       "5            1       4                     Homer Simpson: We're late.   \n",
       "\n",
       "    character_id  location_id raw_character_text raw_location_text  \\\n",
       "id                                                                   \n",
       "1              0            1                               Street   \n",
       "2              0            2                                  Car   \n",
       "3              1            2      Marge Simpson               Car   \n",
       "4              2            2      Homer Simpson               Car   \n",
       "5              2            2      Homer Simpson               Car   \n",
       "\n",
       "                      spoken_words  word_count  \n",
       "id                                              \n",
       "1                                            0  \n",
       "2                                            0  \n",
       "3             Ooo, careful, Homer.           3  \n",
       "4   There's no time to be careful.           6  \n",
       "5                      We're late.           2  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping unneccssary columns\n",
    "df.drop('timestamp_in_ms', axis=1, inplace=True)\n",
    "df.drop('speaking_line', axis=1, inplace=True)\n",
    "df.drop('normalized_text', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = '  '\n",
    "for index in df.index:\n",
    "    if df.loc[index].word_count != 0:\n",
    "        text+= df.spoken_words.loc[index]\n",
    "        text+='\\n'\n",
    "    else:\n",
    "        if text[-2] != '\\n': text+='\\n'\n",
    "text = text[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data.\n",
    "\n",
    "As you can see below, the scripts are processed by removing everything except the body of the scripts. I made this compromise because using the network to generate useful sentences is the primary objective. Adding characters and scene locations will make it very difficult for the algorithm to train. I don't have the computational power to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 111028\n",
      "Number of scenes: 21242\n",
      "Average number of sentences in each scene: 5.217164108840976\n",
      "Number of lines: 132065\n",
      "Average number of words in each line: 9.92447658350055\n",
      "\n",
      "The sentences 0 to 9:\n",
      "Ooo, careful, Homer.\n",
      "There's no time to be careful.\n",
      "We're late.\n",
      "\n",
      "Sorry, Excuse us. Pardon me...\n",
      "Hey, Norman. How's it going? So you got dragged down here, too... heh, heh. How ya doing, Fred? Excuse me, Fred.\n",
      "Pardon my galoshes.\n",
      "Wasn't that wonderful? And now, \"Santas of Many Lands,\" as presented by the entire second grade class.\n",
      "Oh... Lisa's class.\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 9)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get,reverse= True)\n",
    "    vocab_to_int = {word:i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = dict(enumerate(vocab))\n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    tokens= {\n",
    "        '.' : '||period||',\n",
    "        ',' : '||comma||',\n",
    "        '\"' : '||quotation_mark||',\n",
    "        ';' : '||semicolon||',\n",
    "        '!' : '||exclamation_mark||',\n",
    "        '?' : '||question_mark||',\n",
    "        '(' : '||left_parantheses||',\n",
    "        ')' : '||right_parantheses||',\n",
    "        '--' : '||dash||',\n",
    "        '\\n' : '||return||'\n",
    "    }\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cells below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Data\n",
    "helper.preprocess_and_save_data(text, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.11.0\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.3'), 'Please use TensorFlow version 1.3 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    Input = tf.placeholder(tf.int32 ,[None, None] ,name = 'input')\n",
    "    Targets = tf.placeholder(tf.int32 ,[None,None] ,name = 'targets')\n",
    "    LearningRate = tf.placeholder(tf.float32 ,name ='learning_rate')\n",
    "    return (Input, Targets, LearningRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size, keep_prob=.8, layers=3):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell', num_units=rnn_size)\n",
    "    \n",
    "    stack_cell = []\n",
    "    for i in range(layers):\n",
    "        stack_cell.append(tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=random.uniform(.75,.85)))\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(stack_cell)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32), name='initial_state')\n",
    "    return (cell, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence. Part of the embedding matrix will be initilized with Google's pre-trained Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47947, 300)\n",
      "18232\n",
      "ratio of unitilised words = 0.38025319623751225\n"
     ]
    }
   ],
   "source": [
    "embed_matrix = np.zeros((len(int_to_vocab),300),dtype='float32')\n",
    "counts = 0\n",
    "for word, int_word in vocab_to_int.items(): \n",
    "    try:\n",
    "        embed_matrix[int_word, :] = model.get_vector(word)\n",
    "    except KeyError:\n",
    "        counts+=1\n",
    "        embed_matrix[int_word, :] = np.zeros(300)\n",
    "\n",
    "print(embed_matrix.shape)\n",
    "print(counts)\n",
    "print('ratio of unitilised words = {}'.format(counts/embed_matrix.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embed_tensor = tf.Variable(embed_matrix, name='embed_tensor')\n",
    "    embed = tf.nn.embedding_lookup(embed_tensor,input_data)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    outputs = tf.identity(outputs, name= 'outputs')\n",
    "    return (outputs, final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "- Apply embedding to `input_data` using `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    \n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        outputs, vocab_size, activation_fn=None,)\n",
    "    \n",
    "    logits = tf.identity(logits, name= 'logits')\n",
    "    \n",
    "    return logits, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For example, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(number of batches, 2, batch size, sequence length)\n",
    "\n",
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ints_per_batch = batch_size * seq_length\n",
    "    n_batches = len(int_text)//ints_per_batch\n",
    "    int_text = np.array(int_text[: n_batches*ints_per_batch])\n",
    "    #int_text = int_text.reshape((batch_size,-1))\n",
    "    \n",
    "    idx=0\n",
    "    array = np.array( np.zeros(shape=(n_batches,2,batch_size,seq_length)) )\n",
    "    for batch_size_idx in range(batch_size):\n",
    "        for n_batches_idx in range(n_batches):\n",
    "            for seq_length_idx in range(seq_length):\n",
    "                array[n_batches_idx,0,batch_size_idx,seq_length_idx] = int_text[idx]\n",
    "                try:\n",
    "                    array[n_batches_idx,1,batch_size_idx,seq_length_idx] = int_text[idx+1]\n",
    "                except IndexError:\n",
    "                    array[n_batches_idx,1,batch_size_idx,seq_length_idx] = int_text[0]\n",
    "                idx+=1\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Number of Epochs\n",
    "num_epochs = 1000\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# Cell\n",
    "layers = 5\n",
    "keep_prob = .8\n",
    "\n",
    "# RNN Size\n",
    "rnn_size = 300\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 20\n",
    "# Learning Rate\n",
    "learning_rate = .001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 100\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    \n",
    "    input_text, targets, lr = get_inputs()\n",
    "    \n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    \n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size, .8, 3)\n",
    "    \n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/718   train_loss = 10.778\n",
      "Epoch   0 Batch  100/718   train_loss = 6.120\n",
      "Epoch   0 Batch  200/718   train_loss = 6.225\n",
      "Epoch   0 Batch  300/718   train_loss = 6.293\n",
      "Epoch   0 Batch  400/718   train_loss = 6.053\n",
      "Epoch   0 Batch  500/718   train_loss = 6.233\n",
      "Epoch   0 Batch  600/718   train_loss = 6.165\n",
      "Epoch   0 Batch  700/718   train_loss = 6.143\n",
      "Epoch   1 Batch   82/718   train_loss = 6.119\n",
      "Epoch   1 Batch  182/718   train_loss = 5.996\n",
      "Epoch   1 Batch  282/718   train_loss = 5.849\n",
      "Epoch   1 Batch  382/718   train_loss = 5.739\n",
      "Epoch   1 Batch  482/718   train_loss = 5.504\n",
      "Epoch   1 Batch  582/718   train_loss = 5.331\n",
      "Epoch   1 Batch  682/718   train_loss = 5.402\n",
      "Epoch   2 Batch   64/718   train_loss = 5.129\n",
      "Epoch   2 Batch  164/718   train_loss = 5.060\n",
      "Epoch   2 Batch  264/718   train_loss = 5.086\n",
      "Epoch   2 Batch  364/718   train_loss = 4.916\n",
      "Epoch   2 Batch  464/718   train_loss = 5.120\n",
      "Epoch   2 Batch  564/718   train_loss = 4.938\n",
      "Epoch   2 Batch  664/718   train_loss = 4.929\n",
      "Epoch   3 Batch   46/718   train_loss = 4.762\n",
      "Epoch   3 Batch  146/718   train_loss = 5.025\n",
      "Epoch   3 Batch  246/718   train_loss = 4.885\n",
      "Epoch   3 Batch  346/718   train_loss = 4.819\n",
      "Epoch   3 Batch  446/718   train_loss = 4.735\n",
      "Epoch   3 Batch  546/718   train_loss = 4.792\n",
      "Epoch   3 Batch  646/718   train_loss = 4.688\n",
      "Epoch   4 Batch   28/718   train_loss = 4.625\n",
      "Epoch   4 Batch  128/718   train_loss = 4.739\n",
      "Epoch   4 Batch  228/718   train_loss = 4.642\n",
      "Epoch   4 Batch  328/718   train_loss = 4.698\n",
      "Epoch   4 Batch  428/718   train_loss = 4.553\n",
      "Epoch   4 Batch  528/718   train_loss = 4.734\n",
      "Epoch   4 Batch  628/718   train_loss = 4.524\n",
      "Epoch   5 Batch   10/718   train_loss = 4.592\n",
      "Epoch   5 Batch  110/718   train_loss = 4.501\n",
      "Epoch   5 Batch  210/718   train_loss = 4.472\n",
      "Epoch   5 Batch  310/718   train_loss = 4.634\n",
      "Epoch   5 Batch  410/718   train_loss = 4.567\n",
      "Epoch   5 Batch  510/718   train_loss = 4.500\n",
      "Epoch   5 Batch  610/718   train_loss = 4.414\n",
      "Epoch   5 Batch  710/718   train_loss = 4.473\n",
      "Epoch   6 Batch   92/718   train_loss = 4.383\n",
      "Epoch   6 Batch  192/718   train_loss = 4.516\n",
      "Epoch   6 Batch  292/718   train_loss = 4.419\n",
      "Epoch   6 Batch  392/718   train_loss = 4.426\n",
      "Epoch   6 Batch  492/718   train_loss = 4.601\n",
      "Epoch   6 Batch  592/718   train_loss = 4.377\n",
      "Epoch   6 Batch  692/718   train_loss = 4.526\n",
      "Epoch   7 Batch   74/718   train_loss = 4.420\n",
      "Epoch   7 Batch  174/718   train_loss = 4.271\n",
      "Epoch   7 Batch  274/718   train_loss = 4.350\n",
      "Epoch   7 Batch  374/718   train_loss = 4.412\n",
      "Epoch   7 Batch  474/718   train_loss = 4.371\n",
      "Epoch   7 Batch  574/718   train_loss = 4.330\n",
      "Epoch   7 Batch  674/718   train_loss = 4.404\n",
      "Epoch   8 Batch   56/718   train_loss = 4.363\n",
      "Epoch   8 Batch  156/718   train_loss = 4.266\n",
      "Epoch   8 Batch  256/718   train_loss = 4.180\n",
      "Epoch   8 Batch  356/718   train_loss = 4.233\n",
      "Epoch   8 Batch  456/718   train_loss = 4.275\n",
      "Epoch   8 Batch  556/718   train_loss = 4.281\n",
      "Epoch   8 Batch  656/718   train_loss = 4.344\n",
      "Epoch   9 Batch   38/718   train_loss = 4.217\n",
      "Epoch   9 Batch  138/718   train_loss = 4.274\n",
      "Epoch   9 Batch  238/718   train_loss = 4.313\n",
      "Epoch   9 Batch  338/718   train_loss = 4.220\n",
      "Epoch   9 Batch  438/718   train_loss = 4.098\n",
      "Epoch   9 Batch  538/718   train_loss = 4.280\n",
      "Epoch   9 Batch  638/718   train_loss = 4.205\n",
      "Epoch  10 Batch   20/718   train_loss = 4.257\n",
      "Epoch  10 Batch  120/718   train_loss = 4.219\n",
      "Epoch  10 Batch  220/718   train_loss = 4.232\n",
      "Epoch  10 Batch  320/718   train_loss = 4.105\n",
      "Epoch  10 Batch  420/718   train_loss = 4.131\n",
      "Epoch  10 Batch  520/718   train_loss = 4.127\n",
      "Epoch  10 Batch  620/718   train_loss = 4.262\n",
      "Epoch  11 Batch    2/718   train_loss = 4.279\n",
      "Epoch  11 Batch  102/718   train_loss = 4.132\n",
      "Epoch  11 Batch  202/718   train_loss = 4.035\n",
      "Epoch  11 Batch  302/718   train_loss = 4.054\n",
      "Epoch  11 Batch  402/718   train_loss = 4.133\n",
      "Epoch  11 Batch  502/718   train_loss = 4.130\n",
      "Epoch  11 Batch  602/718   train_loss = 4.099\n",
      "Epoch  11 Batch  702/718   train_loss = 4.084\n",
      "Epoch  12 Batch   84/718   train_loss = 4.033\n",
      "Epoch  12 Batch  184/718   train_loss = 4.022\n",
      "Epoch  12 Batch  284/718   train_loss = 4.080\n",
      "Epoch  12 Batch  384/718   train_loss = 4.059\n",
      "Epoch  12 Batch  484/718   train_loss = 3.944\n",
      "Epoch  12 Batch  584/718   train_loss = 4.000\n",
      "Epoch  12 Batch  684/718   train_loss = 3.993\n",
      "Epoch  13 Batch   66/718   train_loss = 3.962\n",
      "Epoch  13 Batch  166/718   train_loss = 4.039\n",
      "Epoch  13 Batch  266/718   train_loss = 3.963\n",
      "Epoch  13 Batch  366/718   train_loss = 3.948\n",
      "Epoch  13 Batch  466/718   train_loss = 3.888\n",
      "Epoch  13 Batch  566/718   train_loss = 3.943\n",
      "Epoch  13 Batch  666/718   train_loss = 3.981\n",
      "Epoch  14 Batch   48/718   train_loss = 3.949\n",
      "Epoch  14 Batch  148/718   train_loss = 3.855\n",
      "Epoch  14 Batch  248/718   train_loss = 4.080\n",
      "Epoch  14 Batch  348/718   train_loss = 3.961\n",
      "Epoch  14 Batch  448/718   train_loss = 3.886\n",
      "Epoch  14 Batch  548/718   train_loss = 4.009\n",
      "Epoch  14 Batch  648/718   train_loss = 3.871\n",
      "Epoch  15 Batch   30/718   train_loss = 3.908\n",
      "Epoch  15 Batch  130/718   train_loss = 3.941\n",
      "Epoch  15 Batch  230/718   train_loss = 3.767\n",
      "Epoch  15 Batch  330/718   train_loss = 3.885\n",
      "Epoch  15 Batch  430/718   train_loss = 3.990\n",
      "Epoch  15 Batch  530/718   train_loss = 3.924\n",
      "Epoch  15 Batch  630/718   train_loss = 3.833\n",
      "Epoch  16 Batch   12/718   train_loss = 3.881\n",
      "Epoch  16 Batch  112/718   train_loss = 3.872\n",
      "Epoch  16 Batch  212/718   train_loss = 3.968\n",
      "Epoch  16 Batch  312/718   train_loss = 3.870\n",
      "Epoch  16 Batch  412/718   train_loss = 3.771\n",
      "Epoch  16 Batch  512/718   train_loss = 3.862\n",
      "Epoch  16 Batch  612/718   train_loss = 3.859\n",
      "Epoch  16 Batch  712/718   train_loss = 3.863\n",
      "Epoch  17 Batch   94/718   train_loss = 3.701\n",
      "Epoch  17 Batch  194/718   train_loss = 3.779\n",
      "Epoch  17 Batch  294/718   train_loss = 3.926\n",
      "Epoch  17 Batch  394/718   train_loss = 3.742\n",
      "Epoch  17 Batch  494/718   train_loss = 3.867\n",
      "Epoch  17 Batch  594/718   train_loss = 3.799\n",
      "Epoch  17 Batch  694/718   train_loss = 3.772\n",
      "Epoch  18 Batch   76/718   train_loss = 3.792\n",
      "Epoch  18 Batch  176/718   train_loss = 3.763\n",
      "Epoch  18 Batch  276/718   train_loss = 3.779\n",
      "Epoch  18 Batch  376/718   train_loss = 3.724\n",
      "Epoch  18 Batch  476/718   train_loss = 3.721\n",
      "Epoch  18 Batch  576/718   train_loss = 3.824\n",
      "Epoch  18 Batch  676/718   train_loss = 3.858\n",
      "Epoch  19 Batch   58/718   train_loss = 3.755\n",
      "Epoch  19 Batch  158/718   train_loss = 3.780\n",
      "Epoch  19 Batch  258/718   train_loss = 3.688\n",
      "Epoch  19 Batch  358/718   train_loss = 3.604\n",
      "Epoch  19 Batch  458/718   train_loss = 3.736\n",
      "Epoch  19 Batch  558/718   train_loss = 3.870\n",
      "Epoch  19 Batch  658/718   train_loss = 3.719\n",
      "Epoch  20 Batch   40/718   train_loss = 3.720\n",
      "Epoch  20 Batch  140/718   train_loss = 3.751\n",
      "Epoch  20 Batch  240/718   train_loss = 3.711\n",
      "Epoch  20 Batch  340/718   train_loss = 3.862\n",
      "Epoch  20 Batch  440/718   train_loss = 3.825\n",
      "Epoch  20 Batch  540/718   train_loss = 3.836\n",
      "Epoch  20 Batch  640/718   train_loss = 3.672\n",
      "Epoch  21 Batch   22/718   train_loss = 3.663\n",
      "Epoch  21 Batch  122/718   train_loss = 3.680\n",
      "Epoch  21 Batch  222/718   train_loss = 3.774\n",
      "Epoch  21 Batch  322/718   train_loss = 3.685\n",
      "Epoch  21 Batch  422/718   train_loss = 3.772\n",
      "Epoch  21 Batch  522/718   train_loss = 3.706\n",
      "Epoch  21 Batch  622/718   train_loss = 3.681\n",
      "Epoch  22 Batch    4/718   train_loss = 3.667\n",
      "Epoch  22 Batch  104/718   train_loss = 3.671\n",
      "Epoch  22 Batch  204/718   train_loss = 3.658\n",
      "Epoch  22 Batch  304/718   train_loss = 3.717\n",
      "Epoch  22 Batch  404/718   train_loss = 3.612\n",
      "Epoch  22 Batch  504/718   train_loss = 3.773\n",
      "Epoch  22 Batch  604/718   train_loss = 3.619\n",
      "Epoch  22 Batch  704/718   train_loss = 3.678\n",
      "Epoch  23 Batch   86/718   train_loss = 3.634\n",
      "Epoch  23 Batch  186/718   train_loss = 3.723\n",
      "Epoch  23 Batch  286/718   train_loss = 3.721\n",
      "Epoch  23 Batch  386/718   train_loss = 3.670\n",
      "Epoch  23 Batch  486/718   train_loss = 3.675\n",
      "Epoch  23 Batch  586/718   train_loss = 3.702\n",
      "Epoch  23 Batch  686/718   train_loss = 3.588\n",
      "Epoch  24 Batch   68/718   train_loss = 3.596\n",
      "Epoch  24 Batch  168/718   train_loss = 3.722\n",
      "Epoch  24 Batch  268/718   train_loss = 3.643\n",
      "Epoch  24 Batch  368/718   train_loss = 3.535\n",
      "Epoch  24 Batch  468/718   train_loss = 3.625\n",
      "Epoch  24 Batch  568/718   train_loss = 3.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  24 Batch  668/718   train_loss = 3.646\n",
      "Epoch  25 Batch   50/718   train_loss = 3.590\n",
      "Epoch  25 Batch  150/718   train_loss = 3.658\n",
      "Epoch  25 Batch  250/718   train_loss = 3.621\n",
      "Epoch  25 Batch  350/718   train_loss = 3.612\n",
      "Epoch  25 Batch  450/718   train_loss = 3.625\n",
      "Epoch  25 Batch  550/718   train_loss = 3.575\n",
      "Epoch  25 Batch  650/718   train_loss = 3.552\n",
      "Epoch  26 Batch   32/718   train_loss = 3.574\n",
      "Epoch  26 Batch  132/718   train_loss = 3.566\n",
      "Epoch  26 Batch  232/718   train_loss = 3.556\n",
      "Epoch  26 Batch  332/718   train_loss = 3.555\n",
      "Epoch  26 Batch  432/718   train_loss = 3.698\n",
      "Epoch  26 Batch  532/718   train_loss = 3.686\n",
      "Epoch  26 Batch  632/718   train_loss = 3.510\n",
      "Epoch  27 Batch   14/718   train_loss = 3.729\n",
      "Epoch  27 Batch  114/718   train_loss = 3.541\n",
      "Epoch  27 Batch  214/718   train_loss = 3.635\n",
      "Epoch  27 Batch  314/718   train_loss = 3.539\n",
      "Epoch  27 Batch  414/718   train_loss = 3.521\n",
      "Epoch  27 Batch  514/718   train_loss = 3.664\n",
      "Epoch  27 Batch  614/718   train_loss = 3.550\n",
      "Epoch  27 Batch  714/718   train_loss = 3.598\n",
      "Epoch  28 Batch   96/718   train_loss = 3.533\n",
      "Epoch  28 Batch  196/718   train_loss = 3.497\n",
      "Epoch  28 Batch  296/718   train_loss = 3.584\n",
      "Epoch  28 Batch  396/718   train_loss = 3.512\n",
      "Epoch  28 Batch  496/718   train_loss = 3.574\n",
      "Epoch  28 Batch  596/718   train_loss = 3.537\n",
      "Epoch  28 Batch  696/718   train_loss = 3.547\n",
      "Epoch  29 Batch   78/718   train_loss = 3.602\n",
      "Epoch  29 Batch  178/718   train_loss = 3.593\n",
      "Epoch  29 Batch  278/718   train_loss = 3.523\n",
      "Epoch  29 Batch  378/718   train_loss = 3.483\n",
      "Epoch  29 Batch  478/718   train_loss = 3.537\n",
      "Epoch  29 Batch  578/718   train_loss = 3.421\n",
      "Epoch  29 Batch  678/718   train_loss = 3.529\n",
      "Epoch  30 Batch   60/718   train_loss = 3.509\n",
      "Epoch  30 Batch  160/718   train_loss = 3.556\n",
      "Epoch  30 Batch  260/718   train_loss = 3.639\n",
      "Epoch  30 Batch  360/718   train_loss = 3.501\n",
      "Epoch  30 Batch  460/718   train_loss = 3.493\n",
      "Epoch  30 Batch  560/718   train_loss = 3.506\n",
      "Epoch  30 Batch  660/718   train_loss = 3.588\n",
      "Epoch  31 Batch   42/718   train_loss = 3.518\n",
      "Epoch  31 Batch  142/718   train_loss = 3.602\n",
      "Epoch  31 Batch  242/718   train_loss = 3.472\n",
      "Epoch  31 Batch  342/718   train_loss = 3.503\n",
      "Epoch  31 Batch  442/718   train_loss = 3.581\n",
      "Epoch  31 Batch  542/718   train_loss = 3.498\n",
      "Epoch  31 Batch  642/718   train_loss = 3.528\n",
      "Epoch  32 Batch   24/718   train_loss = 3.463\n",
      "Epoch  32 Batch  124/718   train_loss = 3.454\n",
      "Epoch  32 Batch  224/718   train_loss = 3.467\n",
      "Epoch  32 Batch  324/718   train_loss = 3.517\n",
      "Epoch  32 Batch  424/718   train_loss = 3.439\n",
      "Epoch  32 Batch  524/718   train_loss = 3.493\n",
      "Epoch  32 Batch  624/718   train_loss = 3.484\n",
      "Epoch  33 Batch    6/718   train_loss = 3.465\n",
      "Epoch  33 Batch  106/718   train_loss = 3.418\n",
      "Epoch  33 Batch  206/718   train_loss = 3.428\n",
      "Epoch  33 Batch  306/718   train_loss = 3.446\n",
      "Epoch  33 Batch  406/718   train_loss = 3.436\n",
      "Epoch  33 Batch  506/718   train_loss = 3.475\n",
      "Epoch  33 Batch  606/718   train_loss = 3.428\n",
      "Epoch  33 Batch  706/718   train_loss = 3.459\n",
      "Epoch  34 Batch   88/718   train_loss = 3.477\n",
      "Epoch  34 Batch  188/718   train_loss = 3.444\n",
      "Epoch  34 Batch  288/718   train_loss = 3.450\n",
      "Epoch  34 Batch  388/718   train_loss = 3.359\n",
      "Epoch  34 Batch  488/718   train_loss = 3.423\n",
      "Epoch  34 Batch  588/718   train_loss = 3.483\n",
      "Epoch  34 Batch  688/718   train_loss = 3.421\n",
      "Epoch  35 Batch   70/718   train_loss = 3.439\n",
      "Epoch  35 Batch  170/718   train_loss = 3.455\n",
      "Epoch  35 Batch  270/718   train_loss = 3.514\n",
      "Epoch  35 Batch  370/718   train_loss = 3.423\n",
      "Epoch  35 Batch  470/718   train_loss = 3.422\n",
      "Epoch  35 Batch  570/718   train_loss = 3.508\n",
      "Epoch  35 Batch  670/718   train_loss = 3.404\n",
      "Epoch  36 Batch   52/718   train_loss = 3.361\n",
      "Epoch  36 Batch  152/718   train_loss = 3.407\n",
      "Epoch  36 Batch  252/718   train_loss = 3.421\n",
      "Epoch  36 Batch  352/718   train_loss = 3.428\n",
      "Epoch  36 Batch  452/718   train_loss = 3.454\n",
      "Epoch  36 Batch  552/718   train_loss = 3.473\n",
      "Epoch  36 Batch  652/718   train_loss = 3.420\n",
      "Epoch  37 Batch   34/718   train_loss = 3.421\n",
      "Epoch  37 Batch  134/718   train_loss = 3.375\n",
      "Epoch  37 Batch  234/718   train_loss = 3.485\n",
      "Epoch  37 Batch  334/718   train_loss = 3.462\n",
      "Epoch  37 Batch  434/718   train_loss = 3.484\n",
      "Epoch  37 Batch  534/718   train_loss = 3.458\n",
      "Epoch  37 Batch  634/718   train_loss = 3.471\n",
      "Epoch  38 Batch   16/718   train_loss = 3.383\n",
      "Epoch  38 Batch  116/718   train_loss = 3.343\n",
      "Epoch  38 Batch  216/718   train_loss = 3.355\n",
      "Epoch  38 Batch  316/718   train_loss = 3.377\n",
      "Epoch  38 Batch  416/718   train_loss = 3.458\n",
      "Epoch  38 Batch  516/718   train_loss = 3.411\n",
      "Epoch  38 Batch  616/718   train_loss = 3.357\n",
      "Epoch  38 Batch  716/718   train_loss = 3.420\n",
      "Epoch  39 Batch   98/718   train_loss = 3.304\n",
      "Epoch  39 Batch  198/718   train_loss = 3.366\n",
      "Epoch  39 Batch  298/718   train_loss = 3.383\n",
      "Epoch  39 Batch  398/718   train_loss = 3.390\n",
      "Epoch  39 Batch  498/718   train_loss = 3.389\n",
      "Epoch  39 Batch  598/718   train_loss = 3.455\n",
      "Epoch  39 Batch  698/718   train_loss = 3.409\n",
      "Epoch  40 Batch   80/718   train_loss = 3.422\n",
      "Epoch  40 Batch  180/718   train_loss = 3.348\n",
      "Epoch  40 Batch  280/718   train_loss = 3.437\n",
      "Epoch  40 Batch  380/718   train_loss = 3.399\n",
      "Epoch  40 Batch  480/718   train_loss = 3.439\n",
      "Epoch  40 Batch  580/718   train_loss = 3.348\n",
      "Epoch  40 Batch  680/718   train_loss = 3.473\n",
      "Epoch  41 Batch   62/718   train_loss = 3.369\n",
      "Epoch  41 Batch  162/718   train_loss = 3.364\n",
      "Epoch  41 Batch  262/718   train_loss = 3.410\n",
      "Epoch  41 Batch  362/718   train_loss = 3.382\n",
      "Epoch  41 Batch  462/718   train_loss = 3.330\n",
      "Epoch  41 Batch  562/718   train_loss = 3.379\n",
      "Epoch  41 Batch  662/718   train_loss = 3.364\n",
      "Epoch  42 Batch   44/718   train_loss = 3.374\n",
      "Epoch  42 Batch  144/718   train_loss = 3.402\n",
      "Epoch  42 Batch  244/718   train_loss = 3.414\n",
      "Epoch  42 Batch  344/718   train_loss = 3.370\n",
      "Epoch  42 Batch  444/718   train_loss = 3.346\n",
      "Epoch  42 Batch  544/718   train_loss = 3.377\n",
      "Epoch  42 Batch  644/718   train_loss = 3.498\n",
      "Epoch  43 Batch   26/718   train_loss = 3.405\n",
      "Epoch  43 Batch  126/718   train_loss = 3.373\n",
      "Epoch  43 Batch  226/718   train_loss = 3.371\n",
      "Epoch  43 Batch  326/718   train_loss = 3.334\n",
      "Epoch  43 Batch  426/718   train_loss = 3.309\n",
      "Epoch  43 Batch  526/718   train_loss = 3.319\n",
      "Epoch  43 Batch  626/718   train_loss = 3.403\n",
      "Epoch  44 Batch    8/718   train_loss = 3.362\n",
      "Epoch  44 Batch  108/718   train_loss = 3.276\n",
      "Epoch  44 Batch  208/718   train_loss = 3.331\n",
      "Epoch  44 Batch  308/718   train_loss = 3.419\n",
      "Epoch  44 Batch  408/718   train_loss = 3.428\n",
      "Epoch  44 Batch  508/718   train_loss = 3.439\n",
      "Epoch  44 Batch  608/718   train_loss = 3.413\n",
      "Epoch  44 Batch  708/718   train_loss = 3.311\n",
      "Epoch  45 Batch   90/718   train_loss = 3.306\n",
      "Epoch  45 Batch  190/718   train_loss = 3.429\n",
      "Epoch  45 Batch  290/718   train_loss = 3.319\n",
      "Epoch  45 Batch  390/718   train_loss = 3.300\n",
      "Epoch  45 Batch  490/718   train_loss = 3.348\n",
      "Epoch  45 Batch  590/718   train_loss = 3.294\n",
      "Epoch  45 Batch  690/718   train_loss = 3.361\n",
      "Epoch  46 Batch   72/718   train_loss = 3.380\n",
      "Epoch  46 Batch  172/718   train_loss = 3.403\n",
      "Epoch  46 Batch  272/718   train_loss = 3.328\n",
      "Epoch  46 Batch  372/718   train_loss = 3.309\n",
      "Epoch  46 Batch  472/718   train_loss = 3.375\n",
      "Epoch  46 Batch  572/718   train_loss = 3.438\n",
      "Epoch  46 Batch  672/718   train_loss = 3.264\n",
      "Epoch  47 Batch   54/718   train_loss = 3.282\n",
      "Epoch  47 Batch  154/718   train_loss = 3.319\n",
      "Epoch  47 Batch  254/718   train_loss = 3.299\n",
      "Epoch  47 Batch  354/718   train_loss = 3.410\n",
      "Epoch  47 Batch  454/718   train_loss = 3.239\n",
      "Epoch  47 Batch  554/718   train_loss = 3.339\n",
      "Epoch  47 Batch  654/718   train_loss = 3.377\n",
      "Epoch  48 Batch   36/718   train_loss = 3.263\n",
      "Epoch  48 Batch  136/718   train_loss = 3.249\n",
      "Epoch  48 Batch  236/718   train_loss = 3.225\n",
      "Epoch  48 Batch  336/718   train_loss = 3.298\n",
      "Epoch  48 Batch  436/718   train_loss = 3.331\n",
      "Epoch  48 Batch  536/718   train_loss = 3.363\n",
      "Epoch  48 Batch  636/718   train_loss = 3.303\n",
      "Epoch  49 Batch   18/718   train_loss = 3.336\n",
      "Epoch  49 Batch  118/718   train_loss = 3.263\n",
      "Epoch  49 Batch  218/718   train_loss = 3.298\n",
      "Epoch  49 Batch  318/718   train_loss = 3.305\n",
      "Epoch  49 Batch  418/718   train_loss = 3.390\n",
      "Epoch  49 Batch  518/718   train_loss = 3.389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  49 Batch  618/718   train_loss = 3.227\n",
      "Epoch  50 Batch    0/718   train_loss = 3.363\n",
      "Epoch  50 Batch  100/718   train_loss = 3.257\n",
      "Epoch  50 Batch  200/718   train_loss = 3.292\n",
      "Epoch  50 Batch  300/718   train_loss = 3.340\n",
      "Epoch  50 Batch  400/718   train_loss = 3.263\n",
      "Epoch  50 Batch  500/718   train_loss = 3.354\n",
      "Epoch  50 Batch  600/718   train_loss = 3.281\n",
      "Epoch  50 Batch  700/718   train_loss = 3.201\n",
      "Epoch  51 Batch   82/718   train_loss = 3.300\n",
      "Epoch  51 Batch  182/718   train_loss = 3.344\n",
      "Epoch  51 Batch  282/718   train_loss = 3.319\n",
      "Epoch  51 Batch  382/718   train_loss = 3.255\n",
      "Epoch  51 Batch  482/718   train_loss = 3.373\n",
      "Epoch  51 Batch  582/718   train_loss = 3.243\n",
      "Epoch  51 Batch  682/718   train_loss = 3.302\n",
      "Epoch  52 Batch   64/718   train_loss = 3.222\n",
      "Epoch  52 Batch  164/718   train_loss = 3.157\n",
      "Epoch  52 Batch  264/718   train_loss = 3.284\n",
      "Epoch  52 Batch  364/718   train_loss = 3.315\n",
      "Epoch  52 Batch  464/718   train_loss = 3.235\n",
      "Epoch  52 Batch  564/718   train_loss = 3.305\n",
      "Epoch  52 Batch  664/718   train_loss = 3.317\n",
      "Epoch  53 Batch   46/718   train_loss = 3.188\n",
      "Epoch  53 Batch  146/718   train_loss = 3.246\n",
      "Epoch  53 Batch  246/718   train_loss = 3.309\n",
      "Epoch  53 Batch  346/718   train_loss = 3.234\n",
      "Epoch  53 Batch  446/718   train_loss = 3.256\n",
      "Epoch  53 Batch  546/718   train_loss = 3.162\n",
      "Epoch  53 Batch  646/718   train_loss = 3.331\n",
      "Epoch  54 Batch   28/718   train_loss = 3.170\n",
      "Epoch  54 Batch  128/718   train_loss = 3.246\n",
      "Epoch  54 Batch  228/718   train_loss = 3.308\n",
      "Epoch  54 Batch  328/718   train_loss = 3.224\n",
      "Epoch  54 Batch  428/718   train_loss = 3.189\n",
      "Epoch  54 Batch  528/718   train_loss = 3.345\n",
      "Epoch  54 Batch  628/718   train_loss = 3.337\n",
      "Epoch  55 Batch   10/718   train_loss = 3.267\n",
      "Epoch  55 Batch  110/718   train_loss = 3.231\n",
      "Epoch  55 Batch  210/718   train_loss = 3.216\n",
      "Epoch  55 Batch  310/718   train_loss = 3.268\n",
      "Epoch  55 Batch  410/718   train_loss = 3.197\n",
      "Epoch  55 Batch  510/718   train_loss = 3.314\n",
      "Epoch  55 Batch  610/718   train_loss = 3.201\n",
      "Epoch  55 Batch  710/718   train_loss = 3.250\n",
      "Epoch  56 Batch   92/718   train_loss = 3.226\n",
      "Epoch  56 Batch  192/718   train_loss = 3.208\n",
      "Epoch  56 Batch  292/718   train_loss = 3.124\n",
      "Epoch  56 Batch  392/718   train_loss = 3.222\n",
      "Epoch  56 Batch  492/718   train_loss = 3.276\n",
      "Epoch  56 Batch  592/718   train_loss = 3.298\n",
      "Epoch  56 Batch  692/718   train_loss = 3.275\n",
      "Epoch  57 Batch   74/718   train_loss = 3.260\n",
      "Epoch  57 Batch  174/718   train_loss = 3.280\n",
      "Epoch  57 Batch  274/718   train_loss = 3.240\n",
      "Epoch  57 Batch  374/718   train_loss = 3.186\n",
      "Epoch  57 Batch  474/718   train_loss = 3.270\n",
      "Epoch  57 Batch  574/718   train_loss = 3.174\n",
      "Epoch  57 Batch  674/718   train_loss = 3.223\n",
      "Epoch  58 Batch   56/718   train_loss = 3.325\n",
      "Epoch  58 Batch  156/718   train_loss = 3.170\n",
      "Epoch  58 Batch  256/718   train_loss = 3.110\n",
      "Epoch  58 Batch  356/718   train_loss = 3.196\n",
      "Epoch  58 Batch  456/718   train_loss = 3.179\n",
      "Epoch  58 Batch  556/718   train_loss = 3.135\n",
      "Epoch  58 Batch  656/718   train_loss = 3.194\n",
      "Epoch  59 Batch   38/718   train_loss = 3.227\n",
      "Epoch  59 Batch  138/718   train_loss = 3.249\n",
      "Epoch  59 Batch  238/718   train_loss = 3.290\n",
      "Epoch  59 Batch  338/718   train_loss = 3.238\n",
      "Epoch  59 Batch  438/718   train_loss = 3.092\n",
      "Epoch  59 Batch  538/718   train_loss = 3.283\n",
      "Epoch  59 Batch  638/718   train_loss = 3.174\n",
      "Epoch  60 Batch   20/718   train_loss = 3.226\n",
      "Epoch  60 Batch  120/718   train_loss = 3.229\n",
      "Epoch  60 Batch  220/718   train_loss = 3.194\n",
      "Epoch  60 Batch  320/718   train_loss = 3.198\n",
      "Epoch  60 Batch  420/718   train_loss = 3.284\n",
      "Epoch  60 Batch  520/718   train_loss = 3.210\n",
      "Epoch  60 Batch  620/718   train_loss = 3.348\n",
      "Epoch  61 Batch    2/718   train_loss = 3.271\n",
      "Epoch  61 Batch  102/718   train_loss = 3.164\n",
      "Epoch  61 Batch  202/718   train_loss = 3.162\n",
      "Epoch  61 Batch  302/718   train_loss = 3.236\n",
      "Epoch  61 Batch  402/718   train_loss = 3.225\n",
      "Epoch  61 Batch  502/718   train_loss = 3.206\n",
      "Epoch  61 Batch  602/718   train_loss = 3.264\n",
      "Epoch  61 Batch  702/718   train_loss = 3.164\n",
      "Epoch  62 Batch   84/718   train_loss = 3.150\n",
      "Epoch  62 Batch  184/718   train_loss = 3.178\n",
      "Epoch  62 Batch  284/718   train_loss = 3.201\n",
      "Epoch  62 Batch  384/718   train_loss = 3.175\n",
      "Epoch  62 Batch  484/718   train_loss = 3.186\n",
      "Epoch  62 Batch  584/718   train_loss = 3.211\n",
      "Epoch  62 Batch  684/718   train_loss = 3.191\n",
      "Epoch  63 Batch   66/718   train_loss = 3.150\n",
      "Epoch  63 Batch  166/718   train_loss = 3.185\n",
      "Epoch  63 Batch  266/718   train_loss = 3.225\n",
      "Epoch  63 Batch  366/718   train_loss = 3.256\n",
      "Epoch  63 Batch  466/718   train_loss = 3.111\n",
      "Epoch  63 Batch  566/718   train_loss = 3.160\n",
      "Epoch  63 Batch  666/718   train_loss = 3.165\n",
      "Epoch  64 Batch   48/718   train_loss = 3.095\n",
      "Epoch  64 Batch  148/718   train_loss = 3.170\n",
      "Epoch  64 Batch  248/718   train_loss = 3.240\n",
      "Epoch  64 Batch  348/718   train_loss = 3.179\n",
      "Epoch  64 Batch  448/718   train_loss = 3.199\n",
      "Epoch  64 Batch  548/718   train_loss = 3.247\n",
      "Epoch  64 Batch  648/718   train_loss = 3.177\n",
      "Epoch  65 Batch   30/718   train_loss = 3.159\n",
      "Epoch  65 Batch  130/718   train_loss = 3.162\n",
      "Epoch  65 Batch  230/718   train_loss = 3.161\n",
      "Epoch  65 Batch  330/718   train_loss = 3.186\n",
      "Epoch  65 Batch  430/718   train_loss = 3.307\n",
      "Epoch  65 Batch  530/718   train_loss = 3.127\n",
      "Epoch  65 Batch  630/718   train_loss = 3.118\n",
      "Epoch  66 Batch   12/718   train_loss = 3.102\n",
      "Epoch  66 Batch  112/718   train_loss = 3.192\n",
      "Epoch  66 Batch  212/718   train_loss = 3.264\n",
      "Epoch  66 Batch  312/718   train_loss = 3.206\n",
      "Epoch  66 Batch  412/718   train_loss = 3.056\n",
      "Epoch  66 Batch  512/718   train_loss = 3.173\n",
      "Epoch  66 Batch  612/718   train_loss = 3.221\n",
      "Epoch  66 Batch  712/718   train_loss = 3.226\n",
      "Epoch  67 Batch   94/718   train_loss = 3.009\n",
      "Epoch  67 Batch  194/718   train_loss = 3.066\n",
      "Epoch  67 Batch  294/718   train_loss = 3.256\n",
      "Epoch  67 Batch  394/718   train_loss = 3.119\n",
      "Epoch  67 Batch  494/718   train_loss = 3.279\n",
      "Epoch  67 Batch  594/718   train_loss = 3.204\n",
      "Epoch  67 Batch  694/718   train_loss = 3.169\n",
      "Epoch  68 Batch   76/718   train_loss = 3.083\n",
      "Epoch  68 Batch  176/718   train_loss = 3.222\n",
      "Epoch  68 Batch  276/718   train_loss = 3.180\n",
      "Epoch  68 Batch  376/718   train_loss = 3.135\n",
      "Epoch  68 Batch  476/718   train_loss = 3.159\n",
      "Epoch  68 Batch  576/718   train_loss = 3.191\n",
      "Epoch  68 Batch  676/718   train_loss = 3.158\n",
      "Epoch  69 Batch   58/718   train_loss = 3.140\n",
      "Epoch  69 Batch  158/718   train_loss = 3.174\n",
      "Epoch  69 Batch  258/718   train_loss = 3.104\n",
      "Epoch  69 Batch  358/718   train_loss = 3.031\n",
      "Epoch  69 Batch  458/718   train_loss = 3.139\n",
      "Epoch  69 Batch  558/718   train_loss = 3.242\n",
      "Epoch  69 Batch  658/718   train_loss = 3.067\n",
      "Epoch  70 Batch   40/718   train_loss = 3.145\n",
      "Epoch  70 Batch  140/718   train_loss = 3.135\n",
      "Epoch  70 Batch  240/718   train_loss = 3.120\n",
      "Epoch  70 Batch  340/718   train_loss = 3.246\n",
      "Epoch  70 Batch  440/718   train_loss = 3.167\n",
      "Epoch  70 Batch  540/718   train_loss = 3.208\n",
      "Epoch  70 Batch  640/718   train_loss = 3.062\n",
      "Epoch  71 Batch   22/718   train_loss = 3.034\n",
      "Epoch  71 Batch  122/718   train_loss = 3.111\n",
      "Epoch  71 Batch  222/718   train_loss = 3.164\n",
      "Epoch  71 Batch  322/718   train_loss = 3.177\n",
      "Epoch  71 Batch  422/718   train_loss = 3.200\n",
      "Epoch  71 Batch  522/718   train_loss = 3.112\n",
      "Epoch  71 Batch  622/718   train_loss = 3.127\n",
      "Epoch  72 Batch    4/718   train_loss = 3.107\n",
      "Epoch  72 Batch  104/718   train_loss = 3.117\n",
      "Epoch  72 Batch  204/718   train_loss = 3.078\n",
      "Epoch  72 Batch  304/718   train_loss = 3.155\n",
      "Epoch  72 Batch  404/718   train_loss = 3.083\n",
      "Epoch  72 Batch  504/718   train_loss = 3.249\n",
      "Epoch  72 Batch  604/718   train_loss = 3.073\n",
      "Epoch  72 Batch  704/718   train_loss = 3.123\n",
      "Epoch  73 Batch   86/718   train_loss = 3.114\n",
      "Epoch  73 Batch  186/718   train_loss = 3.128\n",
      "Epoch  73 Batch  286/718   train_loss = 3.225\n",
      "Epoch  73 Batch  386/718   train_loss = 3.118\n",
      "Epoch  73 Batch  486/718   train_loss = 3.088\n",
      "Epoch  73 Batch  586/718   train_loss = 3.150\n",
      "Epoch  73 Batch  686/718   train_loss = 3.079\n",
      "Epoch  74 Batch   68/718   train_loss = 3.155\n",
      "Epoch  74 Batch  168/718   train_loss = 3.195\n",
      "Epoch  74 Batch  268/718   train_loss = 3.110\n",
      "Epoch  74 Batch  368/718   train_loss = 3.022\n",
      "Epoch  74 Batch  468/718   train_loss = 3.181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  74 Batch  568/718   train_loss = 3.230\n",
      "Epoch  74 Batch  668/718   train_loss = 3.116\n",
      "Epoch  75 Batch   50/718   train_loss = 3.145\n",
      "Epoch  75 Batch  150/718   train_loss = 3.142\n",
      "Epoch  75 Batch  250/718   train_loss = 3.100\n",
      "Epoch  75 Batch  350/718   train_loss = 3.094\n",
      "Epoch  75 Batch  450/718   train_loss = 3.083\n",
      "Epoch  75 Batch  550/718   train_loss = 3.051\n",
      "Epoch  75 Batch  650/718   train_loss = 3.040\n",
      "Epoch  76 Batch   32/718   train_loss = 3.044\n",
      "Epoch  76 Batch  132/718   train_loss = 3.084\n",
      "Epoch  76 Batch  232/718   train_loss = 3.110\n",
      "Epoch  76 Batch  332/718   train_loss = 3.050\n",
      "Epoch  76 Batch  432/718   train_loss = 3.236\n",
      "Epoch  76 Batch  532/718   train_loss = 3.199\n",
      "Epoch  76 Batch  632/718   train_loss = 3.012\n",
      "Epoch  77 Batch   14/718   train_loss = 3.222\n",
      "Epoch  77 Batch  114/718   train_loss = 3.065\n",
      "Epoch  77 Batch  214/718   train_loss = 3.088\n",
      "Epoch  77 Batch  314/718   train_loss = 3.090\n",
      "Epoch  77 Batch  414/718   train_loss = 3.023\n",
      "Epoch  77 Batch  514/718   train_loss = 3.157\n",
      "Epoch  77 Batch  614/718   train_loss = 3.076\n",
      "Epoch  77 Batch  714/718   train_loss = 3.094\n",
      "Epoch  78 Batch   96/718   train_loss = 3.075\n",
      "Epoch  78 Batch  196/718   train_loss = 3.013\n",
      "Epoch  78 Batch  296/718   train_loss = 3.082\n",
      "Epoch  78 Batch  396/718   train_loss = 3.106\n",
      "Epoch  78 Batch  496/718   train_loss = 3.141\n",
      "Epoch  78 Batch  596/718   train_loss = 3.137\n",
      "Epoch  78 Batch  696/718   train_loss = 3.056\n",
      "Epoch  79 Batch   78/718   train_loss = 3.093\n",
      "Epoch  79 Batch  178/718   train_loss = 3.134\n",
      "Epoch  79 Batch  278/718   train_loss = 3.030\n",
      "Epoch  79 Batch  378/718   train_loss = 3.041\n",
      "Epoch  79 Batch  478/718   train_loss = 3.064\n",
      "Epoch  79 Batch  578/718   train_loss = 2.989\n",
      "Epoch  79 Batch  678/718   train_loss = 3.104\n",
      "Epoch  80 Batch   60/718   train_loss = 3.075\n",
      "Epoch  80 Batch  160/718   train_loss = 3.115\n",
      "Epoch  80 Batch  260/718   train_loss = 3.167\n",
      "Epoch  80 Batch  360/718   train_loss = 3.053\n",
      "Epoch  80 Batch  460/718   train_loss = 2.985\n",
      "Epoch  80 Batch  560/718   train_loss = 3.096\n",
      "Epoch  80 Batch  660/718   train_loss = 3.202\n",
      "Epoch  81 Batch   42/718   train_loss = 3.089\n",
      "Epoch  81 Batch  142/718   train_loss = 3.164\n",
      "Epoch  81 Batch  242/718   train_loss = 2.978\n",
      "Epoch  81 Batch  342/718   train_loss = 3.074\n",
      "Epoch  81 Batch  442/718   train_loss = 3.098\n",
      "Epoch  81 Batch  542/718   train_loss = 3.029\n",
      "Epoch  81 Batch  642/718   train_loss = 3.122\n",
      "Epoch  82 Batch   24/718   train_loss = 3.051\n",
      "Epoch  82 Batch  124/718   train_loss = 3.009\n",
      "Epoch  82 Batch  224/718   train_loss = 3.041\n",
      "Epoch  82 Batch  324/718   train_loss = 3.066\n",
      "Epoch  82 Batch  424/718   train_loss = 3.080\n",
      "Epoch  82 Batch  524/718   train_loss = 3.112\n",
      "Epoch  82 Batch  624/718   train_loss = 3.084\n",
      "Epoch  83 Batch    6/718   train_loss = 3.012\n",
      "Epoch  83 Batch  106/718   train_loss = 3.011\n",
      "Epoch  83 Batch  206/718   train_loss = 2.963\n",
      "Epoch  83 Batch  306/718   train_loss = 3.056\n",
      "Epoch  83 Batch  406/718   train_loss = 3.049\n",
      "Epoch  83 Batch  506/718   train_loss = 3.056\n",
      "Epoch  83 Batch  606/718   train_loss = 3.005\n",
      "Epoch  83 Batch  706/718   train_loss = 3.022\n",
      "Epoch  84 Batch   88/718   train_loss = 3.117\n",
      "Epoch  84 Batch  188/718   train_loss = 3.039\n",
      "Epoch  84 Batch  288/718   train_loss = 3.044\n",
      "Epoch  84 Batch  388/718   train_loss = 3.005\n",
      "Epoch  84 Batch  488/718   train_loss = 2.985\n",
      "Epoch  84 Batch  588/718   train_loss = 3.143\n",
      "Epoch  84 Batch  688/718   train_loss = 3.015\n",
      "Epoch  85 Batch   70/718   train_loss = 3.065\n",
      "Epoch  85 Batch  170/718   train_loss = 3.121\n",
      "Epoch  85 Batch  270/718   train_loss = 3.125\n",
      "Epoch  85 Batch  370/718   train_loss = 3.024\n",
      "Epoch  85 Batch  470/718   train_loss = 3.111\n",
      "Epoch  85 Batch  570/718   train_loss = 3.069\n",
      "Epoch  85 Batch  670/718   train_loss = 3.036\n",
      "Epoch  86 Batch   52/718   train_loss = 2.991\n",
      "Epoch  86 Batch  152/718   train_loss = 3.021\n",
      "Epoch  86 Batch  252/718   train_loss = 3.047\n",
      "Epoch  86 Batch  352/718   train_loss = 3.028\n",
      "Epoch  86 Batch  452/718   train_loss = 3.113\n",
      "Epoch  86 Batch  552/718   train_loss = 3.054\n",
      "Epoch  86 Batch  652/718   train_loss = 3.023\n",
      "Epoch  87 Batch   34/718   train_loss = 3.012\n",
      "Epoch  87 Batch  134/718   train_loss = 2.998\n",
      "Epoch  87 Batch  234/718   train_loss = 3.120\n",
      "Epoch  87 Batch  334/718   train_loss = 3.100\n",
      "Epoch  87 Batch  434/718   train_loss = 3.116\n",
      "Epoch  87 Batch  534/718   train_loss = 3.033\n",
      "Epoch  87 Batch  634/718   train_loss = 3.132\n",
      "Epoch  88 Batch   16/718   train_loss = 3.006\n",
      "Epoch  88 Batch  116/718   train_loss = 2.942\n",
      "Epoch  88 Batch  216/718   train_loss = 2.998\n",
      "Epoch  88 Batch  316/718   train_loss = 2.988\n",
      "Epoch  88 Batch  416/718   train_loss = 3.067\n",
      "Epoch  88 Batch  516/718   train_loss = 3.027\n",
      "Epoch  88 Batch  616/718   train_loss = 3.042\n",
      "Epoch  88 Batch  716/718   train_loss = 3.064\n",
      "Epoch  89 Batch   98/718   train_loss = 2.932\n",
      "Epoch  89 Batch  198/718   train_loss = 3.030\n",
      "Epoch  89 Batch  298/718   train_loss = 3.013\n",
      "Epoch  89 Batch  398/718   train_loss = 3.016\n",
      "Epoch  89 Batch  498/718   train_loss = 3.063\n",
      "Epoch  89 Batch  598/718   train_loss = 3.101\n",
      "Epoch  89 Batch  698/718   train_loss = 3.098\n",
      "Epoch  90 Batch   80/718   train_loss = 3.070\n",
      "Epoch  90 Batch  180/718   train_loss = 2.981\n",
      "Epoch  90 Batch  280/718   train_loss = 3.121\n",
      "Epoch  90 Batch  380/718   train_loss = 3.082\n",
      "Epoch  90 Batch  480/718   train_loss = 3.081\n",
      "Epoch  90 Batch  580/718   train_loss = 2.956\n",
      "Epoch  90 Batch  680/718   train_loss = 3.106\n",
      "Epoch  91 Batch   62/718   train_loss = 3.023\n",
      "Epoch  91 Batch  162/718   train_loss = 3.026\n",
      "Epoch  91 Batch  262/718   train_loss = 3.075\n",
      "Epoch  91 Batch  362/718   train_loss = 3.064\n",
      "Epoch  91 Batch  462/718   train_loss = 2.985\n",
      "Epoch  91 Batch  562/718   train_loss = 3.035\n",
      "Epoch  91 Batch  662/718   train_loss = 3.054\n",
      "Epoch  92 Batch   44/718   train_loss = 3.005\n",
      "Epoch  92 Batch  144/718   train_loss = 3.089\n",
      "Epoch  92 Batch  244/718   train_loss = 3.120\n",
      "Epoch  92 Batch  344/718   train_loss = 3.034\n",
      "Epoch  92 Batch  444/718   train_loss = 2.981\n",
      "Epoch  92 Batch  544/718   train_loss = 3.031\n",
      "Epoch  92 Batch  644/718   train_loss = 3.154\n",
      "Epoch  93 Batch   26/718   train_loss = 3.075\n",
      "Epoch  93 Batch  126/718   train_loss = 3.077\n",
      "Epoch  93 Batch  226/718   train_loss = 2.994\n",
      "Epoch  93 Batch  326/718   train_loss = 3.015\n",
      "Epoch  93 Batch  426/718   train_loss = 3.009\n",
      "Epoch  93 Batch  526/718   train_loss = 2.978\n",
      "Epoch  93 Batch  626/718   train_loss = 3.106\n",
      "Epoch  94 Batch    8/718   train_loss = 2.986\n",
      "Epoch  94 Batch  108/718   train_loss = 2.941\n",
      "Epoch  94 Batch  208/718   train_loss = 3.018\n",
      "Epoch  94 Batch  308/718   train_loss = 3.092\n",
      "Epoch  94 Batch  408/718   train_loss = 3.104\n",
      "Epoch  94 Batch  508/718   train_loss = 3.090\n",
      "Epoch  94 Batch  608/718   train_loss = 3.115\n",
      "Epoch  94 Batch  708/718   train_loss = 3.009\n",
      "Epoch  95 Batch   90/718   train_loss = 2.949\n",
      "Epoch  95 Batch  190/718   train_loss = 3.131\n",
      "Epoch  95 Batch  290/718   train_loss = 2.999\n",
      "Epoch  95 Batch  390/718   train_loss = 2.959\n",
      "Epoch  95 Batch  490/718   train_loss = 3.014\n",
      "Epoch  95 Batch  590/718   train_loss = 3.008\n",
      "Epoch  95 Batch  690/718   train_loss = 3.065\n",
      "Epoch  96 Batch   72/718   train_loss = 3.090\n",
      "Epoch  96 Batch  172/718   train_loss = 3.081\n",
      "Epoch  96 Batch  272/718   train_loss = 3.026\n",
      "Epoch  96 Batch  372/718   train_loss = 2.985\n",
      "Epoch  96 Batch  472/718   train_loss = 3.048\n",
      "Epoch  96 Batch  572/718   train_loss = 3.077\n",
      "Epoch  96 Batch  672/718   train_loss = 2.968\n",
      "Epoch  97 Batch   54/718   train_loss = 2.912\n",
      "Epoch  97 Batch  154/718   train_loss = 3.042\n",
      "Epoch  97 Batch  254/718   train_loss = 2.982\n",
      "Epoch  97 Batch  354/718   train_loss = 3.048\n",
      "Epoch  97 Batch  454/718   train_loss = 2.959\n",
      "Epoch  97 Batch  554/718   train_loss = 3.055\n",
      "Epoch  97 Batch  654/718   train_loss = 3.070\n",
      "Epoch  98 Batch   36/718   train_loss = 2.980\n",
      "Epoch  98 Batch  136/718   train_loss = 2.945\n",
      "Epoch  98 Batch  236/718   train_loss = 2.938\n",
      "Epoch  98 Batch  336/718   train_loss = 2.969\n",
      "Epoch  98 Batch  436/718   train_loss = 3.033\n",
      "Epoch  98 Batch  536/718   train_loss = 3.040\n",
      "Epoch  98 Batch  636/718   train_loss = 3.045\n",
      "Epoch  99 Batch   18/718   train_loss = 3.001\n",
      "Epoch  99 Batch  118/718   train_loss = 2.931\n",
      "Epoch  99 Batch  218/718   train_loss = 3.007\n",
      "Epoch  99 Batch  318/718   train_loss = 2.995\n",
      "Epoch  99 Batch  418/718   train_loss = 3.135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  99 Batch  518/718   train_loss = 3.060\n",
      "Epoch  99 Batch  618/718   train_loss = 2.931\n",
      "Epoch 100 Batch    0/718   train_loss = 3.111\n",
      "Epoch 100 Batch  100/718   train_loss = 2.984\n",
      "Epoch 100 Batch  200/718   train_loss = 2.976\n",
      "Epoch 100 Batch  300/718   train_loss = 3.059\n",
      "Epoch 100 Batch  400/718   train_loss = 2.998\n",
      "Epoch 100 Batch  500/718   train_loss = 3.102\n",
      "Epoch 100 Batch  600/718   train_loss = 3.028\n",
      "Epoch 100 Batch  700/718   train_loss = 2.914\n",
      "Epoch 101 Batch   82/718   train_loss = 2.965\n",
      "Epoch 101 Batch  182/718   train_loss = 3.078\n",
      "Epoch 101 Batch  282/718   train_loss = 3.084\n",
      "Epoch 101 Batch  382/718   train_loss = 2.985\n",
      "Epoch 101 Batch  482/718   train_loss = 3.118\n",
      "Epoch 101 Batch  582/718   train_loss = 2.937\n",
      "Epoch 101 Batch  682/718   train_loss = 3.009\n",
      "Epoch 102 Batch   64/718   train_loss = 2.982\n",
      "Epoch 102 Batch  164/718   train_loss = 2.873\n",
      "Epoch 102 Batch  264/718   train_loss = 2.993\n",
      "Epoch 102 Batch  364/718   train_loss = 2.995\n",
      "Epoch 102 Batch  464/718   train_loss = 2.949\n",
      "Epoch 102 Batch  564/718   train_loss = 2.988\n",
      "Epoch 102 Batch  664/718   train_loss = 3.058\n",
      "Epoch 103 Batch   46/718   train_loss = 2.874\n",
      "Epoch 103 Batch  146/718   train_loss = 2.946\n",
      "Epoch 103 Batch  246/718   train_loss = 3.055\n",
      "Epoch 103 Batch  346/718   train_loss = 2.928\n",
      "Epoch 103 Batch  446/718   train_loss = 2.965\n",
      "Epoch 103 Batch  546/718   train_loss = 2.875\n",
      "Epoch 103 Batch  646/718   train_loss = 3.085\n",
      "Epoch 104 Batch   28/718   train_loss = 2.923\n",
      "Epoch 104 Batch  128/718   train_loss = 2.962\n",
      "Epoch 104 Batch  228/718   train_loss = 3.015\n",
      "Epoch 104 Batch  328/718   train_loss = 2.950\n",
      "Epoch 104 Batch  428/718   train_loss = 2.906\n",
      "Epoch 104 Batch  528/718   train_loss = 3.032\n",
      "Epoch 104 Batch  628/718   train_loss = 3.082\n",
      "Epoch 105 Batch   10/718   train_loss = 2.954\n",
      "Epoch 105 Batch  110/718   train_loss = 2.969\n",
      "Epoch 105 Batch  210/718   train_loss = 2.916\n",
      "Epoch 105 Batch  310/718   train_loss = 3.030\n",
      "Epoch 105 Batch  410/718   train_loss = 2.883\n",
      "Epoch 105 Batch  510/718   train_loss = 3.023\n",
      "Epoch 105 Batch  610/718   train_loss = 2.981\n",
      "Epoch 105 Batch  710/718   train_loss = 2.940\n",
      "Epoch 106 Batch   92/718   train_loss = 2.915\n",
      "Epoch 106 Batch  192/718   train_loss = 2.929\n",
      "Epoch 106 Batch  292/718   train_loss = 2.864\n",
      "Epoch 106 Batch  392/718   train_loss = 2.967\n",
      "Epoch 106 Batch  492/718   train_loss = 3.043\n",
      "Epoch 106 Batch  592/718   train_loss = 2.988\n",
      "Epoch 106 Batch  692/718   train_loss = 3.008\n",
      "Epoch 107 Batch   74/718   train_loss = 3.017\n",
      "Epoch 107 Batch  174/718   train_loss = 3.057\n",
      "Epoch 107 Batch  274/718   train_loss = 3.006\n",
      "Epoch 107 Batch  374/718   train_loss = 2.931\n",
      "Epoch 107 Batch  474/718   train_loss = 3.004\n",
      "Epoch 107 Batch  574/718   train_loss = 2.887\n",
      "Epoch 107 Batch  674/718   train_loss = 2.927\n",
      "Epoch 108 Batch   56/718   train_loss = 3.072\n",
      "Epoch 108 Batch  156/718   train_loss = 2.905\n",
      "Epoch 108 Batch  256/718   train_loss = 2.880\n",
      "Epoch 108 Batch  356/718   train_loss = 2.888\n",
      "Epoch 108 Batch  456/718   train_loss = 2.943\n",
      "Epoch 108 Batch  556/718   train_loss = 2.888\n",
      "Epoch 108 Batch  656/718   train_loss = 2.935\n",
      "Epoch 109 Batch   38/718   train_loss = 3.002\n",
      "Epoch 109 Batch  138/718   train_loss = 2.970\n",
      "Epoch 109 Batch  238/718   train_loss = 3.032\n",
      "Epoch 109 Batch  338/718   train_loss = 2.984\n",
      "Epoch 109 Batch  438/718   train_loss = 2.881\n",
      "Epoch 109 Batch  538/718   train_loss = 2.936\n",
      "Epoch 109 Batch  638/718   train_loss = 2.919\n",
      "Epoch 110 Batch   20/718   train_loss = 2.952\n",
      "Epoch 110 Batch  120/718   train_loss = 2.934\n",
      "Epoch 110 Batch  220/718   train_loss = 2.909\n",
      "Epoch 110 Batch  320/718   train_loss = 3.010\n",
      "Epoch 110 Batch  420/718   train_loss = 3.067\n",
      "Epoch 110 Batch  520/718   train_loss = 3.002\n",
      "Epoch 110 Batch  620/718   train_loss = 3.128\n",
      "Epoch 111 Batch    2/718   train_loss = 3.020\n",
      "Epoch 111 Batch  102/718   train_loss = 2.904\n",
      "Epoch 111 Batch  202/718   train_loss = 2.906\n",
      "Epoch 111 Batch  302/718   train_loss = 3.004\n",
      "Epoch 111 Batch  402/718   train_loss = 2.982\n",
      "Epoch 111 Batch  502/718   train_loss = 2.939\n",
      "Epoch 111 Batch  602/718   train_loss = 2.980\n",
      "Epoch 111 Batch  702/718   train_loss = 2.914\n",
      "Epoch 112 Batch   84/718   train_loss = 2.894\n",
      "Epoch 112 Batch  184/718   train_loss = 2.931\n",
      "Epoch 112 Batch  284/718   train_loss = 2.970\n",
      "Epoch 112 Batch  384/718   train_loss = 2.930\n",
      "Epoch 112 Batch  484/718   train_loss = 2.927\n",
      "Epoch 112 Batch  584/718   train_loss = 2.963\n",
      "Epoch 112 Batch  684/718   train_loss = 2.927\n",
      "Epoch 113 Batch   66/718   train_loss = 2.905\n",
      "Epoch 113 Batch  166/718   train_loss = 2.923\n",
      "Epoch 113 Batch  266/718   train_loss = 2.972\n",
      "Epoch 113 Batch  366/718   train_loss = 3.015\n",
      "Epoch 113 Batch  466/718   train_loss = 2.903\n",
      "Epoch 113 Batch  566/718   train_loss = 2.923\n",
      "Epoch 113 Batch  666/718   train_loss = 2.936\n",
      "Epoch 114 Batch   48/718   train_loss = 2.877\n",
      "Epoch 114 Batch  148/718   train_loss = 2.913\n",
      "Epoch 114 Batch  248/718   train_loss = 3.012\n",
      "Epoch 114 Batch  348/718   train_loss = 2.861\n",
      "Epoch 114 Batch  448/718   train_loss = 2.973\n",
      "Epoch 114 Batch  548/718   train_loss = 2.983\n",
      "Epoch 114 Batch  648/718   train_loss = 2.916\n",
      "Epoch 115 Batch   30/718   train_loss = 2.906\n",
      "Epoch 115 Batch  130/718   train_loss = 2.903\n",
      "Epoch 115 Batch  230/718   train_loss = 2.932\n",
      "Epoch 115 Batch  330/718   train_loss = 2.976\n",
      "Epoch 115 Batch  430/718   train_loss = 3.079\n",
      "Epoch 115 Batch  530/718   train_loss = 2.904\n",
      "Epoch 115 Batch  630/718   train_loss = 2.893\n",
      "Epoch 116 Batch   12/718   train_loss = 2.845\n",
      "Epoch 116 Batch  112/718   train_loss = 2.951\n",
      "Epoch 116 Batch  212/718   train_loss = 3.013\n",
      "Epoch 116 Batch  312/718   train_loss = 2.946\n",
      "Epoch 116 Batch  412/718   train_loss = 2.828\n",
      "Epoch 116 Batch  512/718   train_loss = 2.920\n",
      "Epoch 116 Batch  612/718   train_loss = 3.004\n",
      "Epoch 116 Batch  712/718   train_loss = 3.014\n",
      "Epoch 117 Batch   94/718   train_loss = 2.781\n",
      "Epoch 117 Batch  194/718   train_loss = 2.822\n",
      "Epoch 117 Batch  294/718   train_loss = 3.010\n",
      "Epoch 117 Batch  394/718   train_loss = 2.899\n",
      "Epoch 117 Batch  494/718   train_loss = 3.043\n",
      "Epoch 117 Batch  594/718   train_loss = 2.989\n",
      "Epoch 117 Batch  694/718   train_loss = 2.968\n",
      "Epoch 118 Batch   76/718   train_loss = 2.835\n",
      "Epoch 118 Batch  176/718   train_loss = 2.983\n",
      "Epoch 118 Batch  276/718   train_loss = 2.965\n",
      "Epoch 118 Batch  376/718   train_loss = 2.907\n",
      "Epoch 118 Batch  476/718   train_loss = 2.901\n",
      "Epoch 118 Batch  576/718   train_loss = 2.973\n",
      "Epoch 118 Batch  676/718   train_loss = 2.920\n",
      "Epoch 119 Batch   58/718   train_loss = 2.910\n",
      "Epoch 119 Batch  158/718   train_loss = 2.954\n",
      "Epoch 119 Batch  258/718   train_loss = 2.873\n",
      "Epoch 119 Batch  358/718   train_loss = 2.834\n",
      "Epoch 119 Batch  458/718   train_loss = 2.985\n",
      "Epoch 119 Batch  558/718   train_loss = 3.049\n",
      "Epoch 119 Batch  658/718   train_loss = 2.813\n",
      "Epoch 120 Batch   40/718   train_loss = 2.914\n",
      "Epoch 120 Batch  140/718   train_loss = 2.943\n",
      "Epoch 120 Batch  240/718   train_loss = 2.823\n",
      "Epoch 120 Batch  340/718   train_loss = 3.008\n",
      "Epoch 120 Batch  440/718   train_loss = 2.962\n",
      "Epoch 120 Batch  540/718   train_loss = 3.001\n",
      "Epoch 120 Batch  640/718   train_loss = 2.896\n",
      "Epoch 121 Batch   22/718   train_loss = 2.781\n",
      "Epoch 121 Batch  122/718   train_loss = 2.889\n",
      "Epoch 121 Batch  222/718   train_loss = 2.932\n",
      "Epoch 121 Batch  322/718   train_loss = 2.965\n",
      "Epoch 121 Batch  422/718   train_loss = 2.986\n",
      "Epoch 121 Batch  522/718   train_loss = 2.869\n",
      "Epoch 121 Batch  622/718   train_loss = 2.893\n",
      "Epoch 122 Batch    4/718   train_loss = 2.835\n",
      "Epoch 122 Batch  104/718   train_loss = 2.878\n",
      "Epoch 122 Batch  204/718   train_loss = 2.823\n",
      "Epoch 122 Batch  304/718   train_loss = 2.932\n",
      "Epoch 122 Batch  404/718   train_loss = 2.816\n",
      "Epoch 122 Batch  504/718   train_loss = 3.037\n",
      "Epoch 122 Batch  604/718   train_loss = 2.830\n",
      "Epoch 122 Batch  704/718   train_loss = 2.886\n",
      "Epoch 123 Batch   86/718   train_loss = 2.886\n",
      "Epoch 123 Batch  186/718   train_loss = 2.918\n",
      "Epoch 123 Batch  286/718   train_loss = 3.027\n",
      "Epoch 123 Batch  386/718   train_loss = 2.917\n",
      "Epoch 123 Batch  486/718   train_loss = 2.910\n",
      "Epoch 123 Batch  586/718   train_loss = 2.916\n",
      "Epoch 123 Batch  686/718   train_loss = 2.851\n",
      "Epoch 124 Batch   68/718   train_loss = 2.903\n",
      "Epoch 124 Batch  168/718   train_loss = 3.015\n",
      "Epoch 124 Batch  268/718   train_loss = 2.905\n",
      "Epoch 124 Batch  368/718   train_loss = 2.848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124 Batch  468/718   train_loss = 2.997\n",
      "Epoch 124 Batch  568/718   train_loss = 3.016\n",
      "Epoch 124 Batch  668/718   train_loss = 2.886\n",
      "Epoch 125 Batch   50/718   train_loss = 2.977\n",
      "Epoch 125 Batch  150/718   train_loss = 2.919\n",
      "Epoch 125 Batch  250/718   train_loss = 2.885\n",
      "Epoch 125 Batch  350/718   train_loss = 2.890\n",
      "Epoch 125 Batch  450/718   train_loss = 2.931\n",
      "Epoch 125 Batch  550/718   train_loss = 2.857\n",
      "Epoch 125 Batch  650/718   train_loss = 2.827\n",
      "Epoch 126 Batch   32/718   train_loss = 2.857\n",
      "Epoch 126 Batch  132/718   train_loss = 2.879\n",
      "Epoch 126 Batch  232/718   train_loss = 2.841\n",
      "Epoch 126 Batch  332/718   train_loss = 2.832\n",
      "Epoch 126 Batch  432/718   train_loss = 3.024\n",
      "Epoch 126 Batch  532/718   train_loss = 3.018\n",
      "Epoch 126 Batch  632/718   train_loss = 2.792\n",
      "Epoch 127 Batch   14/718   train_loss = 2.994\n",
      "Epoch 127 Batch  114/718   train_loss = 2.833\n",
      "Epoch 127 Batch  214/718   train_loss = 2.920\n",
      "Epoch 127 Batch  314/718   train_loss = 2.868\n",
      "Epoch 127 Batch  414/718   train_loss = 2.821\n",
      "Epoch 127 Batch  514/718   train_loss = 2.967\n",
      "Epoch 127 Batch  614/718   train_loss = 2.871\n",
      "Epoch 127 Batch  714/718   train_loss = 2.856\n",
      "Epoch 128 Batch   96/718   train_loss = 2.870\n",
      "Epoch 128 Batch  196/718   train_loss = 2.814\n",
      "Epoch 128 Batch  296/718   train_loss = 2.895\n",
      "Epoch 128 Batch  396/718   train_loss = 2.921\n",
      "Epoch 128 Batch  496/718   train_loss = 2.951\n",
      "Epoch 128 Batch  596/718   train_loss = 2.883\n",
      "Epoch 128 Batch  696/718   train_loss = 2.872\n",
      "Epoch 129 Batch   78/718   train_loss = 2.928\n",
      "Epoch 129 Batch  178/718   train_loss = 2.926\n",
      "Epoch 129 Batch  278/718   train_loss = 2.849\n",
      "Epoch 129 Batch  378/718   train_loss = 2.788\n",
      "Epoch 129 Batch  478/718   train_loss = 2.841\n",
      "Epoch 129 Batch  578/718   train_loss = 2.788\n",
      "Epoch 129 Batch  678/718   train_loss = 2.898\n",
      "Epoch 130 Batch   60/718   train_loss = 2.893\n",
      "Epoch 130 Batch  160/718   train_loss = 2.914\n",
      "Epoch 130 Batch  260/718   train_loss = 2.930\n",
      "Epoch 130 Batch  360/718   train_loss = 2.846\n",
      "Epoch 130 Batch  460/718   train_loss = 2.851\n",
      "Epoch 130 Batch  560/718   train_loss = 2.874\n",
      "Epoch 130 Batch  660/718   train_loss = 2.975\n",
      "Epoch 131 Batch   42/718   train_loss = 2.903\n",
      "Epoch 131 Batch  142/718   train_loss = 2.967\n",
      "Epoch 131 Batch  242/718   train_loss = 2.783\n",
      "Epoch 131 Batch  342/718   train_loss = 2.860\n",
      "Epoch 131 Batch  442/718   train_loss = 2.881\n",
      "Epoch 131 Batch  542/718   train_loss = 2.842\n",
      "Epoch 131 Batch  642/718   train_loss = 2.944\n",
      "Epoch 132 Batch   24/718   train_loss = 2.867\n",
      "Epoch 132 Batch  124/718   train_loss = 2.810\n",
      "Epoch 132 Batch  224/718   train_loss = 2.856\n",
      "Epoch 132 Batch  324/718   train_loss = 2.873\n",
      "Epoch 132 Batch  424/718   train_loss = 2.868\n",
      "Epoch 132 Batch  524/718   train_loss = 2.896\n",
      "Epoch 132 Batch  624/718   train_loss = 2.911\n",
      "Epoch 133 Batch    6/718   train_loss = 2.817\n",
      "Epoch 133 Batch  106/718   train_loss = 2.840\n",
      "Epoch 133 Batch  206/718   train_loss = 2.767\n",
      "Epoch 133 Batch  306/718   train_loss = 2.870\n",
      "Epoch 133 Batch  406/718   train_loss = 2.812\n",
      "Epoch 133 Batch  506/718   train_loss = 2.867\n",
      "Epoch 133 Batch  606/718   train_loss = 2.840\n",
      "Epoch 133 Batch  706/718   train_loss = 2.826\n",
      "Epoch 134 Batch   88/718   train_loss = 2.930\n",
      "Epoch 134 Batch  188/718   train_loss = 2.854\n",
      "Epoch 134 Batch  288/718   train_loss = 2.857\n",
      "Epoch 134 Batch  388/718   train_loss = 2.814\n",
      "Epoch 134 Batch  488/718   train_loss = 2.797\n",
      "Epoch 134 Batch  588/718   train_loss = 2.969\n",
      "Epoch 134 Batch  688/718   train_loss = 2.839\n",
      "Epoch 135 Batch   70/718   train_loss = 2.871\n",
      "Epoch 135 Batch  170/718   train_loss = 2.908\n",
      "Epoch 135 Batch  270/718   train_loss = 2.924\n",
      "Epoch 135 Batch  370/718   train_loss = 2.826\n",
      "Epoch 135 Batch  470/718   train_loss = 2.956\n",
      "Epoch 135 Batch  570/718   train_loss = 2.911\n",
      "Epoch 135 Batch  670/718   train_loss = 2.870\n",
      "Epoch 136 Batch   52/718   train_loss = 2.792\n",
      "Epoch 136 Batch  152/718   train_loss = 2.810\n",
      "Epoch 136 Batch  252/718   train_loss = 2.883\n",
      "Epoch 136 Batch  352/718   train_loss = 2.847\n",
      "Epoch 136 Batch  452/718   train_loss = 2.887\n",
      "Epoch 136 Batch  552/718   train_loss = 2.891\n",
      "Epoch 136 Batch  652/718   train_loss = 2.853\n",
      "Epoch 137 Batch   34/718   train_loss = 2.823\n",
      "Epoch 137 Batch  134/718   train_loss = 2.791\n",
      "Epoch 137 Batch  234/718   train_loss = 2.936\n",
      "Epoch 137 Batch  334/718   train_loss = 2.922\n",
      "Epoch 137 Batch  434/718   train_loss = 2.923\n",
      "Epoch 137 Batch  534/718   train_loss = 2.872\n",
      "Epoch 137 Batch  634/718   train_loss = 2.942\n",
      "Epoch 138 Batch   16/718   train_loss = 2.808\n",
      "Epoch 138 Batch  116/718   train_loss = 2.819\n",
      "Epoch 138 Batch  216/718   train_loss = 2.790\n",
      "Epoch 138 Batch  316/718   train_loss = 2.831\n",
      "Epoch 138 Batch  416/718   train_loss = 2.868\n",
      "Epoch 138 Batch  516/718   train_loss = 2.843\n",
      "Epoch 138 Batch  616/718   train_loss = 2.898\n",
      "Epoch 138 Batch  716/718   train_loss = 2.886\n",
      "Epoch 139 Batch   98/718   train_loss = 2.769\n",
      "Epoch 139 Batch  198/718   train_loss = 2.854\n",
      "Epoch 139 Batch  298/718   train_loss = 2.856\n",
      "Epoch 139 Batch  398/718   train_loss = 2.820\n",
      "Epoch 139 Batch  498/718   train_loss = 2.907\n",
      "Epoch 139 Batch  598/718   train_loss = 2.958\n",
      "Epoch 139 Batch  698/718   train_loss = 2.937\n",
      "Epoch 140 Batch   80/718   train_loss = 2.844\n",
      "Epoch 140 Batch  180/718   train_loss = 2.806\n",
      "Epoch 140 Batch  280/718   train_loss = 2.931\n",
      "Epoch 140 Batch  380/718   train_loss = 2.902\n",
      "Epoch 140 Batch  480/718   train_loss = 2.895\n",
      "Epoch 140 Batch  580/718   train_loss = 2.823\n",
      "Epoch 140 Batch  680/718   train_loss = 2.958\n",
      "Epoch 141 Batch   62/718   train_loss = 2.836\n",
      "Epoch 141 Batch  162/718   train_loss = 2.885\n",
      "Epoch 141 Batch  262/718   train_loss = 2.909\n",
      "Epoch 141 Batch  362/718   train_loss = 2.882\n",
      "Epoch 141 Batch  462/718   train_loss = 2.871\n",
      "Epoch 141 Batch  562/718   train_loss = 2.867\n",
      "Epoch 141 Batch  662/718   train_loss = 2.857\n",
      "Epoch 142 Batch   44/718   train_loss = 2.843\n",
      "Epoch 142 Batch  144/718   train_loss = 2.910\n",
      "Epoch 142 Batch  244/718   train_loss = 2.956\n",
      "Epoch 142 Batch  344/718   train_loss = 2.868\n",
      "Epoch 142 Batch  444/718   train_loss = 2.803\n",
      "Epoch 142 Batch  544/718   train_loss = 2.869\n",
      "Epoch 142 Batch  644/718   train_loss = 2.977\n",
      "Epoch 143 Batch   26/718   train_loss = 2.932\n",
      "Epoch 143 Batch  126/718   train_loss = 2.817\n",
      "Epoch 143 Batch  226/718   train_loss = 2.847\n",
      "Epoch 143 Batch  326/718   train_loss = 2.805\n",
      "Epoch 143 Batch  426/718   train_loss = 2.781\n",
      "Epoch 143 Batch  526/718   train_loss = 2.803\n",
      "Epoch 143 Batch  626/718   train_loss = 2.939\n",
      "Epoch 144 Batch    8/718   train_loss = 2.790\n",
      "Epoch 144 Batch  108/718   train_loss = 2.745\n",
      "Epoch 144 Batch  208/718   train_loss = 2.891\n",
      "Epoch 144 Batch  308/718   train_loss = 2.947\n",
      "Epoch 144 Batch  408/718   train_loss = 2.950\n",
      "Epoch 144 Batch  508/718   train_loss = 2.895\n",
      "Epoch 144 Batch  608/718   train_loss = 2.975\n",
      "Epoch 144 Batch  708/718   train_loss = 2.826\n",
      "Epoch 145 Batch   90/718   train_loss = 2.825\n",
      "Epoch 145 Batch  190/718   train_loss = 2.933\n",
      "Epoch 145 Batch  290/718   train_loss = 2.817\n",
      "Epoch 145 Batch  390/718   train_loss = 2.814\n",
      "Epoch 145 Batch  490/718   train_loss = 2.836\n",
      "Epoch 145 Batch  590/718   train_loss = 2.834\n",
      "Epoch 145 Batch  690/718   train_loss = 2.894\n",
      "Epoch 146 Batch   72/718   train_loss = 2.878\n",
      "Epoch 146 Batch  172/718   train_loss = 2.889\n",
      "Epoch 146 Batch  272/718   train_loss = 2.872\n",
      "Epoch 146 Batch  372/718   train_loss = 2.837\n",
      "Epoch 146 Batch  472/718   train_loss = 2.913\n",
      "Epoch 146 Batch  572/718   train_loss = 2.902\n",
      "Epoch 146 Batch  672/718   train_loss = 2.791\n",
      "Epoch 147 Batch   54/718   train_loss = 2.710\n",
      "Epoch 147 Batch  154/718   train_loss = 2.941\n",
      "Epoch 147 Batch  254/718   train_loss = 2.818\n",
      "Epoch 147 Batch  354/718   train_loss = 2.909\n",
      "Epoch 147 Batch  454/718   train_loss = 2.802\n",
      "Epoch 147 Batch  554/718   train_loss = 2.881\n",
      "Epoch 147 Batch  654/718   train_loss = 2.957\n",
      "Epoch 148 Batch   36/718   train_loss = 2.835\n",
      "Epoch 148 Batch  136/718   train_loss = 2.767\n",
      "Epoch 148 Batch  236/718   train_loss = 2.762\n",
      "Epoch 148 Batch  336/718   train_loss = 2.831\n",
      "Epoch 148 Batch  436/718   train_loss = 2.847\n",
      "Epoch 148 Batch  536/718   train_loss = 2.865\n",
      "Epoch 148 Batch  636/718   train_loss = 2.912\n",
      "Epoch 149 Batch   18/718   train_loss = 2.835\n",
      "Epoch 149 Batch  118/718   train_loss = 2.819\n",
      "Epoch 149 Batch  218/718   train_loss = 2.842\n",
      "Epoch 149 Batch  318/718   train_loss = 2.830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 Batch  418/718   train_loss = 2.924\n",
      "Epoch 149 Batch  518/718   train_loss = 2.925\n",
      "Epoch 149 Batch  618/718   train_loss = 2.823\n",
      "Epoch 150 Batch    0/718   train_loss = 2.918\n",
      "Epoch 150 Batch  100/718   train_loss = 2.803\n",
      "Epoch 150 Batch  200/718   train_loss = 2.790\n",
      "Epoch 150 Batch  300/718   train_loss = 2.869\n",
      "Epoch 150 Batch  400/718   train_loss = 2.842\n",
      "Epoch 150 Batch  500/718   train_loss = 2.946\n",
      "Epoch 150 Batch  600/718   train_loss = 2.901\n",
      "Epoch 150 Batch  700/718   train_loss = 2.753\n",
      "Epoch 151 Batch   82/718   train_loss = 2.837\n",
      "Epoch 151 Batch  182/718   train_loss = 2.891\n",
      "Epoch 151 Batch  282/718   train_loss = 2.911\n",
      "Epoch 151 Batch  382/718   train_loss = 2.767\n",
      "Epoch 151 Batch  482/718   train_loss = 2.934\n",
      "Epoch 151 Batch  582/718   train_loss = 2.788\n",
      "Epoch 151 Batch  682/718   train_loss = 2.841\n",
      "Epoch 152 Batch   64/718   train_loss = 2.801\n",
      "Epoch 152 Batch  164/718   train_loss = 2.723\n",
      "Epoch 152 Batch  264/718   train_loss = 2.823\n",
      "Epoch 152 Batch  364/718   train_loss = 2.890\n",
      "Epoch 152 Batch  464/718   train_loss = 2.762\n",
      "Epoch 152 Batch  564/718   train_loss = 2.850\n",
      "Epoch 152 Batch  664/718   train_loss = 2.907\n",
      "Epoch 153 Batch   46/718   train_loss = 2.749\n",
      "Epoch 153 Batch  146/718   train_loss = 2.762\n",
      "Epoch 153 Batch  246/718   train_loss = 2.921\n",
      "Epoch 153 Batch  346/718   train_loss = 2.754\n",
      "Epoch 153 Batch  446/718   train_loss = 2.810\n",
      "Epoch 153 Batch  546/718   train_loss = 2.695\n",
      "Epoch 153 Batch  646/718   train_loss = 2.907\n",
      "Epoch 154 Batch   28/718   train_loss = 2.781\n",
      "Epoch 154 Batch  128/718   train_loss = 2.785\n",
      "Epoch 154 Batch  228/718   train_loss = 2.892\n",
      "Epoch 154 Batch  328/718   train_loss = 2.778\n",
      "Epoch 154 Batch  428/718   train_loss = 2.706\n",
      "Epoch 154 Batch  528/718   train_loss = 2.870\n",
      "Epoch 154 Batch  628/718   train_loss = 2.945\n",
      "Epoch 155 Batch   10/718   train_loss = 2.794\n",
      "Epoch 155 Batch  110/718   train_loss = 2.834\n",
      "Epoch 155 Batch  210/718   train_loss = 2.750\n",
      "Epoch 155 Batch  310/718   train_loss = 2.905\n",
      "Epoch 155 Batch  410/718   train_loss = 2.707\n",
      "Epoch 155 Batch  510/718   train_loss = 2.900\n",
      "Epoch 155 Batch  610/718   train_loss = 2.813\n",
      "Epoch 155 Batch  710/718   train_loss = 2.769\n",
      "Epoch 156 Batch   92/718   train_loss = 2.765\n",
      "Epoch 156 Batch  192/718   train_loss = 2.801\n",
      "Epoch 156 Batch  292/718   train_loss = 2.712\n",
      "Epoch 156 Batch  392/718   train_loss = 2.767\n",
      "Epoch 156 Batch  492/718   train_loss = 2.853\n",
      "Epoch 156 Batch  592/718   train_loss = 2.819\n",
      "Epoch 156 Batch  692/718   train_loss = 2.834\n",
      "Epoch 157 Batch   74/718   train_loss = 2.820\n",
      "Epoch 157 Batch  174/718   train_loss = 2.891\n",
      "Epoch 157 Batch  274/718   train_loss = 2.877\n",
      "Epoch 157 Batch  374/718   train_loss = 2.782\n",
      "Epoch 157 Batch  474/718   train_loss = 2.873\n",
      "Epoch 157 Batch  574/718   train_loss = 2.695\n",
      "Epoch 157 Batch  674/718   train_loss = 2.794\n",
      "Epoch 158 Batch   56/718   train_loss = 2.922\n",
      "Epoch 158 Batch  156/718   train_loss = 2.722\n",
      "Epoch 158 Batch  256/718   train_loss = 2.716\n",
      "Epoch 158 Batch  356/718   train_loss = 2.756\n",
      "Epoch 158 Batch  456/718   train_loss = 2.782\n",
      "Epoch 158 Batch  556/718   train_loss = 2.729\n",
      "Epoch 158 Batch  656/718   train_loss = 2.802\n",
      "Epoch 159 Batch   38/718   train_loss = 2.871\n",
      "Epoch 159 Batch  138/718   train_loss = 2.841\n",
      "Epoch 159 Batch  238/718   train_loss = 2.843\n",
      "Epoch 159 Batch  338/718   train_loss = 2.834\n",
      "Epoch 159 Batch  438/718   train_loss = 2.720\n",
      "Epoch 159 Batch  538/718   train_loss = 2.789\n",
      "Epoch 159 Batch  638/718   train_loss = 2.746\n",
      "Epoch 160 Batch   20/718   train_loss = 2.813\n",
      "Epoch 160 Batch  120/718   train_loss = 2.799\n",
      "Epoch 160 Batch  220/718   train_loss = 2.808\n",
      "Epoch 160 Batch  320/718   train_loss = 2.881\n",
      "Epoch 160 Batch  420/718   train_loss = 2.882\n",
      "Epoch 160 Batch  520/718   train_loss = 2.844\n",
      "Epoch 160 Batch  620/718   train_loss = 2.952\n",
      "Epoch 161 Batch    2/718   train_loss = 2.844\n",
      "Epoch 161 Batch  102/718   train_loss = 2.721\n",
      "Epoch 161 Batch  202/718   train_loss = 2.757\n",
      "Epoch 161 Batch  302/718   train_loss = 2.893\n",
      "Epoch 161 Batch  402/718   train_loss = 2.870\n",
      "Epoch 161 Batch  502/718   train_loss = 2.798\n",
      "Epoch 161 Batch  602/718   train_loss = 2.856\n",
      "Epoch 161 Batch  702/718   train_loss = 2.766\n",
      "Epoch 162 Batch   84/718   train_loss = 2.689\n",
      "Epoch 162 Batch  184/718   train_loss = 2.790\n",
      "Epoch 162 Batch  284/718   train_loss = 2.772\n",
      "Epoch 162 Batch  384/718   train_loss = 2.776\n",
      "Epoch 162 Batch  484/718   train_loss = 2.780\n",
      "Epoch 162 Batch  584/718   train_loss = 2.757\n",
      "Epoch 162 Batch  684/718   train_loss = 2.832\n",
      "Epoch 163 Batch   66/718   train_loss = 2.748\n",
      "Epoch 163 Batch  166/718   train_loss = 2.761\n",
      "Epoch 163 Batch  266/718   train_loss = 2.839\n",
      "Epoch 163 Batch  366/718   train_loss = 2.847\n",
      "Epoch 163 Batch  466/718   train_loss = 2.817\n",
      "Epoch 163 Batch  566/718   train_loss = 2.749\n",
      "Epoch 163 Batch  666/718   train_loss = 2.751\n",
      "Epoch 164 Batch   48/718   train_loss = 2.656\n",
      "Epoch 164 Batch  148/718   train_loss = 2.792\n",
      "Epoch 164 Batch  248/718   train_loss = 2.853\n",
      "Epoch 164 Batch  348/718   train_loss = 2.769\n",
      "Epoch 164 Batch  448/718   train_loss = 2.798\n",
      "Epoch 164 Batch  548/718   train_loss = 2.891\n",
      "Epoch 164 Batch  648/718   train_loss = 2.830\n",
      "Epoch 165 Batch   30/718   train_loss = 2.766\n",
      "Epoch 165 Batch  130/718   train_loss = 2.705\n",
      "Epoch 165 Batch  230/718   train_loss = 2.829\n",
      "Epoch 165 Batch  330/718   train_loss = 2.829\n",
      "Epoch 165 Batch  430/718   train_loss = 2.933\n",
      "Epoch 165 Batch  530/718   train_loss = 2.777\n",
      "Epoch 165 Batch  630/718   train_loss = 2.739\n",
      "Epoch 166 Batch   12/718   train_loss = 2.690\n",
      "Epoch 166 Batch  112/718   train_loss = 2.829\n",
      "Epoch 166 Batch  212/718   train_loss = 2.838\n",
      "Epoch 166 Batch  312/718   train_loss = 2.807\n",
      "Epoch 166 Batch  412/718   train_loss = 2.701\n",
      "Epoch 166 Batch  512/718   train_loss = 2.780\n",
      "Epoch 166 Batch  612/718   train_loss = 2.871\n",
      "Epoch 166 Batch  712/718   train_loss = 2.854\n",
      "Epoch 167 Batch   94/718   train_loss = 2.685\n",
      "Epoch 167 Batch  194/718   train_loss = 2.677\n",
      "Epoch 167 Batch  294/718   train_loss = 2.892\n",
      "Epoch 167 Batch  394/718   train_loss = 2.727\n",
      "Epoch 167 Batch  494/718   train_loss = 2.901\n",
      "Epoch 167 Batch  594/718   train_loss = 2.809\n",
      "Epoch 167 Batch  694/718   train_loss = 2.838\n",
      "Epoch 168 Batch   76/718   train_loss = 2.740\n",
      "Epoch 168 Batch  176/718   train_loss = 2.846\n",
      "Epoch 168 Batch  276/718   train_loss = 2.792\n",
      "Epoch 168 Batch  376/718   train_loss = 2.809\n",
      "Epoch 168 Batch  476/718   train_loss = 2.775\n",
      "Epoch 168 Batch  576/718   train_loss = 2.813\n",
      "Epoch 168 Batch  676/718   train_loss = 2.825\n",
      "Epoch 169 Batch   58/718   train_loss = 2.774\n",
      "Epoch 169 Batch  158/718   train_loss = 2.850\n",
      "Epoch 169 Batch  258/718   train_loss = 2.779\n",
      "Epoch 169 Batch  358/718   train_loss = 2.670\n",
      "Epoch 169 Batch  458/718   train_loss = 2.852\n",
      "Epoch 169 Batch  558/718   train_loss = 2.893\n",
      "Epoch 169 Batch  658/718   train_loss = 2.709\n",
      "Epoch 170 Batch   40/718   train_loss = 2.812\n",
      "Epoch 170 Batch  140/718   train_loss = 2.785\n",
      "Epoch 170 Batch  240/718   train_loss = 2.721\n",
      "Epoch 170 Batch  340/718   train_loss = 2.822\n",
      "Epoch 170 Batch  440/718   train_loss = 2.823\n",
      "Epoch 170 Batch  540/718   train_loss = 2.862\n",
      "Epoch 170 Batch  640/718   train_loss = 2.768\n",
      "Epoch 171 Batch   22/718   train_loss = 2.622\n",
      "Epoch 171 Batch  122/718   train_loss = 2.753\n",
      "Epoch 171 Batch  222/718   train_loss = 2.794\n",
      "Epoch 171 Batch  322/718   train_loss = 2.810\n",
      "Epoch 171 Batch  422/718   train_loss = 2.837\n",
      "Epoch 171 Batch  522/718   train_loss = 2.733\n",
      "Epoch 171 Batch  622/718   train_loss = 2.828\n",
      "Epoch 172 Batch    4/718   train_loss = 2.696\n",
      "Epoch 172 Batch  104/718   train_loss = 2.781\n",
      "Epoch 172 Batch  204/718   train_loss = 2.691\n",
      "Epoch 172 Batch  304/718   train_loss = 2.779\n",
      "Epoch 172 Batch  404/718   train_loss = 2.700\n",
      "Epoch 172 Batch  504/718   train_loss = 2.916\n",
      "Epoch 172 Batch  604/718   train_loss = 2.673\n",
      "Epoch 172 Batch  704/718   train_loss = 2.739\n",
      "Epoch 173 Batch   86/718   train_loss = 2.765\n",
      "Epoch 173 Batch  186/718   train_loss = 2.754\n",
      "Epoch 173 Batch  286/718   train_loss = 2.832\n",
      "Epoch 173 Batch  386/718   train_loss = 2.783\n",
      "Epoch 173 Batch  486/718   train_loss = 2.779\n",
      "Epoch 173 Batch  586/718   train_loss = 2.734\n",
      "Epoch 173 Batch  686/718   train_loss = 2.740\n",
      "Epoch 174 Batch   68/718   train_loss = 2.797\n",
      "Epoch 174 Batch  168/718   train_loss = 2.880\n",
      "Epoch 174 Batch  268/718   train_loss = 2.774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174 Batch  368/718   train_loss = 2.719\n",
      "Epoch 174 Batch  468/718   train_loss = 2.948\n",
      "Epoch 174 Batch  568/718   train_loss = 2.858\n",
      "Epoch 174 Batch  668/718   train_loss = 2.779\n",
      "Epoch 175 Batch   50/718   train_loss = 2.857\n",
      "Epoch 175 Batch  150/718   train_loss = 2.750\n",
      "Epoch 175 Batch  250/718   train_loss = 2.734\n",
      "Epoch 175 Batch  350/718   train_loss = 2.781\n",
      "Epoch 175 Batch  450/718   train_loss = 2.793\n",
      "Epoch 175 Batch  550/718   train_loss = 2.740\n",
      "Epoch 175 Batch  650/718   train_loss = 2.704\n",
      "Epoch 176 Batch   32/718   train_loss = 2.704\n",
      "Epoch 176 Batch  132/718   train_loss = 2.743\n",
      "Epoch 176 Batch  232/718   train_loss = 2.713\n",
      "Epoch 176 Batch  332/718   train_loss = 2.705\n",
      "Epoch 176 Batch  432/718   train_loss = 2.886\n",
      "Epoch 176 Batch  532/718   train_loss = 2.893\n",
      "Epoch 176 Batch  632/718   train_loss = 2.643\n",
      "Epoch 177 Batch   14/718   train_loss = 2.816\n",
      "Epoch 177 Batch  114/718   train_loss = 2.721\n",
      "Epoch 177 Batch  214/718   train_loss = 2.780\n",
      "Epoch 177 Batch  314/718   train_loss = 2.799\n",
      "Epoch 177 Batch  414/718   train_loss = 2.670\n",
      "Epoch 177 Batch  514/718   train_loss = 2.899\n",
      "Epoch 177 Batch  614/718   train_loss = 2.751\n",
      "Epoch 177 Batch  714/718   train_loss = 2.746\n",
      "Epoch 178 Batch   96/718   train_loss = 2.768\n",
      "Epoch 178 Batch  196/718   train_loss = 2.706\n",
      "Epoch 178 Batch  296/718   train_loss = 2.777\n",
      "Epoch 178 Batch  396/718   train_loss = 2.745\n",
      "Epoch 178 Batch  496/718   train_loss = 2.830\n",
      "Epoch 178 Batch  596/718   train_loss = 2.748\n",
      "Epoch 178 Batch  696/718   train_loss = 2.786\n",
      "Epoch 179 Batch   78/718   train_loss = 2.814\n",
      "Epoch 179 Batch  178/718   train_loss = 2.816\n",
      "Epoch 179 Batch  278/718   train_loss = 2.697\n",
      "Epoch 179 Batch  378/718   train_loss = 2.660\n",
      "Epoch 179 Batch  478/718   train_loss = 2.740\n",
      "Epoch 179 Batch  578/718   train_loss = 2.712\n",
      "Epoch 179 Batch  678/718   train_loss = 2.783\n",
      "Epoch 180 Batch   60/718   train_loss = 2.778\n",
      "Epoch 180 Batch  160/718   train_loss = 2.818\n",
      "Epoch 180 Batch  260/718   train_loss = 2.812\n",
      "Epoch 180 Batch  360/718   train_loss = 2.736\n",
      "Epoch 180 Batch  460/718   train_loss = 2.740\n",
      "Epoch 180 Batch  560/718   train_loss = 2.770\n",
      "Epoch 180 Batch  660/718   train_loss = 2.890\n",
      "Epoch 181 Batch   42/718   train_loss = 2.807\n",
      "Epoch 181 Batch  142/718   train_loss = 2.809\n",
      "Epoch 181 Batch  242/718   train_loss = 2.644\n",
      "Epoch 181 Batch  342/718   train_loss = 2.767\n",
      "Epoch 181 Batch  442/718   train_loss = 2.765\n",
      "Epoch 181 Batch  542/718   train_loss = 2.703\n",
      "Epoch 181 Batch  642/718   train_loss = 2.828\n",
      "Epoch 182 Batch   24/718   train_loss = 2.726\n",
      "Epoch 182 Batch  124/718   train_loss = 2.688\n",
      "Epoch 182 Batch  224/718   train_loss = 2.739\n",
      "Epoch 182 Batch  324/718   train_loss = 2.726\n",
      "Epoch 182 Batch  424/718   train_loss = 2.724\n",
      "Epoch 182 Batch  524/718   train_loss = 2.775\n",
      "Epoch 182 Batch  624/718   train_loss = 2.746\n",
      "Epoch 183 Batch    6/718   train_loss = 2.674\n",
      "Epoch 183 Batch  106/718   train_loss = 2.743\n",
      "Epoch 183 Batch  206/718   train_loss = 2.666\n",
      "Epoch 183 Batch  306/718   train_loss = 2.765\n",
      "Epoch 183 Batch  406/718   train_loss = 2.737\n",
      "Epoch 183 Batch  506/718   train_loss = 2.710\n",
      "Epoch 183 Batch  606/718   train_loss = 2.697\n",
      "Epoch 183 Batch  706/718   train_loss = 2.738\n",
      "Epoch 184 Batch   88/718   train_loss = 2.817\n",
      "Epoch 184 Batch  188/718   train_loss = 2.703\n",
      "Epoch 184 Batch  288/718   train_loss = 2.715\n",
      "Epoch 184 Batch  388/718   train_loss = 2.664\n",
      "Epoch 184 Batch  488/718   train_loss = 2.656\n",
      "Epoch 184 Batch  588/718   train_loss = 2.805\n",
      "Epoch 184 Batch  688/718   train_loss = 2.708\n",
      "Epoch 185 Batch   70/718   train_loss = 2.762\n",
      "Epoch 185 Batch  170/718   train_loss = 2.809\n",
      "Epoch 185 Batch  270/718   train_loss = 2.785\n",
      "Epoch 185 Batch  370/718   train_loss = 2.774\n",
      "Epoch 185 Batch  470/718   train_loss = 2.859\n",
      "Epoch 185 Batch  570/718   train_loss = 2.806\n",
      "Epoch 185 Batch  670/718   train_loss = 2.717\n",
      "Epoch 186 Batch   52/718   train_loss = 2.698\n",
      "Epoch 186 Batch  152/718   train_loss = 2.729\n",
      "Epoch 186 Batch  252/718   train_loss = 2.773\n",
      "Epoch 186 Batch  352/718   train_loss = 2.724\n",
      "Epoch 186 Batch  452/718   train_loss = 2.771\n",
      "Epoch 186 Batch  552/718   train_loss = 2.830\n",
      "Epoch 186 Batch  652/718   train_loss = 2.709\n",
      "Epoch 187 Batch   34/718   train_loss = 2.689\n",
      "Epoch 187 Batch  134/718   train_loss = 2.697\n",
      "Epoch 187 Batch  234/718   train_loss = 2.827\n",
      "Epoch 187 Batch  334/718   train_loss = 2.816\n",
      "Epoch 187 Batch  434/718   train_loss = 2.757\n",
      "Epoch 187 Batch  534/718   train_loss = 2.742\n",
      "Epoch 187 Batch  634/718   train_loss = 2.838\n",
      "Epoch 188 Batch   16/718   train_loss = 2.688\n",
      "Epoch 188 Batch  116/718   train_loss = 2.698\n",
      "Epoch 188 Batch  216/718   train_loss = 2.708\n",
      "Epoch 188 Batch  316/718   train_loss = 2.673\n",
      "Epoch 188 Batch  416/718   train_loss = 2.758\n",
      "Epoch 188 Batch  516/718   train_loss = 2.743\n",
      "Epoch 188 Batch  616/718   train_loss = 2.773\n",
      "Epoch 188 Batch  716/718   train_loss = 2.786\n",
      "Epoch 189 Batch   98/718   train_loss = 2.681\n",
      "Epoch 189 Batch  198/718   train_loss = 2.707\n",
      "Epoch 189 Batch  298/718   train_loss = 2.726\n",
      "Epoch 189 Batch  398/718   train_loss = 2.718\n",
      "Epoch 189 Batch  498/718   train_loss = 2.754\n",
      "Epoch 189 Batch  598/718   train_loss = 2.805\n",
      "Epoch 189 Batch  698/718   train_loss = 2.784\n",
      "Epoch 190 Batch   80/718   train_loss = 2.697\n",
      "Epoch 190 Batch  180/718   train_loss = 2.697\n",
      "Epoch 190 Batch  280/718   train_loss = 2.825\n",
      "Epoch 190 Batch  380/718   train_loss = 2.784\n",
      "Epoch 190 Batch  480/718   train_loss = 2.774\n",
      "Epoch 190 Batch  580/718   train_loss = 2.676\n",
      "Epoch 190 Batch  680/718   train_loss = 2.781\n",
      "Epoch 191 Batch   62/718   train_loss = 2.722\n",
      "Epoch 191 Batch  162/718   train_loss = 2.740\n",
      "Epoch 191 Batch  262/718   train_loss = 2.798\n",
      "Epoch 191 Batch  362/718   train_loss = 2.785\n",
      "Epoch 191 Batch  462/718   train_loss = 2.729\n",
      "Epoch 191 Batch  562/718   train_loss = 2.715\n",
      "Epoch 191 Batch  662/718   train_loss = 2.740\n",
      "Epoch 192 Batch   44/718   train_loss = 2.710\n",
      "Epoch 192 Batch  144/718   train_loss = 2.816\n",
      "Epoch 192 Batch  244/718   train_loss = 2.845\n",
      "Epoch 192 Batch  344/718   train_loss = 2.781\n",
      "Epoch 192 Batch  444/718   train_loss = 2.721\n",
      "Epoch 192 Batch  544/718   train_loss = 2.727\n",
      "Epoch 192 Batch  644/718   train_loss = 2.875\n",
      "Epoch 193 Batch   26/718   train_loss = 2.843\n",
      "Epoch 193 Batch  126/718   train_loss = 2.706\n",
      "Epoch 193 Batch  226/718   train_loss = 2.705\n",
      "Epoch 193 Batch  326/718   train_loss = 2.699\n",
      "Epoch 193 Batch  426/718   train_loss = 2.686\n",
      "Epoch 193 Batch  526/718   train_loss = 2.697\n",
      "Epoch 193 Batch  626/718   train_loss = 2.803\n",
      "Epoch 194 Batch    8/718   train_loss = 2.679\n",
      "Epoch 194 Batch  108/718   train_loss = 2.647\n",
      "Epoch 194 Batch  208/718   train_loss = 2.747\n",
      "Epoch 194 Batch  308/718   train_loss = 2.821\n",
      "Epoch 194 Batch  408/718   train_loss = 2.823\n",
      "Epoch 194 Batch  508/718   train_loss = 2.820\n",
      "Epoch 194 Batch  608/718   train_loss = 2.871\n",
      "Epoch 194 Batch  708/718   train_loss = 2.710\n",
      "Epoch 195 Batch   90/718   train_loss = 2.658\n",
      "Epoch 195 Batch  190/718   train_loss = 2.805\n",
      "Epoch 195 Batch  290/718   train_loss = 2.633\n",
      "Epoch 195 Batch  390/718   train_loss = 2.657\n",
      "Epoch 195 Batch  490/718   train_loss = 2.735\n",
      "Epoch 195 Batch  590/718   train_loss = 2.743\n",
      "Epoch 195 Batch  690/718   train_loss = 2.820\n",
      "Epoch 196 Batch   72/718   train_loss = 2.787\n",
      "Epoch 196 Batch  172/718   train_loss = 2.814\n",
      "Epoch 196 Batch  272/718   train_loss = 2.743\n",
      "Epoch 196 Batch  372/718   train_loss = 2.731\n",
      "Epoch 196 Batch  472/718   train_loss = 2.835\n",
      "Epoch 196 Batch  572/718   train_loss = 2.811\n",
      "Epoch 196 Batch  672/718   train_loss = 2.650\n",
      "Epoch 197 Batch   54/718   train_loss = 2.633\n",
      "Epoch 197 Batch  154/718   train_loss = 2.852\n",
      "Epoch 197 Batch  254/718   train_loss = 2.723\n",
      "Epoch 197 Batch  354/718   train_loss = 2.817\n",
      "Epoch 197 Batch  454/718   train_loss = 2.671\n",
      "Epoch 197 Batch  554/718   train_loss = 2.796\n",
      "Epoch 197 Batch  654/718   train_loss = 2.799\n",
      "Epoch 198 Batch   36/718   train_loss = 2.697\n",
      "Epoch 198 Batch  136/718   train_loss = 2.671\n",
      "Epoch 198 Batch  236/718   train_loss = 2.677\n",
      "Epoch 198 Batch  336/718   train_loss = 2.739\n",
      "Epoch 198 Batch  436/718   train_loss = 2.752\n",
      "Epoch 198 Batch  536/718   train_loss = 2.764\n",
      "Epoch 198 Batch  636/718   train_loss = 2.781\n",
      "Epoch 199 Batch   18/718   train_loss = 2.720\n",
      "Epoch 199 Batch  118/718   train_loss = 2.700\n",
      "Epoch 199 Batch  218/718   train_loss = 2.737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199 Batch  318/718   train_loss = 2.741\n",
      "Epoch 199 Batch  418/718   train_loss = 2.786\n",
      "Epoch 199 Batch  518/718   train_loss = 2.780\n",
      "Epoch 199 Batch  618/718   train_loss = 2.719\n",
      "Epoch 200 Batch    0/718   train_loss = 2.792\n",
      "Epoch 200 Batch  100/718   train_loss = 2.719\n",
      "Epoch 200 Batch  200/718   train_loss = 2.682\n",
      "Epoch 200 Batch  300/718   train_loss = 2.762\n",
      "Epoch 200 Batch  400/718   train_loss = 2.783\n",
      "Epoch 200 Batch  500/718   train_loss = 2.835\n",
      "Epoch 200 Batch  600/718   train_loss = 2.798\n",
      "Epoch 200 Batch  700/718   train_loss = 2.681\n",
      "Epoch 201 Batch   82/718   train_loss = 2.701\n",
      "Epoch 201 Batch  182/718   train_loss = 2.829\n",
      "Epoch 201 Batch  282/718   train_loss = 2.795\n",
      "Epoch 201 Batch  382/718   train_loss = 2.691\n",
      "Epoch 201 Batch  482/718   train_loss = 2.852\n",
      "Epoch 201 Batch  582/718   train_loss = 2.670\n",
      "Epoch 201 Batch  682/718   train_loss = 2.738\n",
      "Epoch 202 Batch   64/718   train_loss = 2.689\n",
      "Epoch 202 Batch  164/718   train_loss = 2.618\n",
      "Epoch 202 Batch  264/718   train_loss = 2.776\n",
      "Epoch 202 Batch  364/718   train_loss = 2.800\n",
      "Epoch 202 Batch  464/718   train_loss = 2.679\n",
      "Epoch 202 Batch  564/718   train_loss = 2.712\n",
      "Epoch 202 Batch  664/718   train_loss = 2.805\n",
      "Epoch 203 Batch   46/718   train_loss = 2.667\n",
      "Epoch 203 Batch  146/718   train_loss = 2.640\n",
      "Epoch 203 Batch  246/718   train_loss = 2.809\n",
      "Epoch 203 Batch  346/718   train_loss = 2.656\n",
      "Epoch 203 Batch  446/718   train_loss = 2.716\n",
      "Epoch 203 Batch  546/718   train_loss = 2.645\n",
      "Epoch 203 Batch  646/718   train_loss = 2.832\n",
      "Epoch 204 Batch   28/718   train_loss = 2.652\n",
      "Epoch 204 Batch  128/718   train_loss = 2.692\n",
      "Epoch 204 Batch  228/718   train_loss = 2.785\n",
      "Epoch 204 Batch  328/718   train_loss = 2.692\n",
      "Epoch 204 Batch  428/718   train_loss = 2.650\n",
      "Epoch 204 Batch  528/718   train_loss = 2.788\n",
      "Epoch 204 Batch  628/718   train_loss = 2.808\n",
      "Epoch 205 Batch   10/718   train_loss = 2.678\n",
      "Epoch 205 Batch  110/718   train_loss = 2.705\n",
      "Epoch 205 Batch  210/718   train_loss = 2.662\n",
      "Epoch 205 Batch  310/718   train_loss = 2.780\n",
      "Epoch 205 Batch  410/718   train_loss = 2.605\n",
      "Epoch 205 Batch  510/718   train_loss = 2.777\n",
      "Epoch 205 Batch  610/718   train_loss = 2.730\n",
      "Epoch 205 Batch  710/718   train_loss = 2.701\n",
      "Epoch 206 Batch   92/718   train_loss = 2.625\n",
      "Epoch 206 Batch  192/718   train_loss = 2.690\n",
      "Epoch 206 Batch  292/718   train_loss = 2.599\n",
      "Epoch 206 Batch  392/718   train_loss = 2.679\n",
      "Epoch 206 Batch  492/718   train_loss = 2.767\n",
      "Epoch 206 Batch  592/718   train_loss = 2.741\n",
      "Epoch 206 Batch  692/718   train_loss = 2.685\n",
      "Epoch 207 Batch   74/718   train_loss = 2.748\n",
      "Epoch 207 Batch  174/718   train_loss = 2.801\n",
      "Epoch 207 Batch  274/718   train_loss = 2.754\n",
      "Epoch 207 Batch  374/718   train_loss = 2.668\n",
      "Epoch 207 Batch  474/718   train_loss = 2.768\n",
      "Epoch 207 Batch  574/718   train_loss = 2.596\n",
      "Epoch 207 Batch  674/718   train_loss = 2.649\n",
      "Epoch 208 Batch   56/718   train_loss = 2.835\n",
      "Epoch 208 Batch  156/718   train_loss = 2.630\n",
      "Epoch 208 Batch  256/718   train_loss = 2.592\n",
      "Epoch 208 Batch  356/718   train_loss = 2.665\n",
      "Epoch 208 Batch  456/718   train_loss = 2.690\n",
      "Epoch 208 Batch  556/718   train_loss = 2.651\n",
      "Epoch 208 Batch  656/718   train_loss = 2.683\n",
      "Epoch 209 Batch   38/718   train_loss = 2.732\n",
      "Epoch 209 Batch  138/718   train_loss = 2.729\n",
      "Epoch 209 Batch  238/718   train_loss = 2.758\n",
      "Epoch 209 Batch  338/718   train_loss = 2.721\n",
      "Epoch 209 Batch  438/718   train_loss = 2.653\n",
      "Epoch 209 Batch  538/718   train_loss = 2.705\n",
      "Epoch 209 Batch  638/718   train_loss = 2.649\n",
      "Epoch 210 Batch   20/718   train_loss = 2.737\n",
      "Epoch 210 Batch  120/718   train_loss = 2.695\n",
      "Epoch 210 Batch  220/718   train_loss = 2.666\n",
      "Epoch 210 Batch  320/718   train_loss = 2.769\n",
      "Epoch 210 Batch  420/718   train_loss = 2.770\n",
      "Epoch 210 Batch  520/718   train_loss = 2.732\n",
      "Epoch 210 Batch  620/718   train_loss = 2.870\n",
      "Epoch 211 Batch    2/718   train_loss = 2.773\n",
      "Epoch 211 Batch  102/718   train_loss = 2.638\n",
      "Epoch 211 Batch  202/718   train_loss = 2.702\n",
      "Epoch 211 Batch  302/718   train_loss = 2.772\n",
      "Epoch 211 Batch  402/718   train_loss = 2.731\n",
      "Epoch 211 Batch  502/718   train_loss = 2.708\n",
      "Epoch 211 Batch  602/718   train_loss = 2.728\n",
      "Epoch 211 Batch  702/718   train_loss = 2.687\n",
      "Epoch 212 Batch   84/718   train_loss = 2.623\n",
      "Epoch 212 Batch  184/718   train_loss = 2.685\n",
      "Epoch 212 Batch  284/718   train_loss = 2.695\n",
      "Epoch 212 Batch  384/718   train_loss = 2.677\n",
      "Epoch 212 Batch  484/718   train_loss = 2.720\n",
      "Epoch 212 Batch  584/718   train_loss = 2.685\n",
      "Epoch 212 Batch  684/718   train_loss = 2.693\n",
      "Epoch 213 Batch   66/718   train_loss = 2.639\n",
      "Epoch 213 Batch  166/718   train_loss = 2.669\n",
      "Epoch 213 Batch  266/718   train_loss = 2.733\n",
      "Epoch 213 Batch  366/718   train_loss = 2.769\n",
      "Epoch 213 Batch  466/718   train_loss = 2.682\n",
      "Epoch 213 Batch  566/718   train_loss = 2.650\n",
      "Epoch 213 Batch  666/718   train_loss = 2.654\n",
      "Epoch 214 Batch   48/718   train_loss = 2.592\n",
      "Epoch 214 Batch  148/718   train_loss = 2.722\n",
      "Epoch 214 Batch  248/718   train_loss = 2.709\n",
      "Epoch 214 Batch  348/718   train_loss = 2.617\n",
      "Epoch 214 Batch  448/718   train_loss = 2.712\n",
      "Epoch 214 Batch  548/718   train_loss = 2.762\n",
      "Epoch 214 Batch  648/718   train_loss = 2.717\n",
      "Epoch 215 Batch   30/718   train_loss = 2.695\n",
      "Epoch 215 Batch  130/718   train_loss = 2.626\n",
      "Epoch 215 Batch  230/718   train_loss = 2.730\n",
      "Epoch 215 Batch  330/718   train_loss = 2.768\n",
      "Epoch 215 Batch  430/718   train_loss = 2.826\n",
      "Epoch 215 Batch  530/718   train_loss = 2.637\n",
      "Epoch 215 Batch  630/718   train_loss = 2.675\n",
      "Epoch 216 Batch   12/718   train_loss = 2.597\n",
      "Epoch 216 Batch  112/718   train_loss = 2.716\n",
      "Epoch 216 Batch  212/718   train_loss = 2.784\n",
      "Epoch 216 Batch  312/718   train_loss = 2.716\n",
      "Epoch 216 Batch  412/718   train_loss = 2.614\n",
      "Epoch 216 Batch  512/718   train_loss = 2.712\n",
      "Epoch 216 Batch  612/718   train_loss = 2.779\n",
      "Epoch 216 Batch  712/718   train_loss = 2.790\n",
      "Epoch 217 Batch   94/718   train_loss = 2.546\n",
      "Epoch 217 Batch  194/718   train_loss = 2.585\n",
      "Epoch 217 Batch  294/718   train_loss = 2.742\n",
      "Epoch 217 Batch  394/718   train_loss = 2.678\n",
      "Epoch 217 Batch  494/718   train_loss = 2.836\n",
      "Epoch 217 Batch  594/718   train_loss = 2.750\n",
      "Epoch 217 Batch  694/718   train_loss = 2.791\n",
      "Epoch 218 Batch   76/718   train_loss = 2.627\n",
      "Epoch 218 Batch  176/718   train_loss = 2.749\n",
      "Epoch 218 Batch  276/718   train_loss = 2.712\n",
      "Epoch 218 Batch  376/718   train_loss = 2.717\n",
      "Epoch 218 Batch  476/718   train_loss = 2.707\n",
      "Epoch 218 Batch  576/718   train_loss = 2.712\n",
      "Epoch 218 Batch  676/718   train_loss = 2.768\n",
      "Epoch 219 Batch   58/718   train_loss = 2.758\n",
      "Epoch 219 Batch  158/718   train_loss = 2.704\n",
      "Epoch 219 Batch  258/718   train_loss = 2.663\n",
      "Epoch 219 Batch  358/718   train_loss = 2.611\n",
      "Epoch 219 Batch  458/718   train_loss = 2.762\n",
      "Epoch 219 Batch  558/718   train_loss = 2.799\n",
      "Epoch 219 Batch  658/718   train_loss = 2.609\n",
      "Epoch 220 Batch   40/718   train_loss = 2.702\n",
      "Epoch 220 Batch  140/718   train_loss = 2.686\n",
      "Epoch 220 Batch  240/718   train_loss = 2.621\n",
      "Epoch 220 Batch  340/718   train_loss = 2.767\n",
      "Epoch 220 Batch  440/718   train_loss = 2.658\n",
      "Epoch 220 Batch  540/718   train_loss = 2.739\n",
      "Epoch 220 Batch  640/718   train_loss = 2.693\n",
      "Epoch 221 Batch   22/718   train_loss = 2.558\n",
      "Epoch 221 Batch  122/718   train_loss = 2.668\n",
      "Epoch 221 Batch  222/718   train_loss = 2.711\n",
      "Epoch 221 Batch  322/718   train_loss = 2.740\n",
      "Epoch 221 Batch  422/718   train_loss = 2.751\n",
      "Epoch 221 Batch  522/718   train_loss = 2.655\n",
      "Epoch 221 Batch  622/718   train_loss = 2.708\n",
      "Epoch 222 Batch    4/718   train_loss = 2.640\n",
      "Epoch 222 Batch  104/718   train_loss = 2.653\n",
      "Epoch 222 Batch  204/718   train_loss = 2.610\n",
      "Epoch 222 Batch  304/718   train_loss = 2.721\n",
      "Epoch 222 Batch  404/718   train_loss = 2.636\n",
      "Epoch 222 Batch  504/718   train_loss = 2.819\n",
      "Epoch 222 Batch  604/718   train_loss = 2.639\n",
      "Epoch 222 Batch  704/718   train_loss = 2.675\n",
      "Epoch 223 Batch   86/718   train_loss = 2.695\n",
      "Epoch 223 Batch  186/718   train_loss = 2.675\n",
      "Epoch 223 Batch  286/718   train_loss = 2.774\n",
      "Epoch 223 Batch  386/718   train_loss = 2.695\n",
      "Epoch 223 Batch  486/718   train_loss = 2.688\n",
      "Epoch 223 Batch  586/718   train_loss = 2.697\n",
      "Epoch 223 Batch  686/718   train_loss = 2.664\n",
      "Epoch 224 Batch   68/718   train_loss = 2.706\n",
      "Epoch 224 Batch  168/718   train_loss = 2.786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 Batch  268/718   train_loss = 2.687\n",
      "Epoch 224 Batch  368/718   train_loss = 2.630\n",
      "Epoch 224 Batch  468/718   train_loss = 2.836\n",
      "Epoch 224 Batch  568/718   train_loss = 2.780\n",
      "Epoch 224 Batch  668/718   train_loss = 2.671\n",
      "Epoch 225 Batch   50/718   train_loss = 2.769\n",
      "Epoch 225 Batch  150/718   train_loss = 2.702\n",
      "Epoch 225 Batch  250/718   train_loss = 2.663\n",
      "Epoch 225 Batch  350/718   train_loss = 2.682\n",
      "Epoch 225 Batch  450/718   train_loss = 2.684\n",
      "Epoch 225 Batch  550/718   train_loss = 2.638\n",
      "Epoch 225 Batch  650/718   train_loss = 2.643\n",
      "Epoch 226 Batch   32/718   train_loss = 2.652\n",
      "Epoch 226 Batch  132/718   train_loss = 2.704\n",
      "Epoch 226 Batch  232/718   train_loss = 2.651\n",
      "Epoch 226 Batch  332/718   train_loss = 2.618\n",
      "Epoch 226 Batch  432/718   train_loss = 2.798\n",
      "Epoch 226 Batch  532/718   train_loss = 2.817\n",
      "Epoch 226 Batch  632/718   train_loss = 2.565\n",
      "Epoch 227 Batch   14/718   train_loss = 2.728\n",
      "Epoch 227 Batch  114/718   train_loss = 2.664\n",
      "Epoch 227 Batch  214/718   train_loss = 2.676\n",
      "Epoch 227 Batch  314/718   train_loss = 2.715\n",
      "Epoch 227 Batch  414/718   train_loss = 2.576\n",
      "Epoch 227 Batch  514/718   train_loss = 2.767\n",
      "Epoch 227 Batch  614/718   train_loss = 2.654\n",
      "Epoch 227 Batch  714/718   train_loss = 2.686\n",
      "Epoch 228 Batch   96/718   train_loss = 2.665\n",
      "Epoch 228 Batch  196/718   train_loss = 2.604\n",
      "Epoch 228 Batch  296/718   train_loss = 2.653\n",
      "Epoch 228 Batch  396/718   train_loss = 2.674\n",
      "Epoch 228 Batch  496/718   train_loss = 2.742\n",
      "Epoch 228 Batch  596/718   train_loss = 2.655\n",
      "Epoch 228 Batch  696/718   train_loss = 2.665\n",
      "Epoch 229 Batch   78/718   train_loss = 2.737\n",
      "Epoch 229 Batch  178/718   train_loss = 2.697\n",
      "Epoch 229 Batch  278/718   train_loss = 2.617\n",
      "Epoch 229 Batch  378/718   train_loss = 2.559\n",
      "Epoch 229 Batch  478/718   train_loss = 2.642\n",
      "Epoch 229 Batch  578/718   train_loss = 2.546\n",
      "Epoch 229 Batch  678/718   train_loss = 2.711\n",
      "Epoch 230 Batch   60/718   train_loss = 2.690\n",
      "Epoch 230 Batch  160/718   train_loss = 2.688\n",
      "Epoch 230 Batch  260/718   train_loss = 2.724\n",
      "Epoch 230 Batch  360/718   train_loss = 2.640\n",
      "Epoch 230 Batch  460/718   train_loss = 2.643\n",
      "Epoch 230 Batch  560/718   train_loss = 2.713\n",
      "Epoch 230 Batch  660/718   train_loss = 2.783\n",
      "Epoch 231 Batch   42/718   train_loss = 2.684\n",
      "Epoch 231 Batch  142/718   train_loss = 2.751\n",
      "Epoch 231 Batch  242/718   train_loss = 2.558\n",
      "Epoch 231 Batch  342/718   train_loss = 2.660\n",
      "Epoch 231 Batch  442/718   train_loss = 2.677\n",
      "Epoch 231 Batch  542/718   train_loss = 2.642\n",
      "Epoch 231 Batch  642/718   train_loss = 2.756\n",
      "Epoch 232 Batch   24/718   train_loss = 2.655\n",
      "Epoch 232 Batch  124/718   train_loss = 2.591\n",
      "Epoch 232 Batch  224/718   train_loss = 2.647\n",
      "Epoch 232 Batch  324/718   train_loss = 2.649\n",
      "Epoch 232 Batch  424/718   train_loss = 2.618\n",
      "Epoch 232 Batch  524/718   train_loss = 2.696\n",
      "Epoch 232 Batch  624/718   train_loss = 2.690\n",
      "Epoch 233 Batch    6/718   train_loss = 2.589\n",
      "Epoch 233 Batch  106/718   train_loss = 2.632\n",
      "Epoch 233 Batch  206/718   train_loss = 2.583\n",
      "Epoch 233 Batch  306/718   train_loss = 2.659\n",
      "Epoch 233 Batch  406/718   train_loss = 2.658\n",
      "Epoch 233 Batch  506/718   train_loss = 2.617\n",
      "Epoch 233 Batch  606/718   train_loss = 2.585\n",
      "Epoch 233 Batch  706/718   train_loss = 2.649\n",
      "Epoch 234 Batch   88/718   train_loss = 2.725\n",
      "Epoch 234 Batch  188/718   train_loss = 2.620\n",
      "Epoch 234 Batch  288/718   train_loss = 2.652\n",
      "Epoch 234 Batch  388/718   train_loss = 2.582\n",
      "Epoch 234 Batch  488/718   train_loss = 2.590\n",
      "Epoch 234 Batch  588/718   train_loss = 2.762\n",
      "Epoch 234 Batch  688/718   train_loss = 2.659\n",
      "Epoch 235 Batch   70/718   train_loss = 2.660\n",
      "Epoch 235 Batch  170/718   train_loss = 2.695\n",
      "Epoch 235 Batch  270/718   train_loss = 2.726\n",
      "Epoch 235 Batch  370/718   train_loss = 2.666\n",
      "Epoch 235 Batch  470/718   train_loss = 2.766\n",
      "Epoch 235 Batch  570/718   train_loss = 2.710\n",
      "Epoch 235 Batch  670/718   train_loss = 2.634\n",
      "Epoch 236 Batch   52/718   train_loss = 2.614\n",
      "Epoch 236 Batch  152/718   train_loss = 2.644\n",
      "Epoch 236 Batch  252/718   train_loss = 2.704\n",
      "Epoch 236 Batch  352/718   train_loss = 2.602\n",
      "Epoch 236 Batch  452/718   train_loss = 2.699\n",
      "Epoch 236 Batch  552/718   train_loss = 2.726\n",
      "Epoch 236 Batch  652/718   train_loss = 2.627\n",
      "Epoch 237 Batch   34/718   train_loss = 2.646\n",
      "Epoch 237 Batch  134/718   train_loss = 2.619\n",
      "Epoch 237 Batch  234/718   train_loss = 2.758\n",
      "Epoch 237 Batch  334/718   train_loss = 2.693\n",
      "Epoch 237 Batch  434/718   train_loss = 2.690\n",
      "Epoch 237 Batch  534/718   train_loss = 2.654\n",
      "Epoch 237 Batch  634/718   train_loss = 2.769\n",
      "Epoch 238 Batch   16/718   train_loss = 2.642\n",
      "Epoch 238 Batch  116/718   train_loss = 2.583\n",
      "Epoch 238 Batch  216/718   train_loss = 2.632\n",
      "Epoch 238 Batch  316/718   train_loss = 2.624\n",
      "Epoch 238 Batch  416/718   train_loss = 2.706\n",
      "Epoch 238 Batch  516/718   train_loss = 2.624\n",
      "Epoch 238 Batch  616/718   train_loss = 2.740\n",
      "Epoch 238 Batch  716/718   train_loss = 2.711\n",
      "Epoch 239 Batch   98/718   train_loss = 2.595\n",
      "Epoch 239 Batch  198/718   train_loss = 2.633\n",
      "Epoch 239 Batch  298/718   train_loss = 2.636\n",
      "Epoch 239 Batch  398/718   train_loss = 2.650\n",
      "Epoch 239 Batch  498/718   train_loss = 2.687\n",
      "Epoch 239 Batch  598/718   train_loss = 2.737\n",
      "Epoch 239 Batch  698/718   train_loss = 2.727\n",
      "Epoch 240 Batch   80/718   train_loss = 2.649\n",
      "Epoch 240 Batch  180/718   train_loss = 2.637\n",
      "Epoch 240 Batch  280/718   train_loss = 2.729\n",
      "Epoch 240 Batch  380/718   train_loss = 2.667\n",
      "Epoch 240 Batch  480/718   train_loss = 2.707\n",
      "Epoch 240 Batch  580/718   train_loss = 2.621\n",
      "Epoch 240 Batch  680/718   train_loss = 2.732\n",
      "Epoch 241 Batch   62/718   train_loss = 2.643\n",
      "Epoch 241 Batch  162/718   train_loss = 2.671\n",
      "Epoch 241 Batch  262/718   train_loss = 2.713\n",
      "Epoch 241 Batch  362/718   train_loss = 2.768\n",
      "Epoch 241 Batch  462/718   train_loss = 2.667\n",
      "Epoch 241 Batch  562/718   train_loss = 2.637\n",
      "Epoch 241 Batch  662/718   train_loss = 2.620\n",
      "Epoch 242 Batch   44/718   train_loss = 2.650\n",
      "Epoch 242 Batch  144/718   train_loss = 2.767\n",
      "Epoch 242 Batch  244/718   train_loss = 2.760\n",
      "Epoch 242 Batch  344/718   train_loss = 2.689\n",
      "Epoch 242 Batch  444/718   train_loss = 2.588\n",
      "Epoch 242 Batch  544/718   train_loss = 2.616\n",
      "Epoch 242 Batch  644/718   train_loss = 2.777\n",
      "Epoch 243 Batch   26/718   train_loss = 2.776\n",
      "Epoch 243 Batch  126/718   train_loss = 2.678\n",
      "Epoch 243 Batch  226/718   train_loss = 2.626\n",
      "Epoch 243 Batch  326/718   train_loss = 2.630\n",
      "Epoch 243 Batch  426/718   train_loss = 2.625\n",
      "Epoch 243 Batch  526/718   train_loss = 2.575\n",
      "Epoch 243 Batch  626/718   train_loss = 2.755\n",
      "Epoch 244 Batch    8/718   train_loss = 2.604\n",
      "Epoch 244 Batch  108/718   train_loss = 2.576\n",
      "Epoch 244 Batch  208/718   train_loss = 2.654\n",
      "Epoch 244 Batch  308/718   train_loss = 2.789\n",
      "Epoch 244 Batch  408/718   train_loss = 2.771\n",
      "Epoch 244 Batch  508/718   train_loss = 2.708\n",
      "Epoch 244 Batch  608/718   train_loss = 2.839\n",
      "Epoch 244 Batch  708/718   train_loss = 2.677\n",
      "Epoch 245 Batch   90/718   train_loss = 2.608\n",
      "Epoch 245 Batch  190/718   train_loss = 2.737\n",
      "Epoch 245 Batch  290/718   train_loss = 2.587\n",
      "Epoch 245 Batch  390/718   train_loss = 2.633\n",
      "Epoch 245 Batch  490/718   train_loss = 2.655\n",
      "Epoch 245 Batch  590/718   train_loss = 2.644\n",
      "Epoch 245 Batch  690/718   train_loss = 2.756\n",
      "Epoch 246 Batch   72/718   train_loss = 2.680\n",
      "Epoch 246 Batch  172/718   train_loss = 2.706\n",
      "Epoch 246 Batch  272/718   train_loss = 2.646\n",
      "Epoch 246 Batch  372/718   train_loss = 2.652\n",
      "Epoch 246 Batch  472/718   train_loss = 2.746\n",
      "Epoch 246 Batch  572/718   train_loss = 2.694\n",
      "Epoch 246 Batch  672/718   train_loss = 2.594\n",
      "Epoch 247 Batch   54/718   train_loss = 2.522\n",
      "Epoch 247 Batch  154/718   train_loss = 2.741\n",
      "Epoch 247 Batch  254/718   train_loss = 2.618\n",
      "Epoch 247 Batch  354/718   train_loss = 2.701\n",
      "Epoch 247 Batch  454/718   train_loss = 2.616\n",
      "Epoch 247 Batch  554/718   train_loss = 2.715\n",
      "Epoch 247 Batch  654/718   train_loss = 2.767\n",
      "Epoch 248 Batch   36/718   train_loss = 2.637\n",
      "Epoch 248 Batch  136/718   train_loss = 2.603\n",
      "Epoch 248 Batch  236/718   train_loss = 2.555\n",
      "Epoch 248 Batch  336/718   train_loss = 2.637\n",
      "Epoch 248 Batch  436/718   train_loss = 2.662\n",
      "Epoch 248 Batch  536/718   train_loss = 2.668\n",
      "Epoch 248 Batch  636/718   train_loss = 2.700\n",
      "Epoch 249 Batch   18/718   train_loss = 2.660\n",
      "Epoch 249 Batch  118/718   train_loss = 2.621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249 Batch  218/718   train_loss = 2.710\n",
      "Epoch 249 Batch  318/718   train_loss = 2.663\n",
      "Epoch 249 Batch  418/718   train_loss = 2.752\n",
      "Epoch 249 Batch  518/718   train_loss = 2.715\n",
      "Epoch 249 Batch  618/718   train_loss = 2.644\n",
      "Epoch 250 Batch    0/718   train_loss = 2.749\n",
      "Epoch 250 Batch  100/718   train_loss = 2.619\n",
      "Epoch 250 Batch  200/718   train_loss = 2.617\n",
      "Epoch 250 Batch  300/718   train_loss = 2.658\n",
      "Epoch 250 Batch  400/718   train_loss = 2.698\n",
      "Epoch 250 Batch  500/718   train_loss = 2.750\n",
      "Epoch 250 Batch  600/718   train_loss = 2.724\n",
      "Epoch 250 Batch  700/718   train_loss = 2.595\n",
      "Epoch 251 Batch   82/718   train_loss = 2.601\n",
      "Epoch 251 Batch  182/718   train_loss = 2.722\n",
      "Epoch 251 Batch  282/718   train_loss = 2.716\n",
      "Epoch 251 Batch  382/718   train_loss = 2.595\n",
      "Epoch 251 Batch  482/718   train_loss = 2.784\n",
      "Epoch 251 Batch  582/718   train_loss = 2.554\n",
      "Epoch 251 Batch  682/718   train_loss = 2.651\n",
      "Epoch 252 Batch   64/718   train_loss = 2.662\n",
      "Epoch 252 Batch  164/718   train_loss = 2.554\n",
      "Epoch 252 Batch  264/718   train_loss = 2.644\n",
      "Epoch 252 Batch  364/718   train_loss = 2.741\n",
      "Epoch 252 Batch  464/718   train_loss = 2.611\n",
      "Epoch 252 Batch  564/718   train_loss = 2.659\n",
      "Epoch 252 Batch  664/718   train_loss = 2.743\n",
      "Epoch 253 Batch   46/718   train_loss = 2.575\n",
      "Epoch 253 Batch  146/718   train_loss = 2.580\n",
      "Epoch 253 Batch  246/718   train_loss = 2.721\n",
      "Epoch 253 Batch  346/718   train_loss = 2.590\n",
      "Epoch 253 Batch  446/718   train_loss = 2.639\n",
      "Epoch 253 Batch  546/718   train_loss = 2.576\n",
      "Epoch 253 Batch  646/718   train_loss = 2.687\n",
      "Epoch 254 Batch   28/718   train_loss = 2.607\n",
      "Epoch 254 Batch  128/718   train_loss = 2.655\n",
      "Epoch 254 Batch  228/718   train_loss = 2.664\n",
      "Epoch 254 Batch  328/718   train_loss = 2.623\n",
      "Epoch 254 Batch  428/718   train_loss = 2.538\n",
      "Epoch 254 Batch  528/718   train_loss = 2.711\n",
      "Epoch 254 Batch  628/718   train_loss = 2.771\n",
      "Epoch 255 Batch   10/718   train_loss = 2.607\n",
      "Epoch 255 Batch  110/718   train_loss = 2.655\n",
      "Epoch 255 Batch  210/718   train_loss = 2.602\n",
      "Epoch 255 Batch  310/718   train_loss = 2.735\n",
      "Epoch 255 Batch  410/718   train_loss = 2.514\n",
      "Epoch 255 Batch  510/718   train_loss = 2.695\n",
      "Epoch 255 Batch  610/718   train_loss = 2.672\n",
      "Epoch 255 Batch  710/718   train_loss = 2.596\n",
      "Epoch 256 Batch   92/718   train_loss = 2.594\n",
      "Epoch 256 Batch  192/718   train_loss = 2.613\n",
      "Epoch 256 Batch  292/718   train_loss = 2.462\n",
      "Epoch 256 Batch  392/718   train_loss = 2.629\n",
      "Epoch 256 Batch  492/718   train_loss = 2.702\n",
      "Epoch 256 Batch  592/718   train_loss = 2.674\n",
      "Epoch 256 Batch  692/718   train_loss = 2.632\n",
      "Epoch 257 Batch   74/718   train_loss = 2.710\n",
      "Epoch 257 Batch  174/718   train_loss = 2.710\n",
      "Epoch 257 Batch  274/718   train_loss = 2.725\n",
      "Epoch 257 Batch  374/718   train_loss = 2.566\n",
      "Epoch 257 Batch  474/718   train_loss = 2.673\n",
      "Epoch 257 Batch  574/718   train_loss = 2.537\n",
      "Epoch 257 Batch  674/718   train_loss = 2.628\n",
      "Epoch 258 Batch   56/718   train_loss = 2.754\n",
      "Epoch 258 Batch  156/718   train_loss = 2.619\n",
      "Epoch 258 Batch  256/718   train_loss = 2.525\n",
      "Epoch 258 Batch  356/718   train_loss = 2.551\n",
      "Epoch 258 Batch  456/718   train_loss = 2.607\n",
      "Epoch 258 Batch  556/718   train_loss = 2.584\n",
      "Epoch 258 Batch  656/718   train_loss = 2.559\n",
      "Epoch 259 Batch   38/718   train_loss = 2.662\n",
      "Epoch 259 Batch  138/718   train_loss = 2.654\n",
      "Epoch 259 Batch  238/718   train_loss = 2.660\n",
      "Epoch 259 Batch  338/718   train_loss = 2.685\n",
      "Epoch 259 Batch  438/718   train_loss = 2.511\n",
      "Epoch 259 Batch  538/718   train_loss = 2.657\n",
      "Epoch 259 Batch  638/718   train_loss = 2.595\n",
      "Epoch 260 Batch   20/718   train_loss = 2.643\n",
      "Epoch 260 Batch  120/718   train_loss = 2.623\n",
      "Epoch 260 Batch  220/718   train_loss = 2.609\n",
      "Epoch 260 Batch  320/718   train_loss = 2.684\n",
      "Epoch 260 Batch  420/718   train_loss = 2.732\n",
      "Epoch 260 Batch  520/718   train_loss = 2.678\n",
      "Epoch 260 Batch  620/718   train_loss = 2.819\n",
      "Epoch 261 Batch    2/718   train_loss = 2.704\n",
      "Epoch 261 Batch  102/718   train_loss = 2.539\n",
      "Epoch 261 Batch  202/718   train_loss = 2.638\n",
      "Epoch 261 Batch  302/718   train_loss = 2.713\n",
      "Epoch 261 Batch  402/718   train_loss = 2.680\n",
      "Epoch 261 Batch  502/718   train_loss = 2.689\n",
      "Epoch 261 Batch  602/718   train_loss = 2.667\n",
      "Epoch 261 Batch  702/718   train_loss = 2.636\n",
      "Epoch 262 Batch   84/718   train_loss = 2.589\n",
      "Epoch 262 Batch  184/718   train_loss = 2.660\n",
      "Epoch 262 Batch  284/718   train_loss = 2.615\n",
      "Epoch 262 Batch  384/718   train_loss = 2.643\n",
      "Epoch 262 Batch  484/718   train_loss = 2.638\n",
      "Epoch 262 Batch  584/718   train_loss = 2.600\n",
      "Epoch 262 Batch  684/718   train_loss = 2.649\n",
      "Epoch 263 Batch   66/718   train_loss = 2.601\n",
      "Epoch 263 Batch  166/718   train_loss = 2.679\n",
      "Epoch 263 Batch  266/718   train_loss = 2.700\n",
      "Epoch 263 Batch  366/718   train_loss = 2.746\n",
      "Epoch 263 Batch  466/718   train_loss = 2.652\n",
      "Epoch 263 Batch  566/718   train_loss = 2.605\n",
      "Epoch 263 Batch  666/718   train_loss = 2.598\n",
      "Epoch 264 Batch   48/718   train_loss = 2.543\n",
      "Epoch 264 Batch  148/718   train_loss = 2.638\n",
      "Epoch 264 Batch  248/718   train_loss = 2.656\n",
      "Epoch 264 Batch  348/718   train_loss = 2.569\n",
      "Epoch 264 Batch  448/718   train_loss = 2.629\n",
      "Epoch 264 Batch  548/718   train_loss = 2.656\n",
      "Epoch 264 Batch  648/718   train_loss = 2.649\n",
      "Epoch 265 Batch   30/718   train_loss = 2.614\n",
      "Epoch 265 Batch  130/718   train_loss = 2.524\n",
      "Epoch 265 Batch  230/718   train_loss = 2.672\n",
      "Epoch 265 Batch  330/718   train_loss = 2.732\n",
      "Epoch 265 Batch  430/718   train_loss = 2.773\n",
      "Epoch 265 Batch  530/718   train_loss = 2.580\n",
      "Epoch 265 Batch  630/718   train_loss = 2.596\n",
      "Epoch 266 Batch   12/718   train_loss = 2.552\n",
      "Epoch 266 Batch  112/718   train_loss = 2.618\n",
      "Epoch 266 Batch  212/718   train_loss = 2.769\n",
      "Epoch 266 Batch  312/718   train_loss = 2.624\n",
      "Epoch 266 Batch  412/718   train_loss = 2.549\n",
      "Epoch 266 Batch  512/718   train_loss = 2.641\n",
      "Epoch 266 Batch  612/718   train_loss = 2.722\n",
      "Epoch 266 Batch  712/718   train_loss = 2.695\n",
      "Epoch 267 Batch   94/718   train_loss = 2.481\n",
      "Epoch 267 Batch  194/718   train_loss = 2.564\n",
      "Epoch 267 Batch  294/718   train_loss = 2.705\n",
      "Epoch 267 Batch  394/718   train_loss = 2.594\n",
      "Epoch 267 Batch  494/718   train_loss = 2.770\n",
      "Epoch 267 Batch  594/718   train_loss = 2.698\n",
      "Epoch 267 Batch  694/718   train_loss = 2.728\n",
      "Epoch 268 Batch   76/718   train_loss = 2.540\n",
      "Epoch 268 Batch  176/718   train_loss = 2.729\n",
      "Epoch 268 Batch  276/718   train_loss = 2.629\n",
      "Epoch 268 Batch  376/718   train_loss = 2.625\n",
      "Epoch 268 Batch  476/718   train_loss = 2.626\n",
      "Epoch 268 Batch  576/718   train_loss = 2.653\n",
      "Epoch 268 Batch  676/718   train_loss = 2.661\n",
      "Epoch 269 Batch   58/718   train_loss = 2.668\n",
      "Epoch 269 Batch  158/718   train_loss = 2.677\n",
      "Epoch 269 Batch  258/718   train_loss = 2.627\n",
      "Epoch 269 Batch  358/718   train_loss = 2.523\n",
      "Epoch 269 Batch  458/718   train_loss = 2.713\n",
      "Epoch 269 Batch  558/718   train_loss = 2.714\n",
      "Epoch 269 Batch  658/718   train_loss = 2.547\n",
      "Epoch 270 Batch   40/718   train_loss = 2.668\n",
      "Epoch 270 Batch  140/718   train_loss = 2.608\n",
      "Epoch 270 Batch  240/718   train_loss = 2.510\n",
      "Epoch 270 Batch  340/718   train_loss = 2.657\n",
      "Epoch 270 Batch  440/718   train_loss = 2.615\n",
      "Epoch 270 Batch  540/718   train_loss = 2.689\n",
      "Epoch 270 Batch  640/718   train_loss = 2.571\n",
      "Epoch 271 Batch   22/718   train_loss = 2.492\n",
      "Epoch 271 Batch  122/718   train_loss = 2.584\n",
      "Epoch 271 Batch  222/718   train_loss = 2.627\n",
      "Epoch 271 Batch  322/718   train_loss = 2.678\n",
      "Epoch 271 Batch  422/718   train_loss = 2.663\n",
      "Epoch 271 Batch  522/718   train_loss = 2.593\n",
      "Epoch 271 Batch  622/718   train_loss = 2.664\n",
      "Epoch 272 Batch    4/718   train_loss = 2.543\n",
      "Epoch 272 Batch  104/718   train_loss = 2.609\n",
      "Epoch 272 Batch  204/718   train_loss = 2.526\n",
      "Epoch 272 Batch  304/718   train_loss = 2.623\n",
      "Epoch 272 Batch  404/718   train_loss = 2.582\n",
      "Epoch 272 Batch  504/718   train_loss = 2.774\n",
      "Epoch 272 Batch  604/718   train_loss = 2.512\n",
      "Epoch 272 Batch  704/718   train_loss = 2.627\n",
      "Epoch 273 Batch   86/718   train_loss = 2.647\n",
      "Epoch 273 Batch  186/718   train_loss = 2.594\n",
      "Epoch 273 Batch  286/718   train_loss = 2.716\n",
      "Epoch 273 Batch  386/718   train_loss = 2.638\n",
      "Epoch 273 Batch  486/718   train_loss = 2.599\n",
      "Epoch 273 Batch  586/718   train_loss = 2.594\n",
      "Epoch 273 Batch  686/718   train_loss = 2.569\n",
      "Epoch 274 Batch   68/718   train_loss = 2.623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 274 Batch  168/718   train_loss = 2.724\n",
      "Epoch 274 Batch  268/718   train_loss = 2.604\n",
      "Epoch 274 Batch  368/718   train_loss = 2.545\n",
      "Epoch 274 Batch  468/718   train_loss = 2.815\n",
      "Epoch 274 Batch  568/718   train_loss = 2.766\n",
      "Epoch 274 Batch  668/718   train_loss = 2.639\n",
      "Epoch 275 Batch   50/718   train_loss = 2.685\n",
      "Epoch 275 Batch  150/718   train_loss = 2.645\n",
      "Epoch 275 Batch  250/718   train_loss = 2.573\n",
      "Epoch 275 Batch  350/718   train_loss = 2.606\n",
      "Epoch 275 Batch  450/718   train_loss = 2.636\n",
      "Epoch 275 Batch  550/718   train_loss = 2.539\n",
      "Epoch 275 Batch  650/718   train_loss = 2.559\n",
      "Epoch 276 Batch   32/718   train_loss = 2.562\n",
      "Epoch 276 Batch  132/718   train_loss = 2.648\n",
      "Epoch 276 Batch  232/718   train_loss = 2.557\n",
      "Epoch 276 Batch  332/718   train_loss = 2.583\n",
      "Epoch 276 Batch  432/718   train_loss = 2.769\n",
      "Epoch 276 Batch  532/718   train_loss = 2.746\n",
      "Epoch 276 Batch  632/718   train_loss = 2.489\n",
      "Epoch 277 Batch   14/718   train_loss = 2.684\n",
      "Epoch 277 Batch  114/718   train_loss = 2.567\n",
      "Epoch 277 Batch  214/718   train_loss = 2.630\n",
      "Epoch 277 Batch  314/718   train_loss = 2.675\n",
      "Epoch 277 Batch  414/718   train_loss = 2.529\n",
      "Epoch 277 Batch  514/718   train_loss = 2.703\n",
      "Epoch 277 Batch  614/718   train_loss = 2.584\n",
      "Epoch 277 Batch  714/718   train_loss = 2.601\n",
      "Epoch 278 Batch   96/718   train_loss = 2.568\n",
      "Epoch 278 Batch  196/718   train_loss = 2.541\n",
      "Epoch 278 Batch  296/718   train_loss = 2.587\n",
      "Epoch 278 Batch  396/718   train_loss = 2.627\n",
      "Epoch 278 Batch  496/718   train_loss = 2.705\n",
      "Epoch 278 Batch  596/718   train_loss = 2.598\n",
      "Epoch 278 Batch  696/718   train_loss = 2.608\n",
      "Epoch 279 Batch   78/718   train_loss = 2.661\n",
      "Epoch 279 Batch  178/718   train_loss = 2.644\n",
      "Epoch 279 Batch  278/718   train_loss = 2.575\n",
      "Epoch 279 Batch  378/718   train_loss = 2.492\n",
      "Epoch 279 Batch  478/718   train_loss = 2.568\n",
      "Epoch 279 Batch  578/718   train_loss = 2.513\n",
      "Epoch 279 Batch  678/718   train_loss = 2.636\n",
      "Epoch 280 Batch   60/718   train_loss = 2.634\n",
      "Epoch 280 Batch  160/718   train_loss = 2.633\n",
      "Epoch 280 Batch  260/718   train_loss = 2.633\n",
      "Epoch 280 Batch  360/718   train_loss = 2.590\n",
      "Epoch 280 Batch  460/718   train_loss = 2.586\n",
      "Epoch 280 Batch  560/718   train_loss = 2.642\n",
      "Epoch 280 Batch  660/718   train_loss = 2.740\n",
      "Epoch 281 Batch   42/718   train_loss = 2.627\n",
      "Epoch 281 Batch  142/718   train_loss = 2.692\n",
      "Epoch 281 Batch  242/718   train_loss = 2.487\n",
      "Epoch 281 Batch  342/718   train_loss = 2.562\n",
      "Epoch 281 Batch  442/718   train_loss = 2.629\n",
      "Epoch 281 Batch  542/718   train_loss = 2.578\n",
      "Epoch 281 Batch  642/718   train_loss = 2.668\n",
      "Epoch 282 Batch   24/718   train_loss = 2.632\n",
      "Epoch 282 Batch  124/718   train_loss = 2.555\n",
      "Epoch 282 Batch  224/718   train_loss = 2.574\n",
      "Epoch 282 Batch  324/718   train_loss = 2.591\n",
      "Epoch 282 Batch  424/718   train_loss = 2.582\n",
      "Epoch 282 Batch  524/718   train_loss = 2.661\n",
      "Epoch 282 Batch  624/718   train_loss = 2.625\n",
      "Epoch 283 Batch    6/718   train_loss = 2.581\n",
      "Epoch 283 Batch  106/718   train_loss = 2.558\n",
      "Epoch 283 Batch  206/718   train_loss = 2.476\n",
      "Epoch 283 Batch  306/718   train_loss = 2.595\n",
      "Epoch 283 Batch  406/718   train_loss = 2.569\n",
      "Epoch 283 Batch  506/718   train_loss = 2.602\n",
      "Epoch 283 Batch  606/718   train_loss = 2.534\n",
      "Epoch 283 Batch  706/718   train_loss = 2.574\n",
      "Epoch 284 Batch   88/718   train_loss = 2.651\n",
      "Epoch 284 Batch  188/718   train_loss = 2.559\n",
      "Epoch 284 Batch  288/718   train_loss = 2.562\n",
      "Epoch 284 Batch  388/718   train_loss = 2.538\n",
      "Epoch 284 Batch  488/718   train_loss = 2.529\n",
      "Epoch 284 Batch  588/718   train_loss = 2.677\n",
      "Epoch 284 Batch  688/718   train_loss = 2.588\n",
      "Epoch 285 Batch   70/718   train_loss = 2.606\n",
      "Epoch 285 Batch  170/718   train_loss = 2.645\n",
      "Epoch 285 Batch  270/718   train_loss = 2.709\n",
      "Epoch 285 Batch  370/718   train_loss = 2.603\n",
      "Epoch 285 Batch  470/718   train_loss = 2.683\n",
      "Epoch 285 Batch  570/718   train_loss = 2.641\n",
      "Epoch 285 Batch  670/718   train_loss = 2.568\n",
      "Epoch 286 Batch   52/718   train_loss = 2.563\n",
      "Epoch 286 Batch  152/718   train_loss = 2.568\n",
      "Epoch 286 Batch  252/718   train_loss = 2.629\n",
      "Epoch 286 Batch  352/718   train_loss = 2.600\n",
      "Epoch 286 Batch  452/718   train_loss = 2.614\n",
      "Epoch 286 Batch  552/718   train_loss = 2.682\n",
      "Epoch 286 Batch  652/718   train_loss = 2.545\n",
      "Epoch 287 Batch   34/718   train_loss = 2.580\n",
      "Epoch 287 Batch  134/718   train_loss = 2.580\n",
      "Epoch 287 Batch  234/718   train_loss = 2.645\n",
      "Epoch 287 Batch  334/718   train_loss = 2.650\n",
      "Epoch 287 Batch  434/718   train_loss = 2.639\n",
      "Epoch 287 Batch  534/718   train_loss = 2.611\n",
      "Epoch 287 Batch  634/718   train_loss = 2.719\n",
      "Epoch 288 Batch   16/718   train_loss = 2.568\n",
      "Epoch 288 Batch  116/718   train_loss = 2.538\n",
      "Epoch 288 Batch  216/718   train_loss = 2.534\n",
      "Epoch 288 Batch  316/718   train_loss = 2.529\n",
      "Epoch 288 Batch  416/718   train_loss = 2.628\n",
      "Epoch 288 Batch  516/718   train_loss = 2.546\n",
      "Epoch 288 Batch  616/718   train_loss = 2.652\n",
      "Epoch 288 Batch  716/718   train_loss = 2.681\n",
      "Epoch 289 Batch   98/718   train_loss = 2.543\n",
      "Epoch 289 Batch  198/718   train_loss = 2.585\n",
      "Epoch 289 Batch  298/718   train_loss = 2.597\n",
      "Epoch 289 Batch  398/718   train_loss = 2.574\n",
      "Epoch 289 Batch  498/718   train_loss = 2.619\n",
      "Epoch 289 Batch  598/718   train_loss = 2.682\n",
      "Epoch 289 Batch  698/718   train_loss = 2.700\n",
      "Epoch 290 Batch   80/718   train_loss = 2.604\n",
      "Epoch 290 Batch  180/718   train_loss = 2.513\n",
      "Epoch 290 Batch  280/718   train_loss = 2.694\n",
      "Epoch 290 Batch  380/718   train_loss = 2.589\n",
      "Epoch 290 Batch  480/718   train_loss = 2.659\n",
      "Epoch 290 Batch  580/718   train_loss = 2.571\n",
      "Epoch 290 Batch  680/718   train_loss = 2.624\n",
      "Epoch 291 Batch   62/718   train_loss = 2.565\n",
      "Epoch 291 Batch  162/718   train_loss = 2.604\n",
      "Epoch 291 Batch  262/718   train_loss = 2.693\n",
      "Epoch 291 Batch  362/718   train_loss = 2.690\n",
      "Epoch 291 Batch  462/718   train_loss = 2.607\n",
      "Epoch 291 Batch  562/718   train_loss = 2.588\n",
      "Epoch 291 Batch  662/718   train_loss = 2.570\n",
      "Epoch 292 Batch   44/718   train_loss = 2.626\n",
      "Epoch 292 Batch  144/718   train_loss = 2.685\n",
      "Epoch 292 Batch  244/718   train_loss = 2.693\n",
      "Epoch 292 Batch  344/718   train_loss = 2.605\n",
      "Epoch 292 Batch  444/718   train_loss = 2.516\n",
      "Epoch 292 Batch  544/718   train_loss = 2.554\n",
      "Epoch 292 Batch  644/718   train_loss = 2.732\n",
      "Epoch 293 Batch   26/718   train_loss = 2.710\n",
      "Epoch 293 Batch  126/718   train_loss = 2.629\n",
      "Epoch 293 Batch  226/718   train_loss = 2.592\n",
      "Epoch 293 Batch  326/718   train_loss = 2.538\n",
      "Epoch 293 Batch  426/718   train_loss = 2.601\n",
      "Epoch 293 Batch  526/718   train_loss = 2.565\n",
      "Epoch 293 Batch  626/718   train_loss = 2.702\n",
      "Epoch 294 Batch    8/718   train_loss = 2.514\n",
      "Epoch 294 Batch  108/718   train_loss = 2.478\n",
      "Epoch 294 Batch  208/718   train_loss = 2.651\n",
      "Epoch 294 Batch  308/718   train_loss = 2.648\n",
      "Epoch 294 Batch  408/718   train_loss = 2.683\n",
      "Epoch 294 Batch  508/718   train_loss = 2.665\n",
      "Epoch 294 Batch  608/718   train_loss = 2.733\n",
      "Epoch 294 Batch  708/718   train_loss = 2.583\n",
      "Epoch 295 Batch   90/718   train_loss = 2.543\n",
      "Epoch 295 Batch  190/718   train_loss = 2.693\n",
      "Epoch 295 Batch  290/718   train_loss = 2.534\n",
      "Epoch 295 Batch  390/718   train_loss = 2.584\n",
      "Epoch 295 Batch  490/718   train_loss = 2.585\n",
      "Epoch 295 Batch  590/718   train_loss = 2.626\n",
      "Epoch 295 Batch  690/718   train_loss = 2.694\n",
      "Epoch 296 Batch   72/718   train_loss = 2.682\n",
      "Epoch 296 Batch  172/718   train_loss = 2.643\n",
      "Epoch 296 Batch  272/718   train_loss = 2.650\n",
      "Epoch 296 Batch  372/718   train_loss = 2.600\n",
      "Epoch 296 Batch  472/718   train_loss = 2.657\n",
      "Epoch 296 Batch  572/718   train_loss = 2.695\n",
      "Epoch 296 Batch  672/718   train_loss = 2.538\n",
      "Epoch 297 Batch   54/718   train_loss = 2.470\n",
      "Epoch 297 Batch  154/718   train_loss = 2.696\n",
      "Epoch 297 Batch  254/718   train_loss = 2.533\n",
      "Epoch 297 Batch  354/718   train_loss = 2.659\n",
      "Epoch 297 Batch  454/718   train_loss = 2.511\n",
      "Epoch 297 Batch  554/718   train_loss = 2.642\n",
      "Epoch 297 Batch  654/718   train_loss = 2.715\n",
      "Epoch 298 Batch   36/718   train_loss = 2.602\n",
      "Epoch 298 Batch  136/718   train_loss = 2.542\n",
      "Epoch 298 Batch  236/718   train_loss = 2.543\n",
      "Epoch 298 Batch  336/718   train_loss = 2.596\n",
      "Epoch 298 Batch  436/718   train_loss = 2.658\n",
      "Epoch 298 Batch  536/718   train_loss = 2.650\n",
      "Epoch 298 Batch  636/718   train_loss = 2.714\n",
      "Epoch 299 Batch   18/718   train_loss = 2.582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Batch  118/718   train_loss = 2.550\n",
      "Epoch 299 Batch  218/718   train_loss = 2.599\n",
      "Epoch 299 Batch  318/718   train_loss = 2.597\n",
      "Epoch 299 Batch  418/718   train_loss = 2.717\n",
      "Epoch 299 Batch  518/718   train_loss = 2.640\n",
      "Epoch 299 Batch  618/718   train_loss = 2.590\n",
      "Epoch 300 Batch    0/718   train_loss = 2.640\n",
      "Epoch 300 Batch  100/718   train_loss = 2.624\n",
      "Epoch 300 Batch  200/718   train_loss = 2.561\n",
      "Epoch 300 Batch  300/718   train_loss = 2.627\n",
      "Epoch 300 Batch  400/718   train_loss = 2.637\n",
      "Epoch 300 Batch  500/718   train_loss = 2.716\n",
      "Epoch 300 Batch  600/718   train_loss = 2.658\n",
      "Epoch 300 Batch  700/718   train_loss = 2.550\n",
      "Epoch 301 Batch   82/718   train_loss = 2.571\n",
      "Epoch 301 Batch  182/718   train_loss = 2.693\n",
      "Epoch 301 Batch  282/718   train_loss = 2.669\n",
      "Epoch 301 Batch  382/718   train_loss = 2.523\n",
      "Epoch 301 Batch  482/718   train_loss = 2.718\n",
      "Epoch 301 Batch  582/718   train_loss = 2.520\n",
      "Epoch 301 Batch  682/718   train_loss = 2.593\n",
      "Epoch 302 Batch   64/718   train_loss = 2.617\n",
      "Epoch 302 Batch  164/718   train_loss = 2.510\n",
      "Epoch 302 Batch  264/718   train_loss = 2.625\n",
      "Epoch 302 Batch  364/718   train_loss = 2.700\n",
      "Epoch 302 Batch  464/718   train_loss = 2.523\n",
      "Epoch 302 Batch  564/718   train_loss = 2.602\n",
      "Epoch 302 Batch  664/718   train_loss = 2.676\n",
      "Epoch 303 Batch   46/718   train_loss = 2.576\n",
      "Epoch 303 Batch  146/718   train_loss = 2.504\n",
      "Epoch 303 Batch  246/718   train_loss = 2.689\n",
      "Epoch 303 Batch  346/718   train_loss = 2.512\n",
      "Epoch 303 Batch  446/718   train_loss = 2.587\n",
      "Epoch 303 Batch  546/718   train_loss = 2.520\n",
      "Epoch 303 Batch  646/718   train_loss = 2.685\n",
      "Epoch 304 Batch   28/718   train_loss = 2.542\n",
      "Epoch 304 Batch  128/718   train_loss = 2.549\n",
      "Epoch 304 Batch  228/718   train_loss = 2.675\n",
      "Epoch 304 Batch  328/718   train_loss = 2.579\n",
      "Epoch 304 Batch  428/718   train_loss = 2.485\n",
      "Epoch 304 Batch  528/718   train_loss = 2.649\n",
      "Epoch 304 Batch  628/718   train_loss = 2.684\n",
      "Epoch 305 Batch   10/718   train_loss = 2.586\n",
      "Epoch 305 Batch  110/718   train_loss = 2.582\n",
      "Epoch 305 Batch  210/718   train_loss = 2.510\n",
      "Epoch 305 Batch  310/718   train_loss = 2.690\n",
      "Epoch 305 Batch  410/718   train_loss = 2.484\n",
      "Epoch 305 Batch  510/718   train_loss = 2.657\n",
      "Epoch 305 Batch  610/718   train_loss = 2.618\n",
      "Epoch 305 Batch  710/718   train_loss = 2.585\n",
      "Epoch 306 Batch   92/718   train_loss = 2.536\n",
      "Epoch 306 Batch  192/718   train_loss = 2.547\n",
      "Epoch 306 Batch  292/718   train_loss = 2.443\n",
      "Epoch 306 Batch  392/718   train_loss = 2.607\n",
      "Epoch 306 Batch  492/718   train_loss = 2.675\n",
      "Epoch 306 Batch  592/718   train_loss = 2.639\n",
      "Epoch 306 Batch  692/718   train_loss = 2.594\n",
      "Epoch 307 Batch   74/718   train_loss = 2.641\n",
      "Epoch 307 Batch  174/718   train_loss = 2.726\n",
      "Epoch 307 Batch  274/718   train_loss = 2.650\n",
      "Epoch 307 Batch  374/718   train_loss = 2.544\n",
      "Epoch 307 Batch  474/718   train_loss = 2.661\n",
      "Epoch 307 Batch  574/718   train_loss = 2.465\n",
      "Epoch 307 Batch  674/718   train_loss = 2.567\n",
      "Epoch 308 Batch   56/718   train_loss = 2.720\n",
      "Epoch 308 Batch  156/718   train_loss = 2.566\n",
      "Epoch 308 Batch  256/718   train_loss = 2.525\n",
      "Epoch 308 Batch  356/718   train_loss = 2.515\n",
      "Epoch 308 Batch  456/718   train_loss = 2.548\n",
      "Epoch 308 Batch  556/718   train_loss = 2.525\n",
      "Epoch 308 Batch  656/718   train_loss = 2.527\n",
      "Epoch 309 Batch   38/718   train_loss = 2.593\n",
      "Epoch 309 Batch  138/718   train_loss = 2.665\n",
      "Epoch 309 Batch  238/718   train_loss = 2.627\n",
      "Epoch 309 Batch  338/718   train_loss = 2.607\n",
      "Epoch 309 Batch  438/718   train_loss = 2.494\n",
      "Epoch 309 Batch  538/718   train_loss = 2.587\n",
      "Epoch 309 Batch  638/718   train_loss = 2.495\n",
      "Epoch 310 Batch   20/718   train_loss = 2.596\n",
      "Epoch 310 Batch  120/718   train_loss = 2.545\n",
      "Epoch 310 Batch  220/718   train_loss = 2.565\n",
      "Epoch 310 Batch  320/718   train_loss = 2.619\n",
      "Epoch 310 Batch  420/718   train_loss = 2.684\n",
      "Epoch 310 Batch  520/718   train_loss = 2.631\n",
      "Epoch 310 Batch  620/718   train_loss = 2.763\n",
      "Epoch 311 Batch    2/718   train_loss = 2.663\n",
      "Epoch 311 Batch  102/718   train_loss = 2.526\n",
      "Epoch 311 Batch  202/718   train_loss = 2.563\n",
      "Epoch 311 Batch  302/718   train_loss = 2.693\n",
      "Epoch 311 Batch  402/718   train_loss = 2.613\n",
      "Epoch 311 Batch  502/718   train_loss = 2.594\n",
      "Epoch 311 Batch  602/718   train_loss = 2.597\n",
      "Epoch 311 Batch  702/718   train_loss = 2.577\n",
      "Epoch 312 Batch   84/718   train_loss = 2.520\n",
      "Epoch 312 Batch  184/718   train_loss = 2.573\n",
      "Epoch 312 Batch  284/718   train_loss = 2.584\n",
      "Epoch 312 Batch  384/718   train_loss = 2.517\n",
      "Epoch 312 Batch  484/718   train_loss = 2.554\n",
      "Epoch 312 Batch  584/718   train_loss = 2.523\n",
      "Epoch 312 Batch  684/718   train_loss = 2.554\n",
      "Epoch 313 Batch   66/718   train_loss = 2.503\n",
      "Epoch 313 Batch  166/718   train_loss = 2.549\n",
      "Epoch 313 Batch  266/718   train_loss = 2.620\n",
      "Epoch 313 Batch  366/718   train_loss = 2.685\n",
      "Epoch 313 Batch  466/718   train_loss = 2.535\n",
      "Epoch 313 Batch  566/718   train_loss = 2.545\n",
      "Epoch 313 Batch  666/718   train_loss = 2.556\n",
      "Epoch 314 Batch   48/718   train_loss = 2.446\n",
      "Epoch 314 Batch  148/718   train_loss = 2.595\n",
      "Epoch 314 Batch  248/718   train_loss = 2.607\n",
      "Epoch 314 Batch  348/718   train_loss = 2.525\n",
      "Epoch 314 Batch  448/718   train_loss = 2.571\n",
      "Epoch 314 Batch  548/718   train_loss = 2.615\n",
      "Epoch 314 Batch  648/718   train_loss = 2.599\n",
      "Epoch 315 Batch   30/718   train_loss = 2.573\n",
      "Epoch 315 Batch  130/718   train_loss = 2.506\n",
      "Epoch 315 Batch  230/718   train_loss = 2.626\n",
      "Epoch 315 Batch  330/718   train_loss = 2.624\n",
      "Epoch 315 Batch  430/718   train_loss = 2.707\n",
      "Epoch 315 Batch  530/718   train_loss = 2.546\n",
      "Epoch 315 Batch  630/718   train_loss = 2.523\n",
      "Epoch 316 Batch   12/718   train_loss = 2.494\n",
      "Epoch 316 Batch  112/718   train_loss = 2.571\n",
      "Epoch 316 Batch  212/718   train_loss = 2.679\n",
      "Epoch 316 Batch  312/718   train_loss = 2.564\n",
      "Epoch 316 Batch  412/718   train_loss = 2.490\n",
      "Epoch 316 Batch  512/718   train_loss = 2.599\n",
      "Epoch 316 Batch  612/718   train_loss = 2.630\n",
      "Epoch 316 Batch  712/718   train_loss = 2.677\n",
      "Epoch 317 Batch   94/718   train_loss = 2.446\n",
      "Epoch 317 Batch  194/718   train_loss = 2.476\n",
      "Epoch 317 Batch  294/718   train_loss = 2.670\n",
      "Epoch 317 Batch  394/718   train_loss = 2.601\n",
      "Epoch 317 Batch  494/718   train_loss = 2.701\n",
      "Epoch 317 Batch  594/718   train_loss = 2.627\n",
      "Epoch 317 Batch  694/718   train_loss = 2.651\n",
      "Epoch 318 Batch   76/718   train_loss = 2.498\n",
      "Epoch 318 Batch  176/718   train_loss = 2.600\n",
      "Epoch 318 Batch  276/718   train_loss = 2.592\n",
      "Epoch 318 Batch  376/718   train_loss = 2.611\n",
      "Epoch 318 Batch  476/718   train_loss = 2.607\n",
      "Epoch 318 Batch  576/718   train_loss = 2.606\n",
      "Epoch 318 Batch  676/718   train_loss = 2.553\n",
      "Epoch 319 Batch   58/718   train_loss = 2.596\n",
      "Epoch 319 Batch  158/718   train_loss = 2.630\n",
      "Epoch 319 Batch  258/718   train_loss = 2.592\n",
      "Epoch 319 Batch  358/718   train_loss = 2.529\n",
      "Epoch 319 Batch  458/718   train_loss = 2.653\n",
      "Epoch 319 Batch  558/718   train_loss = 2.683\n",
      "Epoch 319 Batch  658/718   train_loss = 2.503\n",
      "Epoch 320 Batch   40/718   train_loss = 2.593\n",
      "Epoch 320 Batch  140/718   train_loss = 2.525\n",
      "Epoch 320 Batch  240/718   train_loss = 2.482\n",
      "Epoch 320 Batch  340/718   train_loss = 2.588\n",
      "Epoch 320 Batch  440/718   train_loss = 2.547\n",
      "Epoch 320 Batch  540/718   train_loss = 2.649\n",
      "Epoch 320 Batch  640/718   train_loss = 2.577\n",
      "Epoch 321 Batch   22/718   train_loss = 2.430\n",
      "Epoch 321 Batch  122/718   train_loss = 2.556\n",
      "Epoch 321 Batch  222/718   train_loss = 2.591\n",
      "Epoch 321 Batch  322/718   train_loss = 2.615\n",
      "Epoch 321 Batch  422/718   train_loss = 2.607\n",
      "Epoch 321 Batch  522/718   train_loss = 2.534\n",
      "Epoch 321 Batch  622/718   train_loss = 2.602\n",
      "Epoch 322 Batch    4/718   train_loss = 2.489\n",
      "Epoch 322 Batch  104/718   train_loss = 2.556\n",
      "Epoch 322 Batch  204/718   train_loss = 2.470\n",
      "Epoch 322 Batch  304/718   train_loss = 2.586\n",
      "Epoch 322 Batch  404/718   train_loss = 2.505\n",
      "Epoch 322 Batch  504/718   train_loss = 2.746\n",
      "Epoch 322 Batch  604/718   train_loss = 2.493\n",
      "Epoch 322 Batch  704/718   train_loss = 2.591\n",
      "Epoch 323 Batch   86/718   train_loss = 2.543\n",
      "Epoch 323 Batch  186/718   train_loss = 2.559\n",
      "Epoch 323 Batch  286/718   train_loss = 2.662\n",
      "Epoch 323 Batch  386/718   train_loss = 2.563\n",
      "Epoch 323 Batch  486/718   train_loss = 2.578\n",
      "Epoch 323 Batch  586/718   train_loss = 2.568\n",
      "Epoch 323 Batch  686/718   train_loss = 2.556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 324 Batch   68/718   train_loss = 2.592\n",
      "Epoch 324 Batch  168/718   train_loss = 2.673\n",
      "Epoch 324 Batch  268/718   train_loss = 2.647\n",
      "Epoch 324 Batch  368/718   train_loss = 2.508\n",
      "Epoch 324 Batch  468/718   train_loss = 2.775\n",
      "Epoch 324 Batch  568/718   train_loss = 2.671\n",
      "Epoch 324 Batch  668/718   train_loss = 2.602\n",
      "Epoch 325 Batch   50/718   train_loss = 2.652\n",
      "Epoch 325 Batch  150/718   train_loss = 2.579\n",
      "Epoch 325 Batch  250/718   train_loss = 2.559\n",
      "Epoch 325 Batch  350/718   train_loss = 2.546\n",
      "Epoch 325 Batch  450/718   train_loss = 2.608\n",
      "Epoch 325 Batch  550/718   train_loss = 2.477\n",
      "Epoch 325 Batch  650/718   train_loss = 2.503\n",
      "Epoch 326 Batch   32/718   train_loss = 2.508\n",
      "Epoch 326 Batch  132/718   train_loss = 2.578\n",
      "Epoch 326 Batch  232/718   train_loss = 2.536\n",
      "Epoch 326 Batch  332/718   train_loss = 2.499\n",
      "Epoch 326 Batch  432/718   train_loss = 2.672\n",
      "Epoch 326 Batch  532/718   train_loss = 2.729\n",
      "Epoch 326 Batch  632/718   train_loss = 2.457\n",
      "Epoch 327 Batch   14/718   train_loss = 2.620\n",
      "Epoch 327 Batch  114/718   train_loss = 2.529\n",
      "Epoch 327 Batch  214/718   train_loss = 2.621\n",
      "Epoch 327 Batch  314/718   train_loss = 2.575\n",
      "Epoch 327 Batch  414/718   train_loss = 2.470\n",
      "Epoch 327 Batch  514/718   train_loss = 2.646\n",
      "Epoch 327 Batch  614/718   train_loss = 2.574\n",
      "Epoch 327 Batch  714/718   train_loss = 2.541\n",
      "Epoch 328 Batch   96/718   train_loss = 2.562\n",
      "Epoch 328 Batch  196/718   train_loss = 2.506\n",
      "Epoch 328 Batch  296/718   train_loss = 2.511\n",
      "Epoch 328 Batch  396/718   train_loss = 2.590\n",
      "Epoch 328 Batch  496/718   train_loss = 2.668\n",
      "Epoch 328 Batch  596/718   train_loss = 2.522\n",
      "Epoch 328 Batch  696/718   train_loss = 2.560\n",
      "Epoch 329 Batch   78/718   train_loss = 2.623\n",
      "Epoch 329 Batch  178/718   train_loss = 2.586\n",
      "Epoch 329 Batch  278/718   train_loss = 2.524\n",
      "Epoch 329 Batch  378/718   train_loss = 2.497\n",
      "Epoch 329 Batch  478/718   train_loss = 2.520\n",
      "Epoch 329 Batch  578/718   train_loss = 2.537\n",
      "Epoch 329 Batch  678/718   train_loss = 2.615\n",
      "Epoch 330 Batch   60/718   train_loss = 2.606\n",
      "Epoch 330 Batch  160/718   train_loss = 2.600\n",
      "Epoch 330 Batch  260/718   train_loss = 2.620\n",
      "Epoch 330 Batch  360/718   train_loss = 2.531\n",
      "Epoch 330 Batch  460/718   train_loss = 2.537\n",
      "Epoch 330 Batch  560/718   train_loss = 2.573\n",
      "Epoch 330 Batch  660/718   train_loss = 2.665\n",
      "Epoch 331 Batch   42/718   train_loss = 2.578\n",
      "Epoch 331 Batch  142/718   train_loss = 2.613\n",
      "Epoch 331 Batch  242/718   train_loss = 2.469\n",
      "Epoch 331 Batch  342/718   train_loss = 2.563\n",
      "Epoch 331 Batch  442/718   train_loss = 2.521\n",
      "Epoch 331 Batch  542/718   train_loss = 2.483\n",
      "Epoch 331 Batch  642/718   train_loss = 2.659\n",
      "Epoch 332 Batch   24/718   train_loss = 2.562\n",
      "Epoch 332 Batch  124/718   train_loss = 2.493\n",
      "Epoch 332 Batch  224/718   train_loss = 2.519\n",
      "Epoch 332 Batch  324/718   train_loss = 2.506\n",
      "Epoch 332 Batch  424/718   train_loss = 2.557\n",
      "Epoch 332 Batch  524/718   train_loss = 2.620\n",
      "Epoch 332 Batch  624/718   train_loss = 2.595\n",
      "Epoch 333 Batch    6/718   train_loss = 2.520\n",
      "Epoch 333 Batch  106/718   train_loss = 2.554\n",
      "Epoch 333 Batch  206/718   train_loss = 2.450\n",
      "Epoch 333 Batch  306/718   train_loss = 2.534\n",
      "Epoch 333 Batch  406/718   train_loss = 2.533\n",
      "Epoch 333 Batch  506/718   train_loss = 2.529\n",
      "Epoch 333 Batch  606/718   train_loss = 2.484\n",
      "Epoch 333 Batch  706/718   train_loss = 2.543\n",
      "Epoch 334 Batch   88/718   train_loss = 2.609\n",
      "Epoch 334 Batch  188/718   train_loss = 2.522\n",
      "Epoch 334 Batch  288/718   train_loss = 2.539\n",
      "Epoch 334 Batch  388/718   train_loss = 2.418\n",
      "Epoch 334 Batch  488/718   train_loss = 2.476\n",
      "Epoch 334 Batch  588/718   train_loss = 2.662\n",
      "Epoch 334 Batch  688/718   train_loss = 2.469\n",
      "Epoch 335 Batch   70/718   train_loss = 2.592\n",
      "Epoch 335 Batch  170/718   train_loss = 2.605\n",
      "Epoch 335 Batch  270/718   train_loss = 2.651\n",
      "Epoch 335 Batch  370/718   train_loss = 2.558\n",
      "Epoch 335 Batch  470/718   train_loss = 2.701\n",
      "Epoch 335 Batch  570/718   train_loss = 2.630\n",
      "Epoch 335 Batch  670/718   train_loss = 2.500\n",
      "Epoch 336 Batch   52/718   train_loss = 2.481\n",
      "Epoch 336 Batch  152/718   train_loss = 2.525\n",
      "Epoch 336 Batch  252/718   train_loss = 2.607\n",
      "Epoch 336 Batch  352/718   train_loss = 2.505\n",
      "Epoch 336 Batch  452/718   train_loss = 2.576\n",
      "Epoch 336 Batch  552/718   train_loss = 2.592\n",
      "Epoch 336 Batch  652/718   train_loss = 2.509\n",
      "Epoch 337 Batch   34/718   train_loss = 2.563\n",
      "Epoch 337 Batch  134/718   train_loss = 2.506\n",
      "Epoch 337 Batch  234/718   train_loss = 2.607\n",
      "Epoch 337 Batch  334/718   train_loss = 2.596\n",
      "Epoch 337 Batch  434/718   train_loss = 2.617\n",
      "Epoch 337 Batch  534/718   train_loss = 2.550\n",
      "Epoch 337 Batch  634/718   train_loss = 2.659\n",
      "Epoch 338 Batch   16/718   train_loss = 2.513\n",
      "Epoch 338 Batch  116/718   train_loss = 2.499\n",
      "Epoch 338 Batch  216/718   train_loss = 2.512\n",
      "Epoch 338 Batch  316/718   train_loss = 2.496\n",
      "Epoch 338 Batch  416/718   train_loss = 2.524\n",
      "Epoch 338 Batch  516/718   train_loss = 2.541\n",
      "Epoch 338 Batch  616/718   train_loss = 2.646\n",
      "Epoch 338 Batch  716/718   train_loss = 2.620\n",
      "Epoch 339 Batch   98/718   train_loss = 2.500\n",
      "Epoch 339 Batch  198/718   train_loss = 2.543\n",
      "Epoch 339 Batch  298/718   train_loss = 2.523\n",
      "Epoch 339 Batch  398/718   train_loss = 2.545\n",
      "Epoch 339 Batch  498/718   train_loss = 2.552\n",
      "Epoch 339 Batch  598/718   train_loss = 2.594\n",
      "Epoch 339 Batch  698/718   train_loss = 2.617\n",
      "Epoch 340 Batch   80/718   train_loss = 2.561\n",
      "Epoch 340 Batch  180/718   train_loss = 2.481\n",
      "Epoch 340 Batch  280/718   train_loss = 2.660\n",
      "Epoch 340 Batch  380/718   train_loss = 2.590\n",
      "Epoch 340 Batch  480/718   train_loss = 2.624\n",
      "Epoch 340 Batch  580/718   train_loss = 2.498\n",
      "Epoch 340 Batch  680/718   train_loss = 2.607\n",
      "Epoch 341 Batch   62/718   train_loss = 2.529\n",
      "Epoch 341 Batch  162/718   train_loss = 2.569\n",
      "Epoch 341 Batch  262/718   train_loss = 2.633\n",
      "Epoch 341 Batch  362/718   train_loss = 2.627\n",
      "Epoch 341 Batch  462/718   train_loss = 2.548\n",
      "Epoch 341 Batch  562/718   train_loss = 2.503\n",
      "Epoch 341 Batch  662/718   train_loss = 2.577\n",
      "Epoch 342 Batch   44/718   train_loss = 2.558\n",
      "Epoch 342 Batch  144/718   train_loss = 2.651\n",
      "Epoch 342 Batch  244/718   train_loss = 2.617\n",
      "Epoch 342 Batch  344/718   train_loss = 2.575\n",
      "Epoch 342 Batch  444/718   train_loss = 2.477\n",
      "Epoch 342 Batch  544/718   train_loss = 2.531\n",
      "Epoch 342 Batch  644/718   train_loss = 2.640\n",
      "Epoch 343 Batch   26/718   train_loss = 2.678\n",
      "Epoch 343 Batch  126/718   train_loss = 2.571\n",
      "Epoch 343 Batch  226/718   train_loss = 2.514\n",
      "Epoch 343 Batch  326/718   train_loss = 2.501\n",
      "Epoch 343 Batch  426/718   train_loss = 2.509\n",
      "Epoch 343 Batch  526/718   train_loss = 2.557\n",
      "Epoch 343 Batch  626/718   train_loss = 2.649\n",
      "Epoch 344 Batch    8/718   train_loss = 2.475\n",
      "Epoch 344 Batch  108/718   train_loss = 2.443\n",
      "Epoch 344 Batch  208/718   train_loss = 2.572\n",
      "Epoch 344 Batch  308/718   train_loss = 2.604\n",
      "Epoch 344 Batch  408/718   train_loss = 2.639\n",
      "Epoch 344 Batch  508/718   train_loss = 2.630\n",
      "Epoch 344 Batch  608/718   train_loss = 2.703\n",
      "Epoch 344 Batch  708/718   train_loss = 2.569\n",
      "Epoch 345 Batch   90/718   train_loss = 2.484\n",
      "Epoch 345 Batch  190/718   train_loss = 2.603\n",
      "Epoch 345 Batch  290/718   train_loss = 2.482\n",
      "Epoch 345 Batch  390/718   train_loss = 2.540\n",
      "Epoch 345 Batch  490/718   train_loss = 2.546\n",
      "Epoch 345 Batch  590/718   train_loss = 2.536\n",
      "Epoch 345 Batch  690/718   train_loss = 2.646\n",
      "Epoch 346 Batch   72/718   train_loss = 2.601\n",
      "Epoch 346 Batch  172/718   train_loss = 2.578\n",
      "Epoch 346 Batch  272/718   train_loss = 2.576\n",
      "Epoch 346 Batch  372/718   train_loss = 2.586\n",
      "Epoch 346 Batch  472/718   train_loss = 2.650\n",
      "Epoch 346 Batch  572/718   train_loss = 2.600\n",
      "Epoch 346 Batch  672/718   train_loss = 2.475\n",
      "Epoch 347 Batch   54/718   train_loss = 2.455\n",
      "Epoch 347 Batch  154/718   train_loss = 2.698\n",
      "Epoch 347 Batch  254/718   train_loss = 2.539\n",
      "Epoch 347 Batch  354/718   train_loss = 2.579\n",
      "Epoch 347 Batch  454/718   train_loss = 2.473\n",
      "Epoch 347 Batch  554/718   train_loss = 2.654\n",
      "Epoch 347 Batch  654/718   train_loss = 2.635\n",
      "Epoch 348 Batch   36/718   train_loss = 2.563\n",
      "Epoch 348 Batch  136/718   train_loss = 2.540\n",
      "Epoch 348 Batch  236/718   train_loss = 2.498\n",
      "Epoch 348 Batch  336/718   train_loss = 2.565\n",
      "Epoch 348 Batch  436/718   train_loss = 2.589\n",
      "Epoch 348 Batch  536/718   train_loss = 2.610\n",
      "Epoch 348 Batch  636/718   train_loss = 2.617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349 Batch   18/718   train_loss = 2.559\n",
      "Epoch 349 Batch  118/718   train_loss = 2.526\n",
      "Epoch 349 Batch  218/718   train_loss = 2.626\n",
      "Epoch 349 Batch  318/718   train_loss = 2.530\n",
      "Epoch 349 Batch  418/718   train_loss = 2.673\n",
      "Epoch 349 Batch  518/718   train_loss = 2.582\n",
      "Epoch 349 Batch  618/718   train_loss = 2.565\n",
      "Epoch 350 Batch    0/718   train_loss = 2.637\n",
      "Epoch 350 Batch  100/718   train_loss = 2.567\n",
      "Epoch 350 Batch  200/718   train_loss = 2.495\n",
      "Epoch 350 Batch  300/718   train_loss = 2.544\n",
      "Epoch 350 Batch  400/718   train_loss = 2.579\n",
      "Epoch 350 Batch  500/718   train_loss = 2.681\n",
      "Epoch 350 Batch  600/718   train_loss = 2.615\n",
      "Epoch 350 Batch  700/718   train_loss = 2.492\n",
      "Epoch 351 Batch   82/718   train_loss = 2.489\n",
      "Epoch 351 Batch  182/718   train_loss = 2.643\n",
      "Epoch 351 Batch  282/718   train_loss = 2.649\n",
      "Epoch 351 Batch  382/718   train_loss = 2.481\n",
      "Epoch 351 Batch  482/718   train_loss = 2.707\n",
      "Epoch 351 Batch  582/718   train_loss = 2.485\n",
      "Epoch 351 Batch  682/718   train_loss = 2.535\n",
      "Epoch 352 Batch   64/718   train_loss = 2.562\n",
      "Epoch 352 Batch  164/718   train_loss = 2.454\n",
      "Epoch 352 Batch  264/718   train_loss = 2.568\n",
      "Epoch 352 Batch  364/718   train_loss = 2.674\n",
      "Epoch 352 Batch  464/718   train_loss = 2.517\n",
      "Epoch 352 Batch  564/718   train_loss = 2.587\n",
      "Epoch 352 Batch  664/718   train_loss = 2.677\n",
      "Epoch 353 Batch   46/718   train_loss = 2.509\n",
      "Epoch 353 Batch  146/718   train_loss = 2.491\n",
      "Epoch 353 Batch  246/718   train_loss = 2.636\n",
      "Epoch 353 Batch  346/718   train_loss = 2.478\n",
      "Epoch 353 Batch  446/718   train_loss = 2.522\n",
      "Epoch 353 Batch  546/718   train_loss = 2.478\n",
      "Epoch 353 Batch  646/718   train_loss = 2.665\n",
      "Epoch 354 Batch   28/718   train_loss = 2.489\n",
      "Epoch 354 Batch  128/718   train_loss = 2.543\n",
      "Epoch 354 Batch  228/718   train_loss = 2.592\n",
      "Epoch 354 Batch  328/718   train_loss = 2.514\n",
      "Epoch 354 Batch  428/718   train_loss = 2.464\n",
      "Epoch 354 Batch  528/718   train_loss = 2.622\n",
      "Epoch 354 Batch  628/718   train_loss = 2.681\n",
      "Epoch 355 Batch   10/718   train_loss = 2.515\n",
      "Epoch 355 Batch  110/718   train_loss = 2.530\n",
      "Epoch 355 Batch  210/718   train_loss = 2.524\n",
      "Epoch 355 Batch  310/718   train_loss = 2.640\n",
      "Epoch 355 Batch  410/718   train_loss = 2.448\n",
      "Epoch 355 Batch  510/718   train_loss = 2.572\n",
      "Epoch 355 Batch  610/718   train_loss = 2.559\n",
      "Epoch 355 Batch  710/718   train_loss = 2.552\n",
      "Epoch 356 Batch   92/718   train_loss = 2.514\n",
      "Epoch 356 Batch  192/718   train_loss = 2.544\n",
      "Epoch 356 Batch  292/718   train_loss = 2.419\n",
      "Epoch 356 Batch  392/718   train_loss = 2.492\n",
      "Epoch 356 Batch  492/718   train_loss = 2.613\n",
      "Epoch 356 Batch  592/718   train_loss = 2.612\n",
      "Epoch 356 Batch  692/718   train_loss = 2.553\n",
      "Epoch 357 Batch   74/718   train_loss = 2.637\n",
      "Epoch 357 Batch  174/718   train_loss = 2.657\n",
      "Epoch 357 Batch  274/718   train_loss = 2.607\n",
      "Epoch 357 Batch  374/718   train_loss = 2.497\n",
      "Epoch 357 Batch  474/718   train_loss = 2.637\n",
      "Epoch 357 Batch  574/718   train_loss = 2.424\n",
      "Epoch 357 Batch  674/718   train_loss = 2.520\n",
      "Epoch 358 Batch   56/718   train_loss = 2.678\n",
      "Epoch 358 Batch  156/718   train_loss = 2.513\n",
      "Epoch 358 Batch  256/718   train_loss = 2.441\n",
      "Epoch 358 Batch  356/718   train_loss = 2.499\n",
      "Epoch 358 Batch  456/718   train_loss = 2.514\n",
      "Epoch 358 Batch  556/718   train_loss = 2.495\n",
      "Epoch 358 Batch  656/718   train_loss = 2.500\n",
      "Epoch 359 Batch   38/718   train_loss = 2.584\n",
      "Epoch 359 Batch  138/718   train_loss = 2.603\n",
      "Epoch 359 Batch  238/718   train_loss = 2.597\n",
      "Epoch 359 Batch  338/718   train_loss = 2.576\n",
      "Epoch 359 Batch  438/718   train_loss = 2.461\n",
      "Epoch 359 Batch  538/718   train_loss = 2.523\n",
      "Epoch 359 Batch  638/718   train_loss = 2.500\n",
      "Epoch 360 Batch   20/718   train_loss = 2.592\n",
      "Epoch 360 Batch  120/718   train_loss = 2.489\n",
      "Epoch 360 Batch  220/718   train_loss = 2.524\n",
      "Epoch 360 Batch  320/718   train_loss = 2.593\n",
      "Epoch 360 Batch  420/718   train_loss = 2.658\n",
      "Epoch 360 Batch  520/718   train_loss = 2.612\n",
      "Epoch 360 Batch  620/718   train_loss = 2.709\n",
      "Epoch 361 Batch    2/718   train_loss = 2.585\n",
      "Epoch 361 Batch  102/718   train_loss = 2.451\n",
      "Epoch 361 Batch  202/718   train_loss = 2.496\n",
      "Epoch 361 Batch  302/718   train_loss = 2.648\n",
      "Epoch 361 Batch  402/718   train_loss = 2.588\n",
      "Epoch 361 Batch  502/718   train_loss = 2.581\n",
      "Epoch 361 Batch  602/718   train_loss = 2.563\n",
      "Epoch 361 Batch  702/718   train_loss = 2.512\n",
      "Epoch 362 Batch   84/718   train_loss = 2.441\n",
      "Epoch 362 Batch  184/718   train_loss = 2.532\n",
      "Epoch 362 Batch  284/718   train_loss = 2.519\n",
      "Epoch 362 Batch  384/718   train_loss = 2.547\n",
      "Epoch 362 Batch  484/718   train_loss = 2.552\n",
      "Epoch 362 Batch  584/718   train_loss = 2.483\n",
      "Epoch 362 Batch  684/718   train_loss = 2.570\n",
      "Epoch 363 Batch   66/718   train_loss = 2.497\n",
      "Epoch 363 Batch  166/718   train_loss = 2.536\n",
      "Epoch 363 Batch  266/718   train_loss = 2.598\n",
      "Epoch 363 Batch  366/718   train_loss = 2.632\n",
      "Epoch 363 Batch  466/718   train_loss = 2.550\n",
      "Epoch 363 Batch  566/718   train_loss = 2.475\n",
      "Epoch 363 Batch  666/718   train_loss = 2.478\n",
      "Epoch 364 Batch   48/718   train_loss = 2.439\n",
      "Epoch 364 Batch  148/718   train_loss = 2.585\n",
      "Epoch 364 Batch  248/718   train_loss = 2.574\n",
      "Epoch 364 Batch  348/718   train_loss = 2.498\n",
      "Epoch 364 Batch  448/718   train_loss = 2.555\n",
      "Epoch 364 Batch  548/718   train_loss = 2.608\n",
      "Epoch 364 Batch  648/718   train_loss = 2.559\n",
      "Epoch 365 Batch   30/718   train_loss = 2.538\n",
      "Epoch 365 Batch  130/718   train_loss = 2.499\n",
      "Epoch 365 Batch  230/718   train_loss = 2.626\n",
      "Epoch 365 Batch  330/718   train_loss = 2.616\n",
      "Epoch 365 Batch  430/718   train_loss = 2.673\n",
      "Epoch 365 Batch  530/718   train_loss = 2.503\n",
      "Epoch 365 Batch  630/718   train_loss = 2.480\n",
      "Epoch 366 Batch   12/718   train_loss = 2.445\n",
      "Epoch 366 Batch  112/718   train_loss = 2.546\n",
      "Epoch 366 Batch  212/718   train_loss = 2.593\n",
      "Epoch 366 Batch  312/718   train_loss = 2.563\n",
      "Epoch 366 Batch  412/718   train_loss = 2.467\n",
      "Epoch 366 Batch  512/718   train_loss = 2.530\n",
      "Epoch 366 Batch  612/718   train_loss = 2.646\n",
      "Epoch 366 Batch  712/718   train_loss = 2.627\n",
      "Epoch 367 Batch   94/718   train_loss = 2.435\n",
      "Epoch 367 Batch  194/718   train_loss = 2.450\n",
      "Epoch 367 Batch  294/718   train_loss = 2.579\n",
      "Epoch 367 Batch  394/718   train_loss = 2.519\n",
      "Epoch 367 Batch  494/718   train_loss = 2.646\n",
      "Epoch 367 Batch  594/718   train_loss = 2.578\n",
      "Epoch 367 Batch  694/718   train_loss = 2.660\n",
      "Epoch 368 Batch   76/718   train_loss = 2.495\n",
      "Epoch 368 Batch  176/718   train_loss = 2.593\n",
      "Epoch 368 Batch  276/718   train_loss = 2.558\n",
      "Epoch 368 Batch  376/718   train_loss = 2.581\n",
      "Epoch 368 Batch  476/718   train_loss = 2.575\n",
      "Epoch 368 Batch  576/718   train_loss = 2.579\n",
      "Epoch 368 Batch  676/718   train_loss = 2.561\n",
      "Epoch 369 Batch   58/718   train_loss = 2.528\n",
      "Epoch 369 Batch  158/718   train_loss = 2.584\n",
      "Epoch 369 Batch  258/718   train_loss = 2.522\n",
      "Epoch 369 Batch  358/718   train_loss = 2.441\n",
      "Epoch 369 Batch  458/718   train_loss = 2.615\n",
      "Epoch 369 Batch  558/718   train_loss = 2.623\n",
      "Epoch 369 Batch  658/718   train_loss = 2.428\n",
      "Epoch 370 Batch   40/718   train_loss = 2.561\n",
      "Epoch 370 Batch  140/718   train_loss = 2.501\n",
      "Epoch 370 Batch  240/718   train_loss = 2.428\n",
      "Epoch 370 Batch  340/718   train_loss = 2.544\n",
      "Epoch 370 Batch  440/718   train_loss = 2.522\n",
      "Epoch 370 Batch  540/718   train_loss = 2.556\n",
      "Epoch 370 Batch  640/718   train_loss = 2.499\n",
      "Epoch 371 Batch   22/718   train_loss = 2.405\n",
      "Epoch 371 Batch  122/718   train_loss = 2.524\n",
      "Epoch 371 Batch  222/718   train_loss = 2.552\n",
      "Epoch 371 Batch  322/718   train_loss = 2.596\n",
      "Epoch 371 Batch  422/718   train_loss = 2.548\n",
      "Epoch 371 Batch  522/718   train_loss = 2.515\n",
      "Epoch 371 Batch  622/718   train_loss = 2.563\n",
      "Epoch 372 Batch    4/718   train_loss = 2.443\n",
      "Epoch 372 Batch  104/718   train_loss = 2.544\n",
      "Epoch 372 Batch  204/718   train_loss = 2.415\n",
      "Epoch 372 Batch  304/718   train_loss = 2.580\n",
      "Epoch 372 Batch  404/718   train_loss = 2.497\n",
      "Epoch 372 Batch  504/718   train_loss = 2.671\n",
      "Epoch 372 Batch  604/718   train_loss = 2.440\n",
      "Epoch 372 Batch  704/718   train_loss = 2.548\n",
      "Epoch 373 Batch   86/718   train_loss = 2.554\n",
      "Epoch 373 Batch  186/718   train_loss = 2.501\n",
      "Epoch 373 Batch  286/718   train_loss = 2.619\n",
      "Epoch 373 Batch  386/718   train_loss = 2.478\n",
      "Epoch 373 Batch  486/718   train_loss = 2.532\n",
      "Epoch 373 Batch  586/718   train_loss = 2.536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373 Batch  686/718   train_loss = 2.541\n",
      "Epoch 374 Batch   68/718   train_loss = 2.561\n",
      "Epoch 374 Batch  168/718   train_loss = 2.599\n",
      "Epoch 374 Batch  268/718   train_loss = 2.545\n",
      "Epoch 374 Batch  368/718   train_loss = 2.473\n",
      "Epoch 374 Batch  468/718   train_loss = 2.719\n",
      "Epoch 374 Batch  568/718   train_loss = 2.663\n",
      "Epoch 374 Batch  668/718   train_loss = 2.548\n",
      "Epoch 375 Batch   50/718   train_loss = 2.599\n",
      "Epoch 375 Batch  150/718   train_loss = 2.541\n",
      "Epoch 375 Batch  250/718   train_loss = 2.495\n",
      "Epoch 375 Batch  350/718   train_loss = 2.526\n",
      "Epoch 375 Batch  450/718   train_loss = 2.550\n",
      "Epoch 375 Batch  550/718   train_loss = 2.463\n",
      "Epoch 375 Batch  650/718   train_loss = 2.457\n",
      "Epoch 376 Batch   32/718   train_loss = 2.452\n",
      "Epoch 376 Batch  132/718   train_loss = 2.537\n",
      "Epoch 376 Batch  232/718   train_loss = 2.530\n",
      "Epoch 376 Batch  332/718   train_loss = 2.475\n",
      "Epoch 376 Batch  432/718   train_loss = 2.667\n",
      "Epoch 376 Batch  532/718   train_loss = 2.625\n",
      "Epoch 376 Batch  632/718   train_loss = 2.393\n",
      "Epoch 377 Batch   14/718   train_loss = 2.588\n",
      "Epoch 377 Batch  114/718   train_loss = 2.485\n",
      "Epoch 377 Batch  214/718   train_loss = 2.555\n",
      "Epoch 377 Batch  314/718   train_loss = 2.580\n",
      "Epoch 377 Batch  414/718   train_loss = 2.419\n",
      "Epoch 377 Batch  514/718   train_loss = 2.633\n",
      "Epoch 377 Batch  614/718   train_loss = 2.518\n",
      "Epoch 377 Batch  714/718   train_loss = 2.509\n",
      "Epoch 378 Batch   96/718   train_loss = 2.536\n",
      "Epoch 378 Batch  196/718   train_loss = 2.471\n",
      "Epoch 378 Batch  296/718   train_loss = 2.485\n",
      "Epoch 378 Batch  396/718   train_loss = 2.582\n",
      "Epoch 378 Batch  496/718   train_loss = 2.616\n",
      "Epoch 378 Batch  596/718   train_loss = 2.516\n",
      "Epoch 378 Batch  696/718   train_loss = 2.514\n",
      "Epoch 379 Batch   78/718   train_loss = 2.583\n",
      "Epoch 379 Batch  178/718   train_loss = 2.562\n",
      "Epoch 379 Batch  278/718   train_loss = 2.439\n",
      "Epoch 379 Batch  378/718   train_loss = 2.450\n",
      "Epoch 379 Batch  478/718   train_loss = 2.497\n",
      "Epoch 379 Batch  578/718   train_loss = 2.478\n",
      "Epoch 379 Batch  678/718   train_loss = 2.542\n",
      "Epoch 380 Batch   60/718   train_loss = 2.583\n",
      "Epoch 380 Batch  160/718   train_loss = 2.553\n",
      "Epoch 380 Batch  260/718   train_loss = 2.570\n",
      "Epoch 380 Batch  360/718   train_loss = 2.490\n",
      "Epoch 380 Batch  460/718   train_loss = 2.513\n",
      "Epoch 380 Batch  560/718   train_loss = 2.528\n",
      "Epoch 380 Batch  660/718   train_loss = 2.639\n",
      "Epoch 381 Batch   42/718   train_loss = 2.561\n",
      "Epoch 381 Batch  142/718   train_loss = 2.622\n",
      "Epoch 381 Batch  242/718   train_loss = 2.377\n",
      "Epoch 381 Batch  342/718   train_loss = 2.458\n",
      "Epoch 381 Batch  442/718   train_loss = 2.512\n",
      "Epoch 381 Batch  542/718   train_loss = 2.444\n",
      "Epoch 381 Batch  642/718   train_loss = 2.616\n",
      "Epoch 382 Batch   24/718   train_loss = 2.547\n",
      "Epoch 382 Batch  124/718   train_loss = 2.434\n",
      "Epoch 382 Batch  224/718   train_loss = 2.512\n",
      "Epoch 382 Batch  324/718   train_loss = 2.460\n",
      "Epoch 382 Batch  424/718   train_loss = 2.490\n",
      "Epoch 382 Batch  524/718   train_loss = 2.601\n",
      "Epoch 382 Batch  624/718   train_loss = 2.554\n",
      "Epoch 383 Batch    6/718   train_loss = 2.436\n",
      "Epoch 383 Batch  106/718   train_loss = 2.472\n",
      "Epoch 383 Batch  206/718   train_loss = 2.411\n",
      "Epoch 383 Batch  306/718   train_loss = 2.509\n",
      "Epoch 383 Batch  406/718   train_loss = 2.500\n",
      "Epoch 383 Batch  506/718   train_loss = 2.470\n",
      "Epoch 383 Batch  606/718   train_loss = 2.447\n",
      "Epoch 383 Batch  706/718   train_loss = 2.545\n",
      "Epoch 384 Batch   88/718   train_loss = 2.619\n",
      "Epoch 384 Batch  188/718   train_loss = 2.471\n",
      "Epoch 384 Batch  288/718   train_loss = 2.539\n",
      "Epoch 384 Batch  388/718   train_loss = 2.385\n",
      "Epoch 384 Batch  488/718   train_loss = 2.458\n",
      "Epoch 384 Batch  588/718   train_loss = 2.643\n",
      "Epoch 384 Batch  688/718   train_loss = 2.515\n",
      "Epoch 385 Batch   70/718   train_loss = 2.542\n",
      "Epoch 385 Batch  170/718   train_loss = 2.533\n",
      "Epoch 385 Batch  270/718   train_loss = 2.572\n",
      "Epoch 385 Batch  370/718   train_loss = 2.534\n",
      "Epoch 385 Batch  470/718   train_loss = 2.643\n",
      "Epoch 385 Batch  570/718   train_loss = 2.588\n",
      "Epoch 385 Batch  670/718   train_loss = 2.487\n",
      "Epoch 386 Batch   52/718   train_loss = 2.499\n",
      "Epoch 386 Batch  152/718   train_loss = 2.480\n",
      "Epoch 386 Batch  252/718   train_loss = 2.566\n",
      "Epoch 386 Batch  352/718   train_loss = 2.491\n",
      "Epoch 386 Batch  452/718   train_loss = 2.531\n",
      "Epoch 386 Batch  552/718   train_loss = 2.632\n",
      "Epoch 386 Batch  652/718   train_loss = 2.458\n",
      "Epoch 387 Batch   34/718   train_loss = 2.511\n",
      "Epoch 387 Batch  134/718   train_loss = 2.464\n",
      "Epoch 387 Batch  234/718   train_loss = 2.545\n",
      "Epoch 387 Batch  334/718   train_loss = 2.604\n",
      "Epoch 387 Batch  434/718   train_loss = 2.587\n",
      "Epoch 387 Batch  534/718   train_loss = 2.521\n",
      "Epoch 387 Batch  634/718   train_loss = 2.631\n",
      "Epoch 388 Batch   16/718   train_loss = 2.455\n",
      "Epoch 388 Batch  116/718   train_loss = 2.403\n",
      "Epoch 388 Batch  216/718   train_loss = 2.449\n",
      "Epoch 388 Batch  316/718   train_loss = 2.480\n",
      "Epoch 388 Batch  416/718   train_loss = 2.547\n",
      "Epoch 388 Batch  516/718   train_loss = 2.469\n",
      "Epoch 388 Batch  616/718   train_loss = 2.562\n",
      "Epoch 388 Batch  716/718   train_loss = 2.564\n",
      "Epoch 389 Batch   98/718   train_loss = 2.473\n",
      "Epoch 389 Batch  198/718   train_loss = 2.494\n",
      "Epoch 389 Batch  298/718   train_loss = 2.502\n",
      "Epoch 389 Batch  398/718   train_loss = 2.545\n",
      "Epoch 389 Batch  498/718   train_loss = 2.545\n",
      "Epoch 389 Batch  598/718   train_loss = 2.581\n",
      "Epoch 389 Batch  698/718   train_loss = 2.591\n",
      "Epoch 390 Batch   80/718   train_loss = 2.520\n",
      "Epoch 390 Batch  180/718   train_loss = 2.456\n",
      "Epoch 390 Batch  280/718   train_loss = 2.624\n",
      "Epoch 390 Batch  380/718   train_loss = 2.542\n",
      "Epoch 390 Batch  480/718   train_loss = 2.608\n",
      "Epoch 390 Batch  580/718   train_loss = 2.476\n",
      "Epoch 390 Batch  680/718   train_loss = 2.602\n",
      "Epoch 391 Batch   62/718   train_loss = 2.506\n",
      "Epoch 391 Batch  162/718   train_loss = 2.540\n",
      "Epoch 391 Batch  262/718   train_loss = 2.612\n",
      "Epoch 391 Batch  362/718   train_loss = 2.614\n",
      "Epoch 391 Batch  462/718   train_loss = 2.527\n",
      "Epoch 391 Batch  562/718   train_loss = 2.526\n",
      "Epoch 391 Batch  662/718   train_loss = 2.497\n",
      "Epoch 392 Batch   44/718   train_loss = 2.512\n",
      "Epoch 392 Batch  144/718   train_loss = 2.594\n",
      "Epoch 392 Batch  244/718   train_loss = 2.569\n",
      "Epoch 392 Batch  344/718   train_loss = 2.507\n",
      "Epoch 392 Batch  444/718   train_loss = 2.487\n",
      "Epoch 392 Batch  544/718   train_loss = 2.494\n",
      "Epoch 392 Batch  644/718   train_loss = 2.627\n",
      "Epoch 393 Batch   26/718   train_loss = 2.644\n",
      "Epoch 393 Batch  126/718   train_loss = 2.523\n",
      "Epoch 393 Batch  226/718   train_loss = 2.447\n",
      "Epoch 393 Batch  326/718   train_loss = 2.455\n",
      "Epoch 393 Batch  426/718   train_loss = 2.499\n",
      "Epoch 393 Batch  526/718   train_loss = 2.500\n",
      "Epoch 393 Batch  626/718   train_loss = 2.611\n",
      "Epoch 394 Batch    8/718   train_loss = 2.448\n",
      "Epoch 394 Batch  108/718   train_loss = 2.450\n",
      "Epoch 394 Batch  208/718   train_loss = 2.537\n",
      "Epoch 394 Batch  308/718   train_loss = 2.609\n",
      "Epoch 394 Batch  408/718   train_loss = 2.622\n",
      "Epoch 394 Batch  508/718   train_loss = 2.598\n",
      "Epoch 394 Batch  608/718   train_loss = 2.671\n",
      "Epoch 394 Batch  708/718   train_loss = 2.503\n",
      "Epoch 395 Batch   90/718   train_loss = 2.484\n",
      "Epoch 395 Batch  190/718   train_loss = 2.563\n",
      "Epoch 395 Batch  290/718   train_loss = 2.466\n",
      "Epoch 395 Batch  390/718   train_loss = 2.480\n",
      "Epoch 395 Batch  490/718   train_loss = 2.545\n",
      "Epoch 395 Batch  590/718   train_loss = 2.501\n",
      "Epoch 395 Batch  690/718   train_loss = 2.577\n",
      "Epoch 396 Batch   72/718   train_loss = 2.554\n",
      "Epoch 396 Batch  172/718   train_loss = 2.584\n",
      "Epoch 396 Batch  272/718   train_loss = 2.551\n",
      "Epoch 396 Batch  372/718   train_loss = 2.561\n",
      "Epoch 396 Batch  472/718   train_loss = 2.623\n",
      "Epoch 396 Batch  572/718   train_loss = 2.585\n",
      "Epoch 396 Batch  672/718   train_loss = 2.466\n",
      "Epoch 397 Batch   54/718   train_loss = 2.394\n",
      "Epoch 397 Batch  154/718   train_loss = 2.635\n",
      "Epoch 397 Batch  254/718   train_loss = 2.453\n",
      "Epoch 397 Batch  354/718   train_loss = 2.529\n",
      "Epoch 397 Batch  454/718   train_loss = 2.465\n",
      "Epoch 397 Batch  554/718   train_loss = 2.586\n",
      "Epoch 397 Batch  654/718   train_loss = 2.602\n",
      "Epoch 398 Batch   36/718   train_loss = 2.495\n",
      "Epoch 398 Batch  136/718   train_loss = 2.470\n",
      "Epoch 398 Batch  236/718   train_loss = 2.439\n",
      "Epoch 398 Batch  336/718   train_loss = 2.523\n",
      "Epoch 398 Batch  436/718   train_loss = 2.527\n",
      "Epoch 398 Batch  536/718   train_loss = 2.580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398 Batch  636/718   train_loss = 2.586\n",
      "Epoch 399 Batch   18/718   train_loss = 2.499\n",
      "Epoch 399 Batch  118/718   train_loss = 2.462\n",
      "Epoch 399 Batch  218/718   train_loss = 2.572\n",
      "Epoch 399 Batch  318/718   train_loss = 2.531\n",
      "Epoch 399 Batch  418/718   train_loss = 2.636\n",
      "Epoch 399 Batch  518/718   train_loss = 2.603\n",
      "Epoch 399 Batch  618/718   train_loss = 2.503\n",
      "Epoch 400 Batch    0/718   train_loss = 2.634\n",
      "Epoch 400 Batch  100/718   train_loss = 2.539\n",
      "Epoch 400 Batch  200/718   train_loss = 2.453\n",
      "Epoch 400 Batch  300/718   train_loss = 2.500\n",
      "Epoch 400 Batch  400/718   train_loss = 2.546\n",
      "Epoch 400 Batch  500/718   train_loss = 2.657\n",
      "Epoch 400 Batch  600/718   train_loss = 2.605\n",
      "Epoch 400 Batch  700/718   train_loss = 2.437\n",
      "Epoch 401 Batch   82/718   train_loss = 2.453\n",
      "Epoch 401 Batch  182/718   train_loss = 2.588\n",
      "Epoch 401 Batch  282/718   train_loss = 2.632\n",
      "Epoch 401 Batch  382/718   train_loss = 2.462\n",
      "Epoch 401 Batch  482/718   train_loss = 2.626\n",
      "Epoch 401 Batch  582/718   train_loss = 2.475\n",
      "Epoch 401 Batch  682/718   train_loss = 2.532\n",
      "Epoch 402 Batch   64/718   train_loss = 2.502\n",
      "Epoch 402 Batch  164/718   train_loss = 2.415\n",
      "Epoch 402 Batch  264/718   train_loss = 2.528\n",
      "Epoch 402 Batch  364/718   train_loss = 2.570\n",
      "Epoch 402 Batch  464/718   train_loss = 2.442\n",
      "Epoch 402 Batch  564/718   train_loss = 2.562\n",
      "Epoch 402 Batch  664/718   train_loss = 2.610\n",
      "Epoch 403 Batch   46/718   train_loss = 2.454\n",
      "Epoch 403 Batch  146/718   train_loss = 2.438\n",
      "Epoch 403 Batch  246/718   train_loss = 2.614\n",
      "Epoch 403 Batch  346/718   train_loss = 2.476\n",
      "Epoch 403 Batch  446/718   train_loss = 2.467\n",
      "Epoch 403 Batch  546/718   train_loss = 2.437\n",
      "Epoch 403 Batch  646/718   train_loss = 2.578\n",
      "Epoch 404 Batch   28/718   train_loss = 2.493\n",
      "Epoch 404 Batch  128/718   train_loss = 2.474\n",
      "Epoch 404 Batch  228/718   train_loss = 2.588\n",
      "Epoch 404 Batch  328/718   train_loss = 2.531\n",
      "Epoch 404 Batch  428/718   train_loss = 2.409\n",
      "Epoch 404 Batch  528/718   train_loss = 2.581\n",
      "Epoch 404 Batch  628/718   train_loss = 2.619\n",
      "Epoch 405 Batch   10/718   train_loss = 2.477\n",
      "Epoch 405 Batch  110/718   train_loss = 2.465\n",
      "Epoch 405 Batch  210/718   train_loss = 2.459\n",
      "Epoch 405 Batch  310/718   train_loss = 2.619\n",
      "Epoch 405 Batch  410/718   train_loss = 2.400\n",
      "Epoch 405 Batch  510/718   train_loss = 2.549\n",
      "Epoch 405 Batch  610/718   train_loss = 2.551\n",
      "Epoch 405 Batch  710/718   train_loss = 2.468\n",
      "Epoch 406 Batch   92/718   train_loss = 2.487\n",
      "Epoch 406 Batch  192/718   train_loss = 2.477\n",
      "Epoch 406 Batch  292/718   train_loss = 2.415\n",
      "Epoch 406 Batch  392/718   train_loss = 2.484\n",
      "Epoch 406 Batch  492/718   train_loss = 2.626\n",
      "Epoch 406 Batch  592/718   train_loss = 2.559\n",
      "Epoch 406 Batch  692/718   train_loss = 2.524\n",
      "Epoch 407 Batch   74/718   train_loss = 2.604\n",
      "Epoch 407 Batch  174/718   train_loss = 2.642\n",
      "Epoch 407 Batch  274/718   train_loss = 2.566\n",
      "Epoch 407 Batch  374/718   train_loss = 2.461\n",
      "Epoch 407 Batch  474/718   train_loss = 2.597\n",
      "Epoch 407 Batch  574/718   train_loss = 2.414\n",
      "Epoch 407 Batch  674/718   train_loss = 2.469\n",
      "Epoch 408 Batch   56/718   train_loss = 2.637\n",
      "Epoch 408 Batch  156/718   train_loss = 2.469\n",
      "Epoch 408 Batch  256/718   train_loss = 2.423\n",
      "Epoch 408 Batch  356/718   train_loss = 2.478\n",
      "Epoch 408 Batch  456/718   train_loss = 2.476\n",
      "Epoch 408 Batch  556/718   train_loss = 2.432\n",
      "Epoch 408 Batch  656/718   train_loss = 2.468\n",
      "Epoch 409 Batch   38/718   train_loss = 2.552\n",
      "Epoch 409 Batch  138/718   train_loss = 2.558\n",
      "Epoch 409 Batch  238/718   train_loss = 2.552\n",
      "Epoch 409 Batch  338/718   train_loss = 2.528\n",
      "Epoch 409 Batch  438/718   train_loss = 2.416\n",
      "Epoch 409 Batch  538/718   train_loss = 2.521\n",
      "Epoch 409 Batch  638/718   train_loss = 2.451\n",
      "Epoch 410 Batch   20/718   train_loss = 2.534\n",
      "Epoch 410 Batch  120/718   train_loss = 2.530\n",
      "Epoch 410 Batch  220/718   train_loss = 2.477\n",
      "Epoch 410 Batch  320/718   train_loss = 2.600\n",
      "Epoch 410 Batch  420/718   train_loss = 2.612\n",
      "Epoch 410 Batch  520/718   train_loss = 2.559\n",
      "Epoch 410 Batch  620/718   train_loss = 2.673\n",
      "Epoch 411 Batch    2/718   train_loss = 2.553\n",
      "Epoch 411 Batch  102/718   train_loss = 2.437\n",
      "Epoch 411 Batch  202/718   train_loss = 2.505\n",
      "Epoch 411 Batch  302/718   train_loss = 2.614\n",
      "Epoch 411 Batch  402/718   train_loss = 2.564\n",
      "Epoch 411 Batch  502/718   train_loss = 2.510\n",
      "Epoch 411 Batch  602/718   train_loss = 2.506\n",
      "Epoch 411 Batch  702/718   train_loss = 2.447\n",
      "Epoch 412 Batch   84/718   train_loss = 2.432\n",
      "Epoch 412 Batch  184/718   train_loss = 2.516\n",
      "Epoch 412 Batch  284/718   train_loss = 2.484\n",
      "Epoch 412 Batch  384/718   train_loss = 2.507\n",
      "Epoch 412 Batch  484/718   train_loss = 2.489\n",
      "Epoch 412 Batch  584/718   train_loss = 2.451\n",
      "Epoch 412 Batch  684/718   train_loss = 2.508\n",
      "Epoch 413 Batch   66/718   train_loss = 2.444\n",
      "Epoch 413 Batch  166/718   train_loss = 2.511\n",
      "Epoch 413 Batch  266/718   train_loss = 2.537\n",
      "Epoch 413 Batch  366/718   train_loss = 2.621\n",
      "Epoch 413 Batch  466/718   train_loss = 2.499\n",
      "Epoch 413 Batch  566/718   train_loss = 2.504\n",
      "Epoch 413 Batch  666/718   train_loss = 2.464\n",
      "Epoch 414 Batch   48/718   train_loss = 2.416\n",
      "Epoch 414 Batch  148/718   train_loss = 2.514\n",
      "Epoch 414 Batch  248/718   train_loss = 2.515\n",
      "Epoch 414 Batch  348/718   train_loss = 2.440\n",
      "Epoch 414 Batch  448/718   train_loss = 2.515\n",
      "Epoch 414 Batch  548/718   train_loss = 2.584\n",
      "Epoch 414 Batch  648/718   train_loss = 2.501\n",
      "Epoch 415 Batch   30/718   train_loss = 2.503\n",
      "Epoch 415 Batch  130/718   train_loss = 2.434\n",
      "Epoch 415 Batch  230/718   train_loss = 2.578\n",
      "Epoch 415 Batch  330/718   train_loss = 2.612\n",
      "Epoch 415 Batch  430/718   train_loss = 2.604\n",
      "Epoch 415 Batch  530/718   train_loss = 2.454\n",
      "Epoch 415 Batch  630/718   train_loss = 2.429\n",
      "Epoch 416 Batch   12/718   train_loss = 2.411\n",
      "Epoch 416 Batch  112/718   train_loss = 2.553\n",
      "Epoch 416 Batch  212/718   train_loss = 2.609\n",
      "Epoch 416 Batch  312/718   train_loss = 2.530\n",
      "Epoch 416 Batch  412/718   train_loss = 2.397\n",
      "Epoch 416 Batch  512/718   train_loss = 2.523\n",
      "Epoch 416 Batch  612/718   train_loss = 2.574\n",
      "Epoch 416 Batch  712/718   train_loss = 2.593\n",
      "Epoch 417 Batch   94/718   train_loss = 2.387\n",
      "Epoch 417 Batch  194/718   train_loss = 2.423\n",
      "Epoch 417 Batch  294/718   train_loss = 2.583\n",
      "Epoch 417 Batch  394/718   train_loss = 2.515\n",
      "Epoch 417 Batch  494/718   train_loss = 2.615\n",
      "Epoch 417 Batch  594/718   train_loss = 2.560\n",
      "Epoch 417 Batch  694/718   train_loss = 2.561\n",
      "Epoch 418 Batch   76/718   train_loss = 2.485\n",
      "Epoch 418 Batch  176/718   train_loss = 2.563\n",
      "Epoch 418 Batch  276/718   train_loss = 2.504\n",
      "Epoch 418 Batch  376/718   train_loss = 2.517\n",
      "Epoch 418 Batch  476/718   train_loss = 2.525\n",
      "Epoch 418 Batch  576/718   train_loss = 2.568\n",
      "Epoch 418 Batch  676/718   train_loss = 2.530\n",
      "Epoch 419 Batch   58/718   train_loss = 2.550\n",
      "Epoch 419 Batch  158/718   train_loss = 2.559\n",
      "Epoch 419 Batch  258/718   train_loss = 2.521\n",
      "Epoch 419 Batch  358/718   train_loss = 2.410\n",
      "Epoch 419 Batch  458/718   train_loss = 2.554\n",
      "Epoch 419 Batch  558/718   train_loss = 2.586\n",
      "Epoch 419 Batch  658/718   train_loss = 2.435\n",
      "Epoch 420 Batch   40/718   train_loss = 2.517\n",
      "Epoch 420 Batch  140/718   train_loss = 2.481\n",
      "Epoch 420 Batch  240/718   train_loss = 2.413\n",
      "Epoch 420 Batch  340/718   train_loss = 2.563\n",
      "Epoch 420 Batch  440/718   train_loss = 2.514\n",
      "Epoch 420 Batch  540/718   train_loss = 2.551\n",
      "Epoch 420 Batch  640/718   train_loss = 2.480\n",
      "Epoch 421 Batch   22/718   train_loss = 2.322\n",
      "Epoch 421 Batch  122/718   train_loss = 2.512\n",
      "Epoch 421 Batch  222/718   train_loss = 2.498\n",
      "Epoch 421 Batch  322/718   train_loss = 2.560\n",
      "Epoch 421 Batch  422/718   train_loss = 2.515\n",
      "Epoch 421 Batch  522/718   train_loss = 2.462\n",
      "Epoch 421 Batch  622/718   train_loss = 2.550\n",
      "Epoch 422 Batch    4/718   train_loss = 2.415\n",
      "Epoch 422 Batch  104/718   train_loss = 2.531\n",
      "Epoch 422 Batch  204/718   train_loss = 2.398\n",
      "Epoch 422 Batch  304/718   train_loss = 2.493\n",
      "Epoch 422 Batch  404/718   train_loss = 2.443\n",
      "Epoch 422 Batch  504/718   train_loss = 2.639\n",
      "Epoch 422 Batch  604/718   train_loss = 2.383\n",
      "Epoch 422 Batch  704/718   train_loss = 2.487\n",
      "Epoch 423 Batch   86/718   train_loss = 2.515\n",
      "Epoch 423 Batch  186/718   train_loss = 2.489\n",
      "Epoch 423 Batch  286/718   train_loss = 2.577\n",
      "Epoch 423 Batch  386/718   train_loss = 2.498\n",
      "Epoch 423 Batch  486/718   train_loss = 2.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423 Batch  586/718   train_loss = 2.452\n",
      "Epoch 423 Batch  686/718   train_loss = 2.483\n",
      "Epoch 424 Batch   68/718   train_loss = 2.502\n",
      "Epoch 424 Batch  168/718   train_loss = 2.594\n",
      "Epoch 424 Batch  268/718   train_loss = 2.491\n",
      "Epoch 424 Batch  368/718   train_loss = 2.468\n",
      "Epoch 424 Batch  468/718   train_loss = 2.681\n",
      "Epoch 424 Batch  568/718   train_loss = 2.608\n",
      "Epoch 424 Batch  668/718   train_loss = 2.484\n",
      "Epoch 425 Batch   50/718   train_loss = 2.569\n",
      "Epoch 425 Batch  150/718   train_loss = 2.509\n",
      "Epoch 425 Batch  250/718   train_loss = 2.430\n",
      "Epoch 425 Batch  350/718   train_loss = 2.478\n",
      "Epoch 425 Batch  450/718   train_loss = 2.526\n",
      "Epoch 425 Batch  550/718   train_loss = 2.428\n",
      "Epoch 425 Batch  650/718   train_loss = 2.483\n",
      "Epoch 426 Batch   32/718   train_loss = 2.428\n",
      "Epoch 426 Batch  132/718   train_loss = 2.524\n",
      "Epoch 426 Batch  232/718   train_loss = 2.501\n",
      "Epoch 426 Batch  332/718   train_loss = 2.438\n",
      "Epoch 426 Batch  432/718   train_loss = 2.631\n",
      "Epoch 426 Batch  532/718   train_loss = 2.582\n",
      "Epoch 426 Batch  632/718   train_loss = 2.431\n",
      "Epoch 427 Batch   14/718   train_loss = 2.534\n",
      "Epoch 427 Batch  114/718   train_loss = 2.434\n",
      "Epoch 427 Batch  214/718   train_loss = 2.517\n",
      "Epoch 427 Batch  314/718   train_loss = 2.535\n",
      "Epoch 427 Batch  414/718   train_loss = 2.414\n",
      "Epoch 427 Batch  514/718   train_loss = 2.611\n",
      "Epoch 427 Batch  614/718   train_loss = 2.489\n",
      "Epoch 427 Batch  714/718   train_loss = 2.477\n",
      "Epoch 428 Batch   96/718   train_loss = 2.491\n",
      "Epoch 428 Batch  196/718   train_loss = 2.460\n",
      "Epoch 428 Batch  296/718   train_loss = 2.457\n",
      "Epoch 428 Batch  396/718   train_loss = 2.533\n",
      "Epoch 428 Batch  496/718   train_loss = 2.624\n",
      "Epoch 428 Batch  596/718   train_loss = 2.484\n",
      "Epoch 428 Batch  696/718   train_loss = 2.487\n",
      "Epoch 429 Batch   78/718   train_loss = 2.596\n",
      "Epoch 429 Batch  178/718   train_loss = 2.539\n",
      "Epoch 429 Batch  278/718   train_loss = 2.441\n",
      "Epoch 429 Batch  378/718   train_loss = 2.401\n",
      "Epoch 429 Batch  478/718   train_loss = 2.455\n",
      "Epoch 429 Batch  578/718   train_loss = 2.397\n",
      "Epoch 429 Batch  678/718   train_loss = 2.480\n",
      "Epoch 430 Batch   60/718   train_loss = 2.555\n",
      "Epoch 430 Batch  160/718   train_loss = 2.524\n",
      "Epoch 430 Batch  260/718   train_loss = 2.513\n",
      "Epoch 430 Batch  360/718   train_loss = 2.509\n",
      "Epoch 430 Batch  460/718   train_loss = 2.427\n",
      "Epoch 430 Batch  560/718   train_loss = 2.544\n",
      "Epoch 430 Batch  660/718   train_loss = 2.603\n",
      "Epoch 431 Batch   42/718   train_loss = 2.505\n",
      "Epoch 431 Batch  142/718   train_loss = 2.543\n",
      "Epoch 431 Batch  242/718   train_loss = 2.411\n",
      "Epoch 431 Batch  342/718   train_loss = 2.480\n",
      "Epoch 431 Batch  442/718   train_loss = 2.463\n",
      "Epoch 431 Batch  542/718   train_loss = 2.416\n",
      "Epoch 431 Batch  642/718   train_loss = 2.571\n",
      "Epoch 432 Batch   24/718   train_loss = 2.497\n",
      "Epoch 432 Batch  124/718   train_loss = 2.376\n",
      "Epoch 432 Batch  224/718   train_loss = 2.452\n",
      "Epoch 432 Batch  324/718   train_loss = 2.467\n",
      "Epoch 432 Batch  424/718   train_loss = 2.497\n",
      "Epoch 432 Batch  524/718   train_loss = 2.522\n",
      "Epoch 432 Batch  624/718   train_loss = 2.544\n",
      "Epoch 433 Batch    6/718   train_loss = 2.439\n",
      "Epoch 433 Batch  106/718   train_loss = 2.477\n",
      "Epoch 433 Batch  206/718   train_loss = 2.383\n",
      "Epoch 433 Batch  306/718   train_loss = 2.474\n",
      "Epoch 433 Batch  406/718   train_loss = 2.453\n",
      "Epoch 433 Batch  506/718   train_loss = 2.440\n",
      "Epoch 433 Batch  606/718   train_loss = 2.434\n",
      "Epoch 433 Batch  706/718   train_loss = 2.496\n",
      "Epoch 434 Batch   88/718   train_loss = 2.570\n",
      "Epoch 434 Batch  188/718   train_loss = 2.445\n",
      "Epoch 434 Batch  288/718   train_loss = 2.477\n",
      "Epoch 434 Batch  388/718   train_loss = 2.393\n",
      "Epoch 434 Batch  488/718   train_loss = 2.403\n",
      "Epoch 434 Batch  588/718   train_loss = 2.583\n",
      "Epoch 434 Batch  688/718   train_loss = 2.457\n",
      "Epoch 435 Batch   70/718   train_loss = 2.511\n",
      "Epoch 435 Batch  170/718   train_loss = 2.512\n",
      "Epoch 435 Batch  270/718   train_loss = 2.585\n",
      "Epoch 435 Batch  370/718   train_loss = 2.501\n",
      "Epoch 435 Batch  470/718   train_loss = 2.608\n",
      "Epoch 435 Batch  570/718   train_loss = 2.562\n",
      "Epoch 435 Batch  670/718   train_loss = 2.486\n",
      "Epoch 436 Batch   52/718   train_loss = 2.477\n",
      "Epoch 436 Batch  152/718   train_loss = 2.454\n",
      "Epoch 436 Batch  252/718   train_loss = 2.521\n",
      "Epoch 436 Batch  352/718   train_loss = 2.439\n",
      "Epoch 436 Batch  452/718   train_loss = 2.488\n",
      "Epoch 436 Batch  552/718   train_loss = 2.558\n",
      "Epoch 436 Batch  652/718   train_loss = 2.436\n",
      "Epoch 437 Batch   34/718   train_loss = 2.466\n",
      "Epoch 437 Batch  134/718   train_loss = 2.486\n",
      "Epoch 437 Batch  234/718   train_loss = 2.523\n",
      "Epoch 437 Batch  334/718   train_loss = 2.554\n",
      "Epoch 437 Batch  434/718   train_loss = 2.533\n",
      "Epoch 437 Batch  534/718   train_loss = 2.486\n",
      "Epoch 437 Batch  634/718   train_loss = 2.589\n",
      "Epoch 438 Batch   16/718   train_loss = 2.446\n",
      "Epoch 438 Batch  116/718   train_loss = 2.441\n",
      "Epoch 438 Batch  216/718   train_loss = 2.458\n",
      "Epoch 438 Batch  316/718   train_loss = 2.445\n",
      "Epoch 438 Batch  416/718   train_loss = 2.449\n",
      "Epoch 438 Batch  516/718   train_loss = 2.444\n",
      "Epoch 438 Batch  616/718   train_loss = 2.565\n",
      "Epoch 438 Batch  716/718   train_loss = 2.566\n",
      "Epoch 439 Batch   98/718   train_loss = 2.423\n",
      "Epoch 439 Batch  198/718   train_loss = 2.489\n",
      "Epoch 439 Batch  298/718   train_loss = 2.460\n",
      "Epoch 439 Batch  398/718   train_loss = 2.478\n",
      "Epoch 439 Batch  498/718   train_loss = 2.520\n",
      "Epoch 439 Batch  598/718   train_loss = 2.555\n",
      "Epoch 439 Batch  698/718   train_loss = 2.527\n",
      "Epoch 440 Batch   80/718   train_loss = 2.490\n",
      "Epoch 440 Batch  180/718   train_loss = 2.443\n",
      "Epoch 440 Batch  280/718   train_loss = 2.546\n",
      "Epoch 440 Batch  380/718   train_loss = 2.489\n",
      "Epoch 440 Batch  480/718   train_loss = 2.581\n",
      "Epoch 440 Batch  580/718   train_loss = 2.422\n",
      "Epoch 440 Batch  680/718   train_loss = 2.530\n",
      "Epoch 441 Batch   62/718   train_loss = 2.485\n",
      "Epoch 441 Batch  162/718   train_loss = 2.486\n",
      "Epoch 441 Batch  262/718   train_loss = 2.572\n",
      "Epoch 441 Batch  362/718   train_loss = 2.579\n",
      "Epoch 441 Batch  462/718   train_loss = 2.541\n",
      "Epoch 441 Batch  562/718   train_loss = 2.471\n",
      "Epoch 441 Batch  662/718   train_loss = 2.480\n",
      "Epoch 442 Batch   44/718   train_loss = 2.482\n",
      "Epoch 442 Batch  144/718   train_loss = 2.599\n",
      "Epoch 442 Batch  244/718   train_loss = 2.582\n",
      "Epoch 442 Batch  344/718   train_loss = 2.504\n",
      "Epoch 442 Batch  444/718   train_loss = 2.423\n",
      "Epoch 442 Batch  544/718   train_loss = 2.470\n",
      "Epoch 442 Batch  644/718   train_loss = 2.598\n",
      "Epoch 443 Batch   26/718   train_loss = 2.612\n",
      "Epoch 443 Batch  126/718   train_loss = 2.488\n",
      "Epoch 443 Batch  226/718   train_loss = 2.435\n",
      "Epoch 443 Batch  326/718   train_loss = 2.458\n",
      "Epoch 443 Batch  426/718   train_loss = 2.450\n",
      "Epoch 443 Batch  526/718   train_loss = 2.482\n",
      "Epoch 443 Batch  626/718   train_loss = 2.588\n",
      "Epoch 444 Batch    8/718   train_loss = 2.403\n",
      "Epoch 444 Batch  108/718   train_loss = 2.392\n",
      "Epoch 444 Batch  208/718   train_loss = 2.550\n",
      "Epoch 444 Batch  308/718   train_loss = 2.570\n",
      "Epoch 444 Batch  408/718   train_loss = 2.612\n",
      "Epoch 444 Batch  508/718   train_loss = 2.576\n",
      "Epoch 444 Batch  608/718   train_loss = 2.644\n",
      "Epoch 444 Batch  708/718   train_loss = 2.490\n",
      "Epoch 445 Batch   90/718   train_loss = 2.437\n",
      "Epoch 445 Batch  190/718   train_loss = 2.567\n",
      "Epoch 445 Batch  290/718   train_loss = 2.413\n",
      "Epoch 445 Batch  390/718   train_loss = 2.438\n",
      "Epoch 445 Batch  490/718   train_loss = 2.526\n",
      "Epoch 445 Batch  590/718   train_loss = 2.467\n",
      "Epoch 445 Batch  690/718   train_loss = 2.562\n",
      "Epoch 446 Batch   72/718   train_loss = 2.513\n",
      "Epoch 446 Batch  172/718   train_loss = 2.565\n",
      "Epoch 446 Batch  272/718   train_loss = 2.494\n",
      "Epoch 446 Batch  372/718   train_loss = 2.505\n",
      "Epoch 446 Batch  472/718   train_loss = 2.548\n",
      "Epoch 446 Batch  572/718   train_loss = 2.563\n",
      "Epoch 446 Batch  672/718   train_loss = 2.408\n",
      "Epoch 447 Batch   54/718   train_loss = 2.375\n",
      "Epoch 447 Batch  154/718   train_loss = 2.598\n",
      "Epoch 447 Batch  254/718   train_loss = 2.421\n",
      "Epoch 447 Batch  354/718   train_loss = 2.502\n",
      "Epoch 447 Batch  454/718   train_loss = 2.429\n",
      "Epoch 447 Batch  554/718   train_loss = 2.598\n",
      "Epoch 447 Batch  654/718   train_loss = 2.620\n",
      "Epoch 448 Batch   36/718   train_loss = 2.496\n",
      "Epoch 448 Batch  136/718   train_loss = 2.424\n",
      "Epoch 448 Batch  236/718   train_loss = 2.422\n",
      "Epoch 448 Batch  336/718   train_loss = 2.500\n",
      "Epoch 448 Batch  436/718   train_loss = 2.532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 Batch  536/718   train_loss = 2.555\n",
      "Epoch 448 Batch  636/718   train_loss = 2.575\n",
      "Epoch 449 Batch   18/718   train_loss = 2.482\n",
      "Epoch 449 Batch  118/718   train_loss = 2.450\n",
      "Epoch 449 Batch  218/718   train_loss = 2.525\n",
      "Epoch 449 Batch  318/718   train_loss = 2.483\n",
      "Epoch 449 Batch  418/718   train_loss = 2.626\n",
      "Epoch 449 Batch  518/718   train_loss = 2.557\n",
      "Epoch 449 Batch  618/718   train_loss = 2.512\n",
      "Epoch 450 Batch    0/718   train_loss = 2.543\n",
      "Epoch 450 Batch  100/718   train_loss = 2.488\n",
      "Epoch 450 Batch  200/718   train_loss = 2.466\n",
      "Epoch 450 Batch  300/718   train_loss = 2.469\n",
      "Epoch 450 Batch  400/718   train_loss = 2.546\n",
      "Epoch 450 Batch  500/718   train_loss = 2.659\n",
      "Epoch 450 Batch  600/718   train_loss = 2.536\n",
      "Epoch 450 Batch  700/718   train_loss = 2.431\n",
      "Epoch 451 Batch   82/718   train_loss = 2.457\n",
      "Epoch 451 Batch  182/718   train_loss = 2.613\n",
      "Epoch 451 Batch  282/718   train_loss = 2.577\n",
      "Epoch 451 Batch  382/718   train_loss = 2.407\n",
      "Epoch 451 Batch  482/718   train_loss = 2.654\n",
      "Epoch 451 Batch  582/718   train_loss = 2.397\n",
      "Epoch 451 Batch  682/718   train_loss = 2.470\n",
      "Epoch 452 Batch   64/718   train_loss = 2.505\n",
      "Epoch 452 Batch  164/718   train_loss = 2.378\n",
      "Epoch 452 Batch  264/718   train_loss = 2.513\n",
      "Epoch 452 Batch  364/718   train_loss = 2.569\n",
      "Epoch 452 Batch  464/718   train_loss = 2.463\n",
      "Epoch 452 Batch  564/718   train_loss = 2.516\n",
      "Epoch 452 Batch  664/718   train_loss = 2.553\n",
      "Epoch 453 Batch   46/718   train_loss = 2.442\n",
      "Epoch 453 Batch  146/718   train_loss = 2.399\n",
      "Epoch 453 Batch  246/718   train_loss = 2.560\n",
      "Epoch 453 Batch  346/718   train_loss = 2.435\n",
      "Epoch 453 Batch  446/718   train_loss = 2.468\n",
      "Epoch 453 Batch  546/718   train_loss = 2.381\n",
      "Epoch 453 Batch  646/718   train_loss = 2.521\n",
      "Epoch 454 Batch   28/718   train_loss = 2.419\n",
      "Epoch 454 Batch  128/718   train_loss = 2.455\n",
      "Epoch 454 Batch  228/718   train_loss = 2.572\n",
      "Epoch 454 Batch  328/718   train_loss = 2.483\n",
      "Epoch 454 Batch  428/718   train_loss = 2.397\n",
      "Epoch 454 Batch  528/718   train_loss = 2.560\n",
      "Epoch 454 Batch  628/718   train_loss = 2.639\n",
      "Epoch 455 Batch   10/718   train_loss = 2.454\n",
      "Epoch 455 Batch  110/718   train_loss = 2.468\n",
      "Epoch 455 Batch  210/718   train_loss = 2.485\n",
      "Epoch 455 Batch  310/718   train_loss = 2.564\n",
      "Epoch 455 Batch  410/718   train_loss = 2.436\n",
      "Epoch 455 Batch  510/718   train_loss = 2.541\n",
      "Epoch 455 Batch  610/718   train_loss = 2.553\n",
      "Epoch 455 Batch  710/718   train_loss = 2.447\n",
      "Epoch 456 Batch   92/718   train_loss = 2.462\n",
      "Epoch 456 Batch  192/718   train_loss = 2.442\n",
      "Epoch 456 Batch  292/718   train_loss = 2.349\n",
      "Epoch 456 Batch  392/718   train_loss = 2.448\n",
      "Epoch 456 Batch  492/718   train_loss = 2.567\n",
      "Epoch 456 Batch  592/718   train_loss = 2.568\n",
      "Epoch 456 Batch  692/718   train_loss = 2.502\n",
      "Epoch 457 Batch   74/718   train_loss = 2.535\n",
      "Epoch 457 Batch  174/718   train_loss = 2.575\n",
      "Epoch 457 Batch  274/718   train_loss = 2.527\n",
      "Epoch 457 Batch  374/718   train_loss = 2.409\n",
      "Epoch 457 Batch  474/718   train_loss = 2.530\n",
      "Epoch 457 Batch  574/718   train_loss = 2.362\n",
      "Epoch 457 Batch  674/718   train_loss = 2.456\n",
      "Epoch 458 Batch   56/718   train_loss = 2.590\n",
      "Epoch 458 Batch  156/718   train_loss = 2.487\n",
      "Epoch 458 Batch  256/718   train_loss = 2.398\n",
      "Epoch 458 Batch  356/718   train_loss = 2.444\n",
      "Epoch 458 Batch  456/718   train_loss = 2.450\n",
      "Epoch 458 Batch  556/718   train_loss = 2.402\n",
      "Epoch 458 Batch  656/718   train_loss = 2.438\n",
      "Epoch 459 Batch   38/718   train_loss = 2.516\n",
      "Epoch 459 Batch  138/718   train_loss = 2.554\n",
      "Epoch 459 Batch  238/718   train_loss = 2.543\n",
      "Epoch 459 Batch  338/718   train_loss = 2.506\n",
      "Epoch 459 Batch  438/718   train_loss = 2.398\n",
      "Epoch 459 Batch  538/718   train_loss = 2.532\n",
      "Epoch 459 Batch  638/718   train_loss = 2.450\n",
      "Epoch 460 Batch   20/718   train_loss = 2.519\n",
      "Epoch 460 Batch  120/718   train_loss = 2.481\n",
      "Epoch 460 Batch  220/718   train_loss = 2.477\n",
      "Epoch 460 Batch  320/718   train_loss = 2.564\n",
      "Epoch 460 Batch  420/718   train_loss = 2.610\n",
      "Epoch 460 Batch  520/718   train_loss = 2.527\n",
      "Epoch 460 Batch  620/718   train_loss = 2.670\n",
      "Epoch 461 Batch    2/718   train_loss = 2.568\n",
      "Epoch 461 Batch  102/718   train_loss = 2.389\n",
      "Epoch 461 Batch  202/718   train_loss = 2.477\n",
      "Epoch 461 Batch  302/718   train_loss = 2.537\n",
      "Epoch 461 Batch  402/718   train_loss = 2.522\n",
      "Epoch 461 Batch  502/718   train_loss = 2.500\n",
      "Epoch 461 Batch  602/718   train_loss = 2.475\n",
      "Epoch 461 Batch  702/718   train_loss = 2.456\n",
      "Epoch 462 Batch   84/718   train_loss = 2.394\n",
      "Epoch 462 Batch  184/718   train_loss = 2.467\n",
      "Epoch 462 Batch  284/718   train_loss = 2.482\n",
      "Epoch 462 Batch  384/718   train_loss = 2.484\n",
      "Epoch 462 Batch  484/718   train_loss = 2.462\n",
      "Epoch 462 Batch  584/718   train_loss = 2.432\n",
      "Epoch 462 Batch  684/718   train_loss = 2.452\n",
      "Epoch 463 Batch   66/718   train_loss = 2.439\n",
      "Epoch 463 Batch  166/718   train_loss = 2.456\n",
      "Epoch 463 Batch  266/718   train_loss = 2.532\n",
      "Epoch 463 Batch  366/718   train_loss = 2.557\n",
      "Epoch 463 Batch  466/718   train_loss = 2.495\n",
      "Epoch 463 Batch  566/718   train_loss = 2.453\n",
      "Epoch 463 Batch  666/718   train_loss = 2.476\n",
      "Epoch 464 Batch   48/718   train_loss = 2.364\n",
      "Epoch 464 Batch  148/718   train_loss = 2.493\n",
      "Epoch 464 Batch  248/718   train_loss = 2.507\n",
      "Epoch 464 Batch  348/718   train_loss = 2.394\n",
      "Epoch 464 Batch  448/718   train_loss = 2.495\n",
      "Epoch 464 Batch  548/718   train_loss = 2.511\n",
      "Epoch 464 Batch  648/718   train_loss = 2.495\n",
      "Epoch 465 Batch   30/718   train_loss = 2.474\n",
      "Epoch 465 Batch  130/718   train_loss = 2.422\n",
      "Epoch 465 Batch  230/718   train_loss = 2.555\n",
      "Epoch 465 Batch  330/718   train_loss = 2.555\n",
      "Epoch 465 Batch  430/718   train_loss = 2.620\n",
      "Epoch 465 Batch  530/718   train_loss = 2.435\n",
      "Epoch 465 Batch  630/718   train_loss = 2.436\n",
      "Epoch 466 Batch   12/718   train_loss = 2.364\n",
      "Epoch 466 Batch  112/718   train_loss = 2.524\n",
      "Epoch 466 Batch  212/718   train_loss = 2.573\n",
      "Epoch 466 Batch  312/718   train_loss = 2.495\n",
      "Epoch 466 Batch  412/718   train_loss = 2.422\n",
      "Epoch 466 Batch  512/718   train_loss = 2.469\n",
      "Epoch 466 Batch  612/718   train_loss = 2.556\n",
      "Epoch 466 Batch  712/718   train_loss = 2.537\n",
      "Epoch 467 Batch   94/718   train_loss = 2.364\n",
      "Epoch 467 Batch  194/718   train_loss = 2.396\n",
      "Epoch 467 Batch  294/718   train_loss = 2.552\n",
      "Epoch 467 Batch  394/718   train_loss = 2.471\n",
      "Epoch 467 Batch  494/718   train_loss = 2.591\n",
      "Epoch 467 Batch  594/718   train_loss = 2.540\n",
      "Epoch 467 Batch  694/718   train_loss = 2.575\n",
      "Epoch 468 Batch   76/718   train_loss = 2.416\n",
      "Epoch 468 Batch  176/718   train_loss = 2.559\n",
      "Epoch 468 Batch  276/718   train_loss = 2.471\n",
      "Epoch 468 Batch  376/718   train_loss = 2.497\n",
      "Epoch 468 Batch  476/718   train_loss = 2.504\n",
      "Epoch 468 Batch  576/718   train_loss = 2.524\n",
      "Epoch 468 Batch  676/718   train_loss = 2.496\n",
      "Epoch 469 Batch   58/718   train_loss = 2.490\n",
      "Epoch 469 Batch  158/718   train_loss = 2.513\n",
      "Epoch 469 Batch  258/718   train_loss = 2.468\n",
      "Epoch 469 Batch  358/718   train_loss = 2.411\n",
      "Epoch 469 Batch  458/718   train_loss = 2.549\n",
      "Epoch 469 Batch  558/718   train_loss = 2.586\n",
      "Epoch 469 Batch  658/718   train_loss = 2.383\n",
      "Epoch 470 Batch   40/718   train_loss = 2.496\n",
      "Epoch 470 Batch  140/718   train_loss = 2.449\n",
      "Epoch 470 Batch  240/718   train_loss = 2.394\n",
      "Epoch 470 Batch  340/718   train_loss = 2.514\n",
      "Epoch 470 Batch  440/718   train_loss = 2.464\n",
      "Epoch 470 Batch  540/718   train_loss = 2.491\n",
      "Epoch 470 Batch  640/718   train_loss = 2.461\n",
      "Epoch 471 Batch   22/718   train_loss = 2.315\n",
      "Epoch 471 Batch  122/718   train_loss = 2.482\n",
      "Epoch 471 Batch  222/718   train_loss = 2.478\n",
      "Epoch 471 Batch  322/718   train_loss = 2.515\n",
      "Epoch 471 Batch  422/718   train_loss = 2.531\n",
      "Epoch 471 Batch  522/718   train_loss = 2.471\n",
      "Epoch 471 Batch  622/718   train_loss = 2.524\n",
      "Epoch 472 Batch    4/718   train_loss = 2.382\n",
      "Epoch 472 Batch  104/718   train_loss = 2.473\n",
      "Epoch 472 Batch  204/718   train_loss = 2.413\n",
      "Epoch 472 Batch  304/718   train_loss = 2.466\n",
      "Epoch 472 Batch  404/718   train_loss = 2.452\n",
      "Epoch 472 Batch  504/718   train_loss = 2.611\n",
      "Epoch 472 Batch  604/718   train_loss = 2.388\n",
      "Epoch 472 Batch  704/718   train_loss = 2.444\n",
      "Epoch 473 Batch   86/718   train_loss = 2.501\n",
      "Epoch 473 Batch  186/718   train_loss = 2.492\n",
      "Epoch 473 Batch  286/718   train_loss = 2.556\n",
      "Epoch 473 Batch  386/718   train_loss = 2.457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473 Batch  486/718   train_loss = 2.456\n",
      "Epoch 473 Batch  586/718   train_loss = 2.459\n",
      "Epoch 473 Batch  686/718   train_loss = 2.442\n",
      "Epoch 474 Batch   68/718   train_loss = 2.495\n",
      "Epoch 474 Batch  168/718   train_loss = 2.588\n",
      "Epoch 474 Batch  268/718   train_loss = 2.468\n",
      "Epoch 474 Batch  368/718   train_loss = 2.423\n",
      "Epoch 474 Batch  468/718   train_loss = 2.628\n",
      "Epoch 474 Batch  568/718   train_loss = 2.594\n",
      "Epoch 474 Batch  668/718   train_loss = 2.472\n",
      "Epoch 475 Batch   50/718   train_loss = 2.535\n",
      "Epoch 475 Batch  150/718   train_loss = 2.450\n",
      "Epoch 475 Batch  250/718   train_loss = 2.443\n",
      "Epoch 475 Batch  350/718   train_loss = 2.452\n",
      "Epoch 475 Batch  450/718   train_loss = 2.469\n",
      "Epoch 475 Batch  550/718   train_loss = 2.414\n",
      "Epoch 475 Batch  650/718   train_loss = 2.413\n",
      "Epoch 476 Batch   32/718   train_loss = 2.426\n",
      "Epoch 476 Batch  132/718   train_loss = 2.484\n",
      "Epoch 476 Batch  232/718   train_loss = 2.459\n",
      "Epoch 476 Batch  332/718   train_loss = 2.386\n",
      "Epoch 476 Batch  432/718   train_loss = 2.567\n",
      "Epoch 476 Batch  532/718   train_loss = 2.572\n",
      "Epoch 476 Batch  632/718   train_loss = 2.347\n",
      "Epoch 477 Batch   14/718   train_loss = 2.508\n",
      "Epoch 477 Batch  114/718   train_loss = 2.415\n",
      "Epoch 477 Batch  214/718   train_loss = 2.524\n",
      "Epoch 477 Batch  314/718   train_loss = 2.491\n",
      "Epoch 477 Batch  414/718   train_loss = 2.374\n",
      "Epoch 477 Batch  514/718   train_loss = 2.568\n",
      "Epoch 477 Batch  614/718   train_loss = 2.454\n",
      "Epoch 477 Batch  714/718   train_loss = 2.466\n",
      "Epoch 478 Batch   96/718   train_loss = 2.464\n",
      "Epoch 478 Batch  196/718   train_loss = 2.381\n",
      "Epoch 478 Batch  296/718   train_loss = 2.442\n",
      "Epoch 478 Batch  396/718   train_loss = 2.504\n",
      "Epoch 478 Batch  496/718   train_loss = 2.562\n",
      "Epoch 478 Batch  596/718   train_loss = 2.435\n",
      "Epoch 478 Batch  696/718   train_loss = 2.495\n",
      "Epoch 479 Batch   78/718   train_loss = 2.559\n",
      "Epoch 479 Batch  178/718   train_loss = 2.484\n",
      "Epoch 479 Batch  278/718   train_loss = 2.402\n",
      "Epoch 479 Batch  378/718   train_loss = 2.375\n",
      "Epoch 479 Batch  478/718   train_loss = 2.431\n",
      "Epoch 479 Batch  578/718   train_loss = 2.404\n",
      "Epoch 479 Batch  678/718   train_loss = 2.522\n",
      "Epoch 480 Batch   60/718   train_loss = 2.513\n",
      "Epoch 480 Batch  160/718   train_loss = 2.482\n",
      "Epoch 480 Batch  260/718   train_loss = 2.519\n",
      "Epoch 480 Batch  360/718   train_loss = 2.465\n",
      "Epoch 480 Batch  460/718   train_loss = 2.418\n",
      "Epoch 480 Batch  560/718   train_loss = 2.452\n",
      "Epoch 480 Batch  660/718   train_loss = 2.600\n",
      "Epoch 481 Batch   42/718   train_loss = 2.487\n",
      "Epoch 481 Batch  142/718   train_loss = 2.497\n",
      "Epoch 481 Batch  242/718   train_loss = 2.351\n",
      "Epoch 481 Batch  342/718   train_loss = 2.447\n",
      "Epoch 481 Batch  442/718   train_loss = 2.463\n",
      "Epoch 481 Batch  542/718   train_loss = 2.429\n",
      "Epoch 481 Batch  642/718   train_loss = 2.538\n",
      "Epoch 482 Batch   24/718   train_loss = 2.491\n",
      "Epoch 482 Batch  124/718   train_loss = 2.362\n",
      "Epoch 482 Batch  224/718   train_loss = 2.475\n",
      "Epoch 482 Batch  324/718   train_loss = 2.435\n",
      "Epoch 482 Batch  424/718   train_loss = 2.467\n",
      "Epoch 482 Batch  524/718   train_loss = 2.513\n",
      "Epoch 482 Batch  624/718   train_loss = 2.506\n",
      "Epoch 483 Batch    6/718   train_loss = 2.419\n",
      "Epoch 483 Batch  106/718   train_loss = 2.492\n",
      "Epoch 483 Batch  206/718   train_loss = 2.385\n",
      "Epoch 483 Batch  306/718   train_loss = 2.426\n",
      "Epoch 483 Batch  406/718   train_loss = 2.439\n",
      "Epoch 483 Batch  506/718   train_loss = 2.417\n",
      "Epoch 483 Batch  606/718   train_loss = 2.395\n",
      "Epoch 483 Batch  706/718   train_loss = 2.472\n",
      "Epoch 484 Batch   88/718   train_loss = 2.565\n",
      "Epoch 484 Batch  188/718   train_loss = 2.419\n",
      "Epoch 484 Batch  288/718   train_loss = 2.454\n",
      "Epoch 484 Batch  388/718   train_loss = 2.314\n",
      "Epoch 484 Batch  488/718   train_loss = 2.368\n",
      "Epoch 484 Batch  588/718   train_loss = 2.508\n",
      "Epoch 484 Batch  688/718   train_loss = 2.439\n",
      "Epoch 485 Batch   70/718   train_loss = 2.481\n",
      "Epoch 485 Batch  170/718   train_loss = 2.494\n",
      "Epoch 485 Batch  270/718   train_loss = 2.545\n",
      "Epoch 485 Batch  370/718   train_loss = 2.466\n",
      "Epoch 485 Batch  470/718   train_loss = 2.584\n",
      "Epoch 485 Batch  570/718   train_loss = 2.541\n",
      "Epoch 485 Batch  670/718   train_loss = 2.455\n",
      "Epoch 486 Batch   52/718   train_loss = 2.395\n",
      "Epoch 486 Batch  152/718   train_loss = 2.452\n",
      "Epoch 486 Batch  252/718   train_loss = 2.478\n",
      "Epoch 486 Batch  352/718   train_loss = 2.453\n",
      "Epoch 486 Batch  452/718   train_loss = 2.502\n",
      "Epoch 486 Batch  552/718   train_loss = 2.527\n",
      "Epoch 486 Batch  652/718   train_loss = 2.377\n",
      "Epoch 487 Batch   34/718   train_loss = 2.469\n",
      "Epoch 487 Batch  134/718   train_loss = 2.420\n",
      "Epoch 487 Batch  234/718   train_loss = 2.513\n",
      "Epoch 487 Batch  334/718   train_loss = 2.529\n",
      "Epoch 487 Batch  434/718   train_loss = 2.538\n",
      "Epoch 487 Batch  534/718   train_loss = 2.410\n",
      "Epoch 487 Batch  634/718   train_loss = 2.577\n",
      "Epoch 488 Batch   16/718   train_loss = 2.417\n",
      "Epoch 488 Batch  116/718   train_loss = 2.392\n",
      "Epoch 488 Batch  216/718   train_loss = 2.450\n",
      "Epoch 488 Batch  316/718   train_loss = 2.413\n",
      "Epoch 488 Batch  416/718   train_loss = 2.415\n",
      "Epoch 488 Batch  516/718   train_loss = 2.458\n",
      "Epoch 488 Batch  616/718   train_loss = 2.524\n",
      "Epoch 488 Batch  716/718   train_loss = 2.544\n",
      "Epoch 489 Batch   98/718   train_loss = 2.380\n",
      "Epoch 489 Batch  198/718   train_loss = 2.511\n",
      "Epoch 489 Batch  298/718   train_loss = 2.435\n",
      "Epoch 489 Batch  398/718   train_loss = 2.468\n",
      "Epoch 489 Batch  498/718   train_loss = 2.504\n",
      "Epoch 489 Batch  598/718   train_loss = 2.518\n",
      "Epoch 489 Batch  698/718   train_loss = 2.509\n",
      "Epoch 490 Batch   80/718   train_loss = 2.439\n",
      "Epoch 490 Batch  180/718   train_loss = 2.412\n",
      "Epoch 490 Batch  280/718   train_loss = 2.530\n",
      "Epoch 490 Batch  380/718   train_loss = 2.472\n",
      "Epoch 490 Batch  480/718   train_loss = 2.533\n",
      "Epoch 490 Batch  580/718   train_loss = 2.410\n",
      "Epoch 490 Batch  680/718   train_loss = 2.509\n",
      "Epoch 491 Batch   62/718   train_loss = 2.466\n",
      "Epoch 491 Batch  162/718   train_loss = 2.468\n",
      "Epoch 491 Batch  262/718   train_loss = 2.552\n",
      "Epoch 491 Batch  362/718   train_loss = 2.571\n",
      "Epoch 491 Batch  462/718   train_loss = 2.491\n",
      "Epoch 491 Batch  562/718   train_loss = 2.477\n",
      "Epoch 491 Batch  662/718   train_loss = 2.474\n",
      "Epoch 492 Batch   44/718   train_loss = 2.422\n",
      "Epoch 492 Batch  144/718   train_loss = 2.566\n",
      "Epoch 492 Batch  244/718   train_loss = 2.565\n",
      "Epoch 492 Batch  344/718   train_loss = 2.470\n",
      "Epoch 492 Batch  444/718   train_loss = 2.403\n",
      "Epoch 492 Batch  544/718   train_loss = 2.466\n",
      "Epoch 492 Batch  644/718   train_loss = 2.584\n",
      "Epoch 493 Batch   26/718   train_loss = 2.571\n",
      "Epoch 493 Batch  126/718   train_loss = 2.466\n",
      "Epoch 493 Batch  226/718   train_loss = 2.450\n",
      "Epoch 493 Batch  326/718   train_loss = 2.455\n",
      "Epoch 493 Batch  426/718   train_loss = 2.461\n",
      "Epoch 493 Batch  526/718   train_loss = 2.462\n",
      "Epoch 493 Batch  626/718   train_loss = 2.560\n",
      "Epoch 494 Batch    8/718   train_loss = 2.355\n",
      "Epoch 494 Batch  108/718   train_loss = 2.366\n",
      "Epoch 494 Batch  208/718   train_loss = 2.525\n",
      "Epoch 494 Batch  308/718   train_loss = 2.552\n",
      "Epoch 494 Batch  408/718   train_loss = 2.578\n",
      "Epoch 494 Batch  508/718   train_loss = 2.531\n",
      "Epoch 494 Batch  608/718   train_loss = 2.598\n",
      "Epoch 494 Batch  708/718   train_loss = 2.444\n",
      "Epoch 495 Batch   90/718   train_loss = 2.406\n",
      "Epoch 495 Batch  190/718   train_loss = 2.546\n",
      "Epoch 495 Batch  290/718   train_loss = 2.364\n",
      "Epoch 495 Batch  390/718   train_loss = 2.459\n",
      "Epoch 495 Batch  490/718   train_loss = 2.507\n",
      "Epoch 495 Batch  590/718   train_loss = 2.451\n",
      "Epoch 495 Batch  690/718   train_loss = 2.518\n",
      "Epoch 496 Batch   72/718   train_loss = 2.518\n",
      "Epoch 496 Batch  172/718   train_loss = 2.533\n",
      "Epoch 496 Batch  272/718   train_loss = 2.503\n",
      "Epoch 496 Batch  372/718   train_loss = 2.504\n",
      "Epoch 496 Batch  472/718   train_loss = 2.531\n",
      "Epoch 496 Batch  572/718   train_loss = 2.535\n",
      "Epoch 496 Batch  672/718   train_loss = 2.407\n",
      "Epoch 497 Batch   54/718   train_loss = 2.312\n",
      "Epoch 497 Batch  154/718   train_loss = 2.576\n",
      "Epoch 497 Batch  254/718   train_loss = 2.410\n",
      "Epoch 497 Batch  354/718   train_loss = 2.496\n",
      "Epoch 497 Batch  454/718   train_loss = 2.416\n",
      "Epoch 497 Batch  554/718   train_loss = 2.569\n",
      "Epoch 497 Batch  654/718   train_loss = 2.575\n",
      "Epoch 498 Batch   36/718   train_loss = 2.458\n",
      "Epoch 498 Batch  136/718   train_loss = 2.416\n",
      "Epoch 498 Batch  236/718   train_loss = 2.397\n",
      "Epoch 498 Batch  336/718   train_loss = 2.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498 Batch  436/718   train_loss = 2.467\n",
      "Epoch 498 Batch  536/718   train_loss = 2.521\n",
      "Epoch 498 Batch  636/718   train_loss = 2.524\n",
      "Epoch 499 Batch   18/718   train_loss = 2.466\n",
      "Epoch 499 Batch  118/718   train_loss = 2.403\n",
      "Epoch 499 Batch  218/718   train_loss = 2.490\n",
      "Epoch 499 Batch  318/718   train_loss = 2.474\n",
      "Epoch 499 Batch  418/718   train_loss = 2.579\n",
      "Epoch 499 Batch  518/718   train_loss = 2.521\n",
      "Epoch 499 Batch  618/718   train_loss = 2.445\n",
      "Epoch 500 Batch    0/718   train_loss = 2.517\n",
      "Epoch 500 Batch  100/718   train_loss = 2.477\n",
      "Epoch 500 Batch  200/718   train_loss = 2.406\n",
      "Epoch 500 Batch  300/718   train_loss = 2.453\n",
      "Epoch 500 Batch  400/718   train_loss = 2.498\n",
      "Epoch 500 Batch  500/718   train_loss = 2.615\n",
      "Epoch 500 Batch  600/718   train_loss = 2.542\n",
      "Epoch 500 Batch  700/718   train_loss = 2.383\n",
      "Epoch 501 Batch   82/718   train_loss = 2.433\n",
      "Epoch 501 Batch  182/718   train_loss = 2.516\n",
      "Epoch 501 Batch  282/718   train_loss = 2.507\n",
      "Epoch 501 Batch  382/718   train_loss = 2.400\n",
      "Epoch 501 Batch  482/718   train_loss = 2.594\n",
      "Epoch 501 Batch  582/718   train_loss = 2.396\n",
      "Epoch 501 Batch  682/718   train_loss = 2.481\n",
      "Epoch 502 Batch   64/718   train_loss = 2.436\n",
      "Epoch 502 Batch  164/718   train_loss = 2.343\n",
      "Epoch 502 Batch  264/718   train_loss = 2.460\n",
      "Epoch 502 Batch  364/718   train_loss = 2.529\n",
      "Epoch 502 Batch  464/718   train_loss = 2.386\n",
      "Epoch 502 Batch  564/718   train_loss = 2.526\n",
      "Epoch 502 Batch  664/718   train_loss = 2.547\n",
      "Epoch 503 Batch   46/718   train_loss = 2.393\n",
      "Epoch 503 Batch  146/718   train_loss = 2.359\n",
      "Epoch 503 Batch  246/718   train_loss = 2.538\n",
      "Epoch 503 Batch  346/718   train_loss = 2.420\n",
      "Epoch 503 Batch  446/718   train_loss = 2.489\n",
      "Epoch 503 Batch  546/718   train_loss = 2.382\n",
      "Epoch 503 Batch  646/718   train_loss = 2.519\n",
      "Epoch 504 Batch   28/718   train_loss = 2.422\n",
      "Epoch 504 Batch  128/718   train_loss = 2.388\n",
      "Epoch 504 Batch  228/718   train_loss = 2.503\n",
      "Epoch 504 Batch  328/718   train_loss = 2.455\n",
      "Epoch 504 Batch  428/718   train_loss = 2.342\n",
      "Epoch 504 Batch  528/718   train_loss = 2.531\n",
      "Epoch 504 Batch  628/718   train_loss = 2.533\n",
      "Epoch 505 Batch   10/718   train_loss = 2.447\n",
      "Epoch 505 Batch  110/718   train_loss = 2.435\n",
      "Epoch 505 Batch  210/718   train_loss = 2.390\n",
      "Epoch 505 Batch  310/718   train_loss = 2.529\n",
      "Epoch 505 Batch  410/718   train_loss = 2.314\n",
      "Epoch 505 Batch  510/718   train_loss = 2.480\n",
      "Epoch 505 Batch  610/718   train_loss = 2.521\n",
      "Epoch 505 Batch  710/718   train_loss = 2.383\n",
      "Epoch 506 Batch   92/718   train_loss = 2.444\n",
      "Epoch 506 Batch  192/718   train_loss = 2.444\n",
      "Epoch 506 Batch  292/718   train_loss = 2.357\n",
      "Epoch 506 Batch  392/718   train_loss = 2.461\n",
      "Epoch 506 Batch  492/718   train_loss = 2.533\n",
      "Epoch 506 Batch  592/718   train_loss = 2.490\n",
      "Epoch 506 Batch  692/718   train_loss = 2.429\n",
      "Epoch 507 Batch   74/718   train_loss = 2.535\n",
      "Epoch 507 Batch  174/718   train_loss = 2.596\n",
      "Epoch 507 Batch  274/718   train_loss = 2.497\n",
      "Epoch 507 Batch  374/718   train_loss = 2.391\n",
      "Epoch 507 Batch  474/718   train_loss = 2.512\n",
      "Epoch 507 Batch  574/718   train_loss = 2.333\n",
      "Epoch 507 Batch  674/718   train_loss = 2.397\n",
      "Epoch 508 Batch   56/718   train_loss = 2.537\n",
      "Epoch 508 Batch  156/718   train_loss = 2.381\n",
      "Epoch 508 Batch  256/718   train_loss = 2.402\n",
      "Epoch 508 Batch  356/718   train_loss = 2.398\n",
      "Epoch 508 Batch  456/718   train_loss = 2.409\n",
      "Epoch 508 Batch  556/718   train_loss = 2.419\n",
      "Epoch 508 Batch  656/718   train_loss = 2.396\n",
      "Epoch 509 Batch   38/718   train_loss = 2.516\n",
      "Epoch 509 Batch  138/718   train_loss = 2.533\n",
      "Epoch 509 Batch  238/718   train_loss = 2.501\n",
      "Epoch 509 Batch  338/718   train_loss = 2.437\n",
      "Epoch 509 Batch  438/718   train_loss = 2.345\n",
      "Epoch 509 Batch  538/718   train_loss = 2.461\n",
      "Epoch 509 Batch  638/718   train_loss = 2.392\n",
      "Epoch 510 Batch   20/718   train_loss = 2.484\n",
      "Epoch 510 Batch  120/718   train_loss = 2.454\n",
      "Epoch 510 Batch  220/718   train_loss = 2.416\n",
      "Epoch 510 Batch  320/718   train_loss = 2.496\n",
      "Epoch 510 Batch  420/718   train_loss = 2.616\n",
      "Epoch 510 Batch  520/718   train_loss = 2.521\n",
      "Epoch 510 Batch  620/718   train_loss = 2.617\n",
      "Epoch 511 Batch    2/718   train_loss = 2.517\n",
      "Epoch 511 Batch  102/718   train_loss = 2.353\n",
      "Epoch 511 Batch  202/718   train_loss = 2.420\n",
      "Epoch 511 Batch  302/718   train_loss = 2.563\n",
      "Epoch 511 Batch  402/718   train_loss = 2.525\n",
      "Epoch 511 Batch  502/718   train_loss = 2.511\n",
      "Epoch 511 Batch  602/718   train_loss = 2.473\n",
      "Epoch 511 Batch  702/718   train_loss = 2.388\n",
      "Epoch 512 Batch   84/718   train_loss = 2.376\n",
      "Epoch 512 Batch  184/718   train_loss = 2.471\n",
      "Epoch 512 Batch  284/718   train_loss = 2.438\n",
      "Epoch 512 Batch  384/718   train_loss = 2.418\n",
      "Epoch 512 Batch  484/718   train_loss = 2.440\n",
      "Epoch 512 Batch  584/718   train_loss = 2.410\n",
      "Epoch 512 Batch  684/718   train_loss = 2.455\n",
      "Epoch 513 Batch   66/718   train_loss = 2.400\n",
      "Epoch 513 Batch  166/718   train_loss = 2.442\n",
      "Epoch 513 Batch  266/718   train_loss = 2.501\n",
      "Epoch 513 Batch  366/718   train_loss = 2.528\n",
      "Epoch 513 Batch  466/718   train_loss = 2.424\n",
      "Epoch 513 Batch  566/718   train_loss = 2.407\n",
      "Epoch 513 Batch  666/718   train_loss = 2.401\n",
      "Epoch 514 Batch   48/718   train_loss = 2.327\n",
      "Epoch 514 Batch  148/718   train_loss = 2.469\n",
      "Epoch 514 Batch  248/718   train_loss = 2.489\n",
      "Epoch 514 Batch  348/718   train_loss = 2.375\n",
      "Epoch 514 Batch  448/718   train_loss = 2.447\n",
      "Epoch 514 Batch  548/718   train_loss = 2.544\n",
      "Epoch 514 Batch  648/718   train_loss = 2.441\n",
      "Epoch 515 Batch   30/718   train_loss = 2.426\n",
      "Epoch 515 Batch  130/718   train_loss = 2.385\n",
      "Epoch 515 Batch  230/718   train_loss = 2.532\n",
      "Epoch 515 Batch  330/718   train_loss = 2.547\n",
      "Epoch 515 Batch  430/718   train_loss = 2.535\n",
      "Epoch 515 Batch  530/718   train_loss = 2.411\n",
      "Epoch 515 Batch  630/718   train_loss = 2.394\n",
      "Epoch 516 Batch   12/718   train_loss = 2.337\n",
      "Epoch 516 Batch  112/718   train_loss = 2.483\n",
      "Epoch 516 Batch  212/718   train_loss = 2.536\n",
      "Epoch 516 Batch  312/718   train_loss = 2.503\n",
      "Epoch 516 Batch  412/718   train_loss = 2.405\n",
      "Epoch 516 Batch  512/718   train_loss = 2.404\n",
      "Epoch 516 Batch  612/718   train_loss = 2.513\n",
      "Epoch 516 Batch  712/718   train_loss = 2.557\n",
      "Epoch 517 Batch   94/718   train_loss = 2.330\n",
      "Epoch 517 Batch  194/718   train_loss = 2.329\n",
      "Epoch 517 Batch  294/718   train_loss = 2.501\n",
      "Epoch 517 Batch  394/718   train_loss = 2.454\n",
      "Epoch 517 Batch  494/718   train_loss = 2.547\n",
      "Epoch 517 Batch  594/718   train_loss = 2.488\n",
      "Epoch 517 Batch  694/718   train_loss = 2.536\n",
      "Epoch 518 Batch   76/718   train_loss = 2.411\n",
      "Epoch 518 Batch  176/718   train_loss = 2.541\n",
      "Epoch 518 Batch  276/718   train_loss = 2.461\n",
      "Epoch 518 Batch  376/718   train_loss = 2.483\n",
      "Epoch 518 Batch  476/718   train_loss = 2.466\n",
      "Epoch 518 Batch  576/718   train_loss = 2.464\n",
      "Epoch 518 Batch  676/718   train_loss = 2.443\n",
      "Epoch 519 Batch   58/718   train_loss = 2.491\n",
      "Epoch 519 Batch  158/718   train_loss = 2.465\n",
      "Epoch 519 Batch  258/718   train_loss = 2.435\n",
      "Epoch 519 Batch  358/718   train_loss = 2.382\n",
      "Epoch 519 Batch  458/718   train_loss = 2.531\n",
      "Epoch 519 Batch  558/718   train_loss = 2.604\n",
      "Epoch 519 Batch  658/718   train_loss = 2.361\n",
      "Epoch 520 Batch   40/718   train_loss = 2.465\n",
      "Epoch 520 Batch  140/718   train_loss = 2.421\n",
      "Epoch 520 Batch  240/718   train_loss = 2.359\n",
      "Epoch 520 Batch  340/718   train_loss = 2.493\n",
      "Epoch 520 Batch  440/718   train_loss = 2.384\n",
      "Epoch 520 Batch  540/718   train_loss = 2.545\n",
      "Epoch 520 Batch  640/718   train_loss = 2.407\n",
      "Epoch 521 Batch   22/718   train_loss = 2.316\n",
      "Epoch 521 Batch  122/718   train_loss = 2.449\n",
      "Epoch 521 Batch  222/718   train_loss = 2.403\n",
      "Epoch 521 Batch  322/718   train_loss = 2.464\n",
      "Epoch 521 Batch  422/718   train_loss = 2.523\n",
      "Epoch 521 Batch  522/718   train_loss = 2.440\n",
      "Epoch 521 Batch  622/718   train_loss = 2.461\n",
      "Epoch 522 Batch    4/718   train_loss = 2.342\n",
      "Epoch 522 Batch  104/718   train_loss = 2.466\n",
      "Epoch 522 Batch  204/718   train_loss = 2.358\n",
      "Epoch 522 Batch  304/718   train_loss = 2.422\n",
      "Epoch 522 Batch  404/718   train_loss = 2.405\n",
      "Epoch 522 Batch  504/718   train_loss = 2.600\n",
      "Epoch 522 Batch  604/718   train_loss = 2.386\n",
      "Epoch 522 Batch  704/718   train_loss = 2.456\n",
      "Epoch 523 Batch   86/718   train_loss = 2.442\n",
      "Epoch 523 Batch  186/718   train_loss = 2.430\n",
      "Epoch 523 Batch  286/718   train_loss = 2.523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523 Batch  386/718   train_loss = 2.471\n",
      "Epoch 523 Batch  486/718   train_loss = 2.470\n",
      "Epoch 523 Batch  586/718   train_loss = 2.456\n",
      "Epoch 523 Batch  686/718   train_loss = 2.459\n",
      "Epoch 524 Batch   68/718   train_loss = 2.471\n",
      "Epoch 524 Batch  168/718   train_loss = 2.528\n",
      "Epoch 524 Batch  268/718   train_loss = 2.435\n",
      "Epoch 524 Batch  368/718   train_loss = 2.403\n",
      "Epoch 524 Batch  468/718   train_loss = 2.623\n",
      "Epoch 524 Batch  568/718   train_loss = 2.558\n",
      "Epoch 524 Batch  668/718   train_loss = 2.447\n",
      "Epoch 525 Batch   50/718   train_loss = 2.527\n",
      "Epoch 525 Batch  150/718   train_loss = 2.406\n",
      "Epoch 525 Batch  250/718   train_loss = 2.417\n",
      "Epoch 525 Batch  350/718   train_loss = 2.427\n",
      "Epoch 525 Batch  450/718   train_loss = 2.438\n",
      "Epoch 525 Batch  550/718   train_loss = 2.416\n",
      "Epoch 525 Batch  650/718   train_loss = 2.390\n",
      "Epoch 526 Batch   32/718   train_loss = 2.374\n",
      "Epoch 526 Batch  132/718   train_loss = 2.480\n",
      "Epoch 526 Batch  232/718   train_loss = 2.398\n",
      "Epoch 526 Batch  332/718   train_loss = 2.385\n",
      "Epoch 526 Batch  432/718   train_loss = 2.583\n",
      "Epoch 526 Batch  532/718   train_loss = 2.553\n",
      "Epoch 526 Batch  632/718   train_loss = 2.335\n",
      "Epoch 527 Batch   14/718   train_loss = 2.515\n",
      "Epoch 527 Batch  114/718   train_loss = 2.379\n",
      "Epoch 527 Batch  214/718   train_loss = 2.515\n",
      "Epoch 527 Batch  314/718   train_loss = 2.485\n",
      "Epoch 527 Batch  414/718   train_loss = 2.319\n",
      "Epoch 527 Batch  514/718   train_loss = 2.558\n",
      "Epoch 527 Batch  614/718   train_loss = 2.412\n",
      "Epoch 527 Batch  714/718   train_loss = 2.388\n",
      "Epoch 528 Batch   96/718   train_loss = 2.398\n",
      "Epoch 528 Batch  196/718   train_loss = 2.378\n",
      "Epoch 528 Batch  296/718   train_loss = 2.371\n",
      "Epoch 528 Batch  396/718   train_loss = 2.484\n",
      "Epoch 528 Batch  496/718   train_loss = 2.563\n",
      "Epoch 528 Batch  596/718   train_loss = 2.401\n",
      "Epoch 528 Batch  696/718   train_loss = 2.467\n",
      "Epoch 529 Batch   78/718   train_loss = 2.488\n",
      "Epoch 529 Batch  178/718   train_loss = 2.452\n",
      "Epoch 529 Batch  278/718   train_loss = 2.410\n",
      "Epoch 529 Batch  378/718   train_loss = 2.346\n",
      "Epoch 529 Batch  478/718   train_loss = 2.392\n",
      "Epoch 529 Batch  578/718   train_loss = 2.356\n",
      "Epoch 529 Batch  678/718   train_loss = 2.498\n",
      "Epoch 530 Batch   60/718   train_loss = 2.518\n",
      "Epoch 530 Batch  160/718   train_loss = 2.508\n",
      "Epoch 530 Batch  260/718   train_loss = 2.454\n",
      "Epoch 530 Batch  360/718   train_loss = 2.445\n",
      "Epoch 530 Batch  460/718   train_loss = 2.437\n",
      "Epoch 530 Batch  560/718   train_loss = 2.441\n",
      "Epoch 530 Batch  660/718   train_loss = 2.558\n",
      "Epoch 531 Batch   42/718   train_loss = 2.475\n",
      "Epoch 531 Batch  142/718   train_loss = 2.496\n",
      "Epoch 531 Batch  242/718   train_loss = 2.360\n",
      "Epoch 531 Batch  342/718   train_loss = 2.453\n",
      "Epoch 531 Batch  442/718   train_loss = 2.465\n",
      "Epoch 531 Batch  542/718   train_loss = 2.372\n",
      "Epoch 531 Batch  642/718   train_loss = 2.522\n",
      "Epoch 532 Batch   24/718   train_loss = 2.455\n",
      "Epoch 532 Batch  124/718   train_loss = 2.396\n",
      "Epoch 532 Batch  224/718   train_loss = 2.446\n",
      "Epoch 532 Batch  324/718   train_loss = 2.426\n",
      "Epoch 532 Batch  424/718   train_loss = 2.424\n",
      "Epoch 532 Batch  524/718   train_loss = 2.438\n",
      "Epoch 532 Batch  624/718   train_loss = 2.464\n",
      "Epoch 533 Batch    6/718   train_loss = 2.355\n",
      "Epoch 533 Batch  106/718   train_loss = 2.442\n",
      "Epoch 533 Batch  206/718   train_loss = 2.319\n",
      "Epoch 533 Batch  306/718   train_loss = 2.431\n",
      "Epoch 533 Batch  406/718   train_loss = 2.436\n",
      "Epoch 533 Batch  506/718   train_loss = 2.430\n",
      "Epoch 533 Batch  606/718   train_loss = 2.380\n",
      "Epoch 533 Batch  706/718   train_loss = 2.458\n",
      "Epoch 534 Batch   88/718   train_loss = 2.525\n",
      "Epoch 534 Batch  188/718   train_loss = 2.406\n",
      "Epoch 534 Batch  288/718   train_loss = 2.407\n",
      "Epoch 534 Batch  388/718   train_loss = 2.369\n",
      "Epoch 534 Batch  488/718   train_loss = 2.325\n",
      "Epoch 534 Batch  588/718   train_loss = 2.514\n",
      "Epoch 534 Batch  688/718   train_loss = 2.398\n",
      "Epoch 535 Batch   70/718   train_loss = 2.428\n",
      "Epoch 535 Batch  170/718   train_loss = 2.461\n",
      "Epoch 535 Batch  270/718   train_loss = 2.557\n",
      "Epoch 535 Batch  370/718   train_loss = 2.458\n",
      "Epoch 535 Batch  470/718   train_loss = 2.568\n",
      "Epoch 535 Batch  570/718   train_loss = 2.494\n",
      "Epoch 535 Batch  670/718   train_loss = 2.397\n",
      "Epoch 536 Batch   52/718   train_loss = 2.395\n",
      "Epoch 536 Batch  152/718   train_loss = 2.376\n",
      "Epoch 536 Batch  252/718   train_loss = 2.487\n",
      "Epoch 536 Batch  352/718   train_loss = 2.406\n",
      "Epoch 536 Batch  452/718   train_loss = 2.442\n",
      "Epoch 536 Batch  552/718   train_loss = 2.507\n",
      "Epoch 536 Batch  652/718   train_loss = 2.429\n",
      "Epoch 537 Batch   34/718   train_loss = 2.390\n",
      "Epoch 537 Batch  134/718   train_loss = 2.385\n",
      "Epoch 537 Batch  234/718   train_loss = 2.560\n",
      "Epoch 537 Batch  334/718   train_loss = 2.499\n",
      "Epoch 537 Batch  434/718   train_loss = 2.480\n",
      "Epoch 537 Batch  534/718   train_loss = 2.466\n",
      "Epoch 537 Batch  634/718   train_loss = 2.568\n",
      "Epoch 538 Batch   16/718   train_loss = 2.374\n",
      "Epoch 538 Batch  116/718   train_loss = 2.343\n",
      "Epoch 538 Batch  216/718   train_loss = 2.403\n",
      "Epoch 538 Batch  316/718   train_loss = 2.359\n",
      "Epoch 538 Batch  416/718   train_loss = 2.463\n",
      "Epoch 538 Batch  516/718   train_loss = 2.452\n",
      "Epoch 538 Batch  616/718   train_loss = 2.536\n",
      "Epoch 538 Batch  716/718   train_loss = 2.501\n",
      "Epoch 539 Batch   98/718   train_loss = 2.400\n",
      "Epoch 539 Batch  198/718   train_loss = 2.413\n",
      "Epoch 539 Batch  298/718   train_loss = 2.378\n",
      "Epoch 539 Batch  398/718   train_loss = 2.457\n",
      "Epoch 539 Batch  498/718   train_loss = 2.461\n",
      "Epoch 539 Batch  598/718   train_loss = 2.475\n",
      "Epoch 539 Batch  698/718   train_loss = 2.492\n",
      "Epoch 540 Batch   80/718   train_loss = 2.441\n",
      "Epoch 540 Batch  180/718   train_loss = 2.372\n",
      "Epoch 540 Batch  280/718   train_loss = 2.535\n",
      "Epoch 540 Batch  380/718   train_loss = 2.426\n",
      "Epoch 540 Batch  480/718   train_loss = 2.509\n",
      "Epoch 540 Batch  580/718   train_loss = 2.366\n",
      "Epoch 540 Batch  680/718   train_loss = 2.511\n",
      "Epoch 541 Batch   62/718   train_loss = 2.429\n",
      "Epoch 541 Batch  162/718   train_loss = 2.448\n",
      "Epoch 541 Batch  262/718   train_loss = 2.514\n",
      "Epoch 541 Batch  362/718   train_loss = 2.508\n",
      "Epoch 541 Batch  462/718   train_loss = 2.451\n",
      "Epoch 541 Batch  562/718   train_loss = 2.380\n",
      "Epoch 541 Batch  662/718   train_loss = 2.429\n",
      "Epoch 542 Batch   44/718   train_loss = 2.430\n",
      "Epoch 542 Batch  144/718   train_loss = 2.567\n",
      "Epoch 542 Batch  244/718   train_loss = 2.543\n",
      "Epoch 542 Batch  344/718   train_loss = 2.476\n",
      "Epoch 542 Batch  444/718   train_loss = 2.407\n",
      "Epoch 542 Batch  544/718   train_loss = 2.396\n",
      "Epoch 542 Batch  644/718   train_loss = 2.550\n",
      "Epoch 543 Batch   26/718   train_loss = 2.562\n",
      "Epoch 543 Batch  126/718   train_loss = 2.388\n",
      "Epoch 543 Batch  226/718   train_loss = 2.389\n",
      "Epoch 543 Batch  326/718   train_loss = 2.408\n",
      "Epoch 543 Batch  426/718   train_loss = 2.400\n",
      "Epoch 543 Batch  526/718   train_loss = 2.423\n",
      "Epoch 543 Batch  626/718   train_loss = 2.550\n",
      "Epoch 544 Batch    8/718   train_loss = 2.368\n",
      "Epoch 544 Batch  108/718   train_loss = 2.350\n",
      "Epoch 544 Batch  208/718   train_loss = 2.466\n",
      "Epoch 544 Batch  308/718   train_loss = 2.498\n",
      "Epoch 544 Batch  408/718   train_loss = 2.520\n",
      "Epoch 544 Batch  508/718   train_loss = 2.550\n",
      "Epoch 544 Batch  608/718   train_loss = 2.588\n",
      "Epoch 544 Batch  708/718   train_loss = 2.398\n",
      "Epoch 545 Batch   90/718   train_loss = 2.343\n",
      "Epoch 545 Batch  190/718   train_loss = 2.502\n",
      "Epoch 545 Batch  290/718   train_loss = 2.354\n",
      "Epoch 545 Batch  390/718   train_loss = 2.402\n",
      "Epoch 545 Batch  490/718   train_loss = 2.436\n",
      "Epoch 545 Batch  590/718   train_loss = 2.428\n",
      "Epoch 545 Batch  690/718   train_loss = 2.508\n",
      "Epoch 546 Batch   72/718   train_loss = 2.522\n",
      "Epoch 546 Batch  172/718   train_loss = 2.516\n",
      "Epoch 546 Batch  272/718   train_loss = 2.496\n",
      "Epoch 546 Batch  372/718   train_loss = 2.464\n",
      "Epoch 546 Batch  472/718   train_loss = 2.523\n",
      "Epoch 546 Batch  572/718   train_loss = 2.516\n",
      "Epoch 546 Batch  672/718   train_loss = 2.422\n",
      "Epoch 547 Batch   54/718   train_loss = 2.319\n",
      "Epoch 547 Batch  154/718   train_loss = 2.554\n",
      "Epoch 547 Batch  254/718   train_loss = 2.383\n",
      "Epoch 547 Batch  354/718   train_loss = 2.451\n",
      "Epoch 547 Batch  454/718   train_loss = 2.363\n",
      "Epoch 547 Batch  554/718   train_loss = 2.527\n",
      "Epoch 547 Batch  654/718   train_loss = 2.535\n",
      "Epoch 548 Batch   36/718   train_loss = 2.426\n",
      "Epoch 548 Batch  136/718   train_loss = 2.384\n",
      "Epoch 548 Batch  236/718   train_loss = 2.381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 548 Batch  336/718   train_loss = 2.521\n",
      "Epoch 548 Batch  436/718   train_loss = 2.443\n",
      "Epoch 548 Batch  536/718   train_loss = 2.497\n",
      "Epoch 548 Batch  636/718   train_loss = 2.510\n",
      "Epoch 549 Batch   18/718   train_loss = 2.453\n",
      "Epoch 549 Batch  118/718   train_loss = 2.434\n",
      "Epoch 549 Batch  218/718   train_loss = 2.498\n",
      "Epoch 549 Batch  318/718   train_loss = 2.451\n",
      "Epoch 549 Batch  418/718   train_loss = 2.574\n",
      "Epoch 549 Batch  518/718   train_loss = 2.499\n",
      "Epoch 549 Batch  618/718   train_loss = 2.419\n",
      "Epoch 550 Batch    0/718   train_loss = 2.548\n",
      "Epoch 550 Batch  100/718   train_loss = 2.454\n",
      "Epoch 550 Batch  200/718   train_loss = 2.348\n",
      "Epoch 550 Batch  300/718   train_loss = 2.424\n",
      "Epoch 550 Batch  400/718   train_loss = 2.496\n",
      "Epoch 550 Batch  500/718   train_loss = 2.592\n",
      "Epoch 550 Batch  600/718   train_loss = 2.539\n",
      "Epoch 550 Batch  700/718   train_loss = 2.399\n",
      "Epoch 551 Batch   82/718   train_loss = 2.390\n",
      "Epoch 551 Batch  182/718   train_loss = 2.530\n",
      "Epoch 551 Batch  282/718   train_loss = 2.502\n",
      "Epoch 551 Batch  382/718   train_loss = 2.360\n",
      "Epoch 551 Batch  482/718   train_loss = 2.530\n",
      "Epoch 551 Batch  582/718   train_loss = 2.375\n",
      "Epoch 551 Batch  682/718   train_loss = 2.414\n",
      "Epoch 552 Batch   64/718   train_loss = 2.426\n",
      "Epoch 552 Batch  164/718   train_loss = 2.338\n",
      "Epoch 552 Batch  264/718   train_loss = 2.433\n",
      "Epoch 552 Batch  364/718   train_loss = 2.573\n",
      "Epoch 552 Batch  464/718   train_loss = 2.353\n",
      "Epoch 552 Batch  564/718   train_loss = 2.483\n",
      "Epoch 552 Batch  664/718   train_loss = 2.539\n",
      "Epoch 553 Batch   46/718   train_loss = 2.425\n",
      "Epoch 553 Batch  146/718   train_loss = 2.321\n",
      "Epoch 553 Batch  246/718   train_loss = 2.557\n",
      "Epoch 553 Batch  346/718   train_loss = 2.334\n",
      "Epoch 553 Batch  446/718   train_loss = 2.381\n",
      "Epoch 553 Batch  546/718   train_loss = 2.373\n",
      "Epoch 553 Batch  646/718   train_loss = 2.533\n",
      "Epoch 554 Batch   28/718   train_loss = 2.391\n",
      "Epoch 554 Batch  128/718   train_loss = 2.410\n",
      "Epoch 554 Batch  228/718   train_loss = 2.492\n",
      "Epoch 554 Batch  328/718   train_loss = 2.429\n",
      "Epoch 554 Batch  428/718   train_loss = 2.331\n",
      "Epoch 554 Batch  528/718   train_loss = 2.535\n",
      "Epoch 554 Batch  628/718   train_loss = 2.537\n",
      "Epoch 555 Batch   10/718   train_loss = 2.377\n",
      "Epoch 555 Batch  110/718   train_loss = 2.445\n",
      "Epoch 555 Batch  210/718   train_loss = 2.390\n",
      "Epoch 555 Batch  310/718   train_loss = 2.550\n",
      "Epoch 555 Batch  410/718   train_loss = 2.299\n",
      "Epoch 555 Batch  510/718   train_loss = 2.508\n",
      "Epoch 555 Batch  610/718   train_loss = 2.470\n",
      "Epoch 555 Batch  710/718   train_loss = 2.386\n",
      "Epoch 556 Batch   92/718   train_loss = 2.407\n",
      "Epoch 556 Batch  192/718   train_loss = 2.434\n",
      "Epoch 556 Batch  292/718   train_loss = 2.307\n",
      "Epoch 556 Batch  392/718   train_loss = 2.393\n",
      "Epoch 556 Batch  492/718   train_loss = 2.492\n",
      "Epoch 556 Batch  592/718   train_loss = 2.461\n",
      "Epoch 556 Batch  692/718   train_loss = 2.431\n",
      "Epoch 557 Batch   74/718   train_loss = 2.477\n",
      "Epoch 557 Batch  174/718   train_loss = 2.536\n",
      "Epoch 557 Batch  274/718   train_loss = 2.480\n",
      "Epoch 557 Batch  374/718   train_loss = 2.380\n",
      "Epoch 557 Batch  474/718   train_loss = 2.492\n",
      "Epoch 557 Batch  574/718   train_loss = 2.323\n",
      "Epoch 557 Batch  674/718   train_loss = 2.427\n",
      "Epoch 558 Batch   56/718   train_loss = 2.545\n",
      "Epoch 558 Batch  156/718   train_loss = 2.384\n",
      "Epoch 558 Batch  256/718   train_loss = 2.351\n",
      "Epoch 558 Batch  356/718   train_loss = 2.374\n",
      "Epoch 558 Batch  456/718   train_loss = 2.428\n",
      "Epoch 558 Batch  556/718   train_loss = 2.328\n",
      "Epoch 558 Batch  656/718   train_loss = 2.389\n",
      "Epoch 559 Batch   38/718   train_loss = 2.525\n",
      "Epoch 559 Batch  138/718   train_loss = 2.475\n",
      "Epoch 559 Batch  238/718   train_loss = 2.458\n",
      "Epoch 559 Batch  338/718   train_loss = 2.443\n",
      "Epoch 559 Batch  438/718   train_loss = 2.339\n",
      "Epoch 559 Batch  538/718   train_loss = 2.404\n",
      "Epoch 559 Batch  638/718   train_loss = 2.388\n",
      "Epoch 560 Batch   20/718   train_loss = 2.450\n",
      "Epoch 560 Batch  120/718   train_loss = 2.402\n",
      "Epoch 560 Batch  220/718   train_loss = 2.440\n",
      "Epoch 560 Batch  320/718   train_loss = 2.517\n",
      "Epoch 560 Batch  420/718   train_loss = 2.557\n",
      "Epoch 560 Batch  520/718   train_loss = 2.478\n",
      "Epoch 560 Batch  620/718   train_loss = 2.548\n",
      "Epoch 561 Batch    2/718   train_loss = 2.465\n",
      "Epoch 561 Batch  102/718   train_loss = 2.351\n",
      "Epoch 561 Batch  202/718   train_loss = 2.412\n",
      "Epoch 561 Batch  302/718   train_loss = 2.522\n",
      "Epoch 561 Batch  402/718   train_loss = 2.480\n",
      "Epoch 561 Batch  502/718   train_loss = 2.470\n",
      "Epoch 561 Batch  602/718   train_loss = 2.426\n",
      "Epoch 561 Batch  702/718   train_loss = 2.408\n",
      "Epoch 562 Batch   84/718   train_loss = 2.349\n",
      "Epoch 562 Batch  184/718   train_loss = 2.427\n",
      "Epoch 562 Batch  284/718   train_loss = 2.371\n",
      "Epoch 562 Batch  384/718   train_loss = 2.427\n",
      "Epoch 562 Batch  484/718   train_loss = 2.418\n",
      "Epoch 562 Batch  584/718   train_loss = 2.389\n",
      "Epoch 562 Batch  684/718   train_loss = 2.427\n",
      "Epoch 563 Batch   66/718   train_loss = 2.409\n",
      "Epoch 563 Batch  166/718   train_loss = 2.433\n",
      "Epoch 563 Batch  266/718   train_loss = 2.485\n",
      "Epoch 563 Batch  366/718   train_loss = 2.543\n",
      "Epoch 563 Batch  466/718   train_loss = 2.448\n",
      "Epoch 563 Batch  566/718   train_loss = 2.413\n",
      "Epoch 563 Batch  666/718   train_loss = 2.381\n",
      "Epoch 564 Batch   48/718   train_loss = 2.355\n",
      "Epoch 564 Batch  148/718   train_loss = 2.410\n",
      "Epoch 564 Batch  248/718   train_loss = 2.462\n",
      "Epoch 564 Batch  348/718   train_loss = 2.366\n",
      "Epoch 564 Batch  448/718   train_loss = 2.450\n",
      "Epoch 564 Batch  548/718   train_loss = 2.491\n",
      "Epoch 564 Batch  648/718   train_loss = 2.445\n",
      "Epoch 565 Batch   30/718   train_loss = 2.395\n",
      "Epoch 565 Batch  130/718   train_loss = 2.393\n",
      "Epoch 565 Batch  230/718   train_loss = 2.506\n",
      "Epoch 565 Batch  330/718   train_loss = 2.506\n",
      "Epoch 565 Batch  430/718   train_loss = 2.535\n",
      "Epoch 565 Batch  530/718   train_loss = 2.425\n",
      "Epoch 565 Batch  630/718   train_loss = 2.404\n",
      "Epoch 566 Batch   12/718   train_loss = 2.324\n",
      "Epoch 566 Batch  112/718   train_loss = 2.473\n",
      "Epoch 566 Batch  212/718   train_loss = 2.484\n",
      "Epoch 566 Batch  312/718   train_loss = 2.460\n",
      "Epoch 566 Batch  412/718   train_loss = 2.373\n",
      "Epoch 566 Batch  512/718   train_loss = 2.451\n",
      "Epoch 566 Batch  612/718   train_loss = 2.513\n",
      "Epoch 566 Batch  712/718   train_loss = 2.524\n",
      "Epoch 567 Batch   94/718   train_loss = 2.280\n",
      "Epoch 567 Batch  194/718   train_loss = 2.314\n",
      "Epoch 567 Batch  294/718   train_loss = 2.526\n",
      "Epoch 567 Batch  394/718   train_loss = 2.405\n",
      "Epoch 567 Batch  494/718   train_loss = 2.585\n",
      "Epoch 567 Batch  594/718   train_loss = 2.511\n",
      "Epoch 567 Batch  694/718   train_loss = 2.513\n",
      "Epoch 568 Batch   76/718   train_loss = 2.382\n",
      "Epoch 568 Batch  176/718   train_loss = 2.512\n",
      "Epoch 568 Batch  276/718   train_loss = 2.435\n",
      "Epoch 568 Batch  376/718   train_loss = 2.462\n",
      "Epoch 568 Batch  476/718   train_loss = 2.508\n",
      "Epoch 568 Batch  576/718   train_loss = 2.486\n",
      "Epoch 568 Batch  676/718   train_loss = 2.426\n",
      "Epoch 569 Batch   58/718   train_loss = 2.430\n",
      "Epoch 569 Batch  158/718   train_loss = 2.502\n",
      "Epoch 569 Batch  258/718   train_loss = 2.402\n",
      "Epoch 569 Batch  358/718   train_loss = 2.374\n",
      "Epoch 569 Batch  458/718   train_loss = 2.516\n",
      "Epoch 569 Batch  558/718   train_loss = 2.532\n",
      "Epoch 569 Batch  658/718   train_loss = 2.335\n",
      "Epoch 570 Batch   40/718   train_loss = 2.497\n",
      "Epoch 570 Batch  140/718   train_loss = 2.398\n",
      "Epoch 570 Batch  240/718   train_loss = 2.309\n",
      "Epoch 570 Batch  340/718   train_loss = 2.498\n",
      "Epoch 570 Batch  440/718   train_loss = 2.426\n",
      "Epoch 570 Batch  540/718   train_loss = 2.469\n",
      "Epoch 570 Batch  640/718   train_loss = 2.419\n",
      "Epoch 571 Batch   22/718   train_loss = 2.225\n",
      "Epoch 571 Batch  122/718   train_loss = 2.447\n",
      "Epoch 571 Batch  222/718   train_loss = 2.446\n",
      "Epoch 571 Batch  322/718   train_loss = 2.520\n",
      "Epoch 571 Batch  422/718   train_loss = 2.467\n",
      "Epoch 571 Batch  522/718   train_loss = 2.439\n",
      "Epoch 571 Batch  622/718   train_loss = 2.479\n",
      "Epoch 572 Batch    4/718   train_loss = 2.350\n",
      "Epoch 572 Batch  104/718   train_loss = 2.461\n",
      "Epoch 572 Batch  204/718   train_loss = 2.349\n",
      "Epoch 572 Batch  304/718   train_loss = 2.395\n",
      "Epoch 572 Batch  404/718   train_loss = 2.390\n",
      "Epoch 572 Batch  504/718   train_loss = 2.590\n",
      "Epoch 572 Batch  604/718   train_loss = 2.338\n",
      "Epoch 572 Batch  704/718   train_loss = 2.414\n",
      "Epoch 573 Batch   86/718   train_loss = 2.415\n",
      "Epoch 573 Batch  186/718   train_loss = 2.373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 573 Batch  286/718   train_loss = 2.517\n",
      "Epoch 573 Batch  386/718   train_loss = 2.419\n",
      "Epoch 573 Batch  486/718   train_loss = 2.401\n",
      "Epoch 573 Batch  586/718   train_loss = 2.388\n",
      "Epoch 573 Batch  686/718   train_loss = 2.403\n",
      "Epoch 574 Batch   68/718   train_loss = 2.442\n",
      "Epoch 574 Batch  168/718   train_loss = 2.492\n",
      "Epoch 574 Batch  268/718   train_loss = 2.402\n",
      "Epoch 574 Batch  368/718   train_loss = 2.384\n",
      "Epoch 574 Batch  468/718   train_loss = 2.577\n",
      "Epoch 574 Batch  568/718   train_loss = 2.574\n",
      "Epoch 574 Batch  668/718   train_loss = 2.396\n",
      "Epoch 575 Batch   50/718   train_loss = 2.481\n",
      "Epoch 575 Batch  150/718   train_loss = 2.444\n",
      "Epoch 575 Batch  250/718   train_loss = 2.392\n",
      "Epoch 575 Batch  350/718   train_loss = 2.411\n",
      "Epoch 575 Batch  450/718   train_loss = 2.487\n",
      "Epoch 575 Batch  550/718   train_loss = 2.377\n",
      "Epoch 575 Batch  650/718   train_loss = 2.371\n",
      "Epoch 576 Batch   32/718   train_loss = 2.325\n",
      "Epoch 576 Batch  132/718   train_loss = 2.416\n",
      "Epoch 576 Batch  232/718   train_loss = 2.374\n",
      "Epoch 576 Batch  332/718   train_loss = 2.359\n",
      "Epoch 576 Batch  432/718   train_loss = 2.542\n",
      "Epoch 576 Batch  532/718   train_loss = 2.564\n",
      "Epoch 576 Batch  632/718   train_loss = 2.320\n",
      "Epoch 577 Batch   14/718   train_loss = 2.488\n",
      "Epoch 577 Batch  114/718   train_loss = 2.383\n",
      "Epoch 577 Batch  214/718   train_loss = 2.436\n",
      "Epoch 577 Batch  314/718   train_loss = 2.467\n",
      "Epoch 577 Batch  414/718   train_loss = 2.317\n",
      "Epoch 577 Batch  514/718   train_loss = 2.557\n",
      "Epoch 577 Batch  614/718   train_loss = 2.374\n",
      "Epoch 577 Batch  714/718   train_loss = 2.418\n",
      "Epoch 578 Batch   96/718   train_loss = 2.434\n",
      "Epoch 578 Batch  196/718   train_loss = 2.388\n",
      "Epoch 578 Batch  296/718   train_loss = 2.349\n",
      "Epoch 578 Batch  396/718   train_loss = 2.488\n",
      "Epoch 578 Batch  496/718   train_loss = 2.540\n",
      "Epoch 578 Batch  596/718   train_loss = 2.377\n",
      "Epoch 578 Batch  696/718   train_loss = 2.400\n",
      "Epoch 579 Batch   78/718   train_loss = 2.496\n",
      "Epoch 579 Batch  178/718   train_loss = 2.463\n",
      "Epoch 579 Batch  278/718   train_loss = 2.376\n",
      "Epoch 579 Batch  378/718   train_loss = 2.333\n",
      "Epoch 579 Batch  478/718   train_loss = 2.371\n",
      "Epoch 579 Batch  578/718   train_loss = 2.346\n",
      "Epoch 579 Batch  678/718   train_loss = 2.459\n",
      "Epoch 580 Batch   60/718   train_loss = 2.494\n",
      "Epoch 580 Batch  160/718   train_loss = 2.433\n",
      "Epoch 580 Batch  260/718   train_loss = 2.450\n",
      "Epoch 580 Batch  360/718   train_loss = 2.419\n",
      "Epoch 580 Batch  460/718   train_loss = 2.369\n",
      "Epoch 580 Batch  560/718   train_loss = 2.450\n",
      "Epoch 580 Batch  660/718   train_loss = 2.565\n",
      "Epoch 581 Batch   42/718   train_loss = 2.447\n",
      "Epoch 581 Batch  142/718   train_loss = 2.456\n",
      "Epoch 581 Batch  242/718   train_loss = 2.339\n",
      "Epoch 581 Batch  342/718   train_loss = 2.450\n",
      "Epoch 581 Batch  442/718   train_loss = 2.432\n",
      "Epoch 581 Batch  542/718   train_loss = 2.340\n",
      "Epoch 581 Batch  642/718   train_loss = 2.509\n",
      "Epoch 582 Batch   24/718   train_loss = 2.443\n",
      "Epoch 582 Batch  124/718   train_loss = 2.349\n",
      "Epoch 582 Batch  224/718   train_loss = 2.392\n",
      "Epoch 582 Batch  324/718   train_loss = 2.434\n",
      "Epoch 582 Batch  424/718   train_loss = 2.390\n",
      "Epoch 582 Batch  524/718   train_loss = 2.447\n",
      "Epoch 582 Batch  624/718   train_loss = 2.438\n",
      "Epoch 583 Batch    6/718   train_loss = 2.352\n",
      "Epoch 583 Batch  106/718   train_loss = 2.380\n",
      "Epoch 583 Batch  206/718   train_loss = 2.311\n",
      "Epoch 583 Batch  306/718   train_loss = 2.389\n",
      "Epoch 583 Batch  406/718   train_loss = 2.409\n",
      "Epoch 583 Batch  506/718   train_loss = 2.386\n",
      "Epoch 583 Batch  606/718   train_loss = 2.388\n",
      "Epoch 583 Batch  706/718   train_loss = 2.405\n",
      "Epoch 584 Batch   88/718   train_loss = 2.505\n",
      "Epoch 584 Batch  188/718   train_loss = 2.350\n",
      "Epoch 584 Batch  288/718   train_loss = 2.427\n",
      "Epoch 584 Batch  388/718   train_loss = 2.307\n",
      "Epoch 584 Batch  488/718   train_loss = 2.342\n",
      "Epoch 584 Batch  588/718   train_loss = 2.501\n",
      "Epoch 584 Batch  688/718   train_loss = 2.387\n",
      "Epoch 585 Batch   70/718   train_loss = 2.444\n",
      "Epoch 585 Batch  170/718   train_loss = 2.432\n",
      "Epoch 585 Batch  270/718   train_loss = 2.516\n",
      "Epoch 585 Batch  370/718   train_loss = 2.413\n",
      "Epoch 585 Batch  470/718   train_loss = 2.551\n",
      "Epoch 585 Batch  570/718   train_loss = 2.476\n",
      "Epoch 585 Batch  670/718   train_loss = 2.387\n",
      "Epoch 586 Batch   52/718   train_loss = 2.367\n",
      "Epoch 586 Batch  152/718   train_loss = 2.400\n",
      "Epoch 586 Batch  252/718   train_loss = 2.449\n",
      "Epoch 586 Batch  352/718   train_loss = 2.378\n",
      "Epoch 586 Batch  452/718   train_loss = 2.415\n",
      "Epoch 586 Batch  552/718   train_loss = 2.451\n",
      "Epoch 586 Batch  652/718   train_loss = 2.357\n",
      "Epoch 587 Batch   34/718   train_loss = 2.383\n",
      "Epoch 587 Batch  134/718   train_loss = 2.377\n",
      "Epoch 587 Batch  234/718   train_loss = 2.495\n",
      "Epoch 587 Batch  334/718   train_loss = 2.468\n",
      "Epoch 587 Batch  434/718   train_loss = 2.474\n",
      "Epoch 587 Batch  534/718   train_loss = 2.374\n",
      "Epoch 587 Batch  634/718   train_loss = 2.528\n",
      "Epoch 588 Batch   16/718   train_loss = 2.361\n",
      "Epoch 588 Batch  116/718   train_loss = 2.342\n",
      "Epoch 588 Batch  216/718   train_loss = 2.350\n",
      "Epoch 588 Batch  316/718   train_loss = 2.367\n",
      "Epoch 588 Batch  416/718   train_loss = 2.426\n",
      "Epoch 588 Batch  516/718   train_loss = 2.398\n",
      "Epoch 588 Batch  616/718   train_loss = 2.488\n",
      "Epoch 588 Batch  716/718   train_loss = 2.484\n",
      "Epoch 589 Batch   98/718   train_loss = 2.372\n",
      "Epoch 589 Batch  198/718   train_loss = 2.402\n",
      "Epoch 589 Batch  298/718   train_loss = 2.367\n",
      "Epoch 589 Batch  398/718   train_loss = 2.426\n",
      "Epoch 589 Batch  498/718   train_loss = 2.441\n",
      "Epoch 589 Batch  598/718   train_loss = 2.465\n",
      "Epoch 589 Batch  698/718   train_loss = 2.419\n",
      "Epoch 590 Batch   80/718   train_loss = 2.410\n",
      "Epoch 590 Batch  180/718   train_loss = 2.399\n",
      "Epoch 590 Batch  280/718   train_loss = 2.491\n",
      "Epoch 590 Batch  380/718   train_loss = 2.435\n",
      "Epoch 590 Batch  480/718   train_loss = 2.511\n",
      "Epoch 590 Batch  580/718   train_loss = 2.375\n",
      "Epoch 590 Batch  680/718   train_loss = 2.477\n",
      "Epoch 591 Batch   62/718   train_loss = 2.383\n",
      "Epoch 591 Batch  162/718   train_loss = 2.436\n",
      "Epoch 591 Batch  262/718   train_loss = 2.521\n",
      "Epoch 591 Batch  362/718   train_loss = 2.528\n",
      "Epoch 591 Batch  462/718   train_loss = 2.424\n",
      "Epoch 591 Batch  562/718   train_loss = 2.398\n",
      "Epoch 591 Batch  662/718   train_loss = 2.385\n",
      "Epoch 592 Batch   44/718   train_loss = 2.411\n",
      "Epoch 592 Batch  144/718   train_loss = 2.530\n",
      "Epoch 592 Batch  244/718   train_loss = 2.525\n",
      "Epoch 592 Batch  344/718   train_loss = 2.384\n",
      "Epoch 592 Batch  444/718   train_loss = 2.393\n",
      "Epoch 592 Batch  544/718   train_loss = 2.386\n",
      "Epoch 592 Batch  644/718   train_loss = 2.526\n",
      "Epoch 593 Batch   26/718   train_loss = 2.519\n",
      "Epoch 593 Batch  126/718   train_loss = 2.437\n",
      "Epoch 593 Batch  226/718   train_loss = 2.399\n",
      "Epoch 593 Batch  326/718   train_loss = 2.368\n",
      "Epoch 593 Batch  426/718   train_loss = 2.383\n",
      "Epoch 593 Batch  526/718   train_loss = 2.394\n",
      "Epoch 593 Batch  626/718   train_loss = 2.508\n",
      "Epoch 594 Batch    8/718   train_loss = 2.308\n",
      "Epoch 594 Batch  108/718   train_loss = 2.320\n",
      "Epoch 594 Batch  208/718   train_loss = 2.456\n",
      "Epoch 594 Batch  308/718   train_loss = 2.464\n",
      "Epoch 594 Batch  408/718   train_loss = 2.523\n",
      "Epoch 594 Batch  508/718   train_loss = 2.484\n",
      "Epoch 594 Batch  608/718   train_loss = 2.545\n",
      "Epoch 594 Batch  708/718   train_loss = 2.393\n",
      "Epoch 595 Batch   90/718   train_loss = 2.351\n",
      "Epoch 595 Batch  190/718   train_loss = 2.504\n",
      "Epoch 595 Batch  290/718   train_loss = 2.363\n",
      "Epoch 595 Batch  390/718   train_loss = 2.386\n",
      "Epoch 595 Batch  490/718   train_loss = 2.444\n",
      "Epoch 595 Batch  590/718   train_loss = 2.411\n",
      "Epoch 595 Batch  690/718   train_loss = 2.513\n",
      "Epoch 596 Batch   72/718   train_loss = 2.444\n",
      "Epoch 596 Batch  172/718   train_loss = 2.469\n",
      "Epoch 596 Batch  272/718   train_loss = 2.452\n",
      "Epoch 596 Batch  372/718   train_loss = 2.462\n",
      "Epoch 596 Batch  472/718   train_loss = 2.475\n",
      "Epoch 596 Batch  572/718   train_loss = 2.484\n",
      "Epoch 596 Batch  672/718   train_loss = 2.375\n",
      "Epoch 597 Batch   54/718   train_loss = 2.280\n",
      "Epoch 597 Batch  154/718   train_loss = 2.555\n",
      "Epoch 597 Batch  254/718   train_loss = 2.344\n",
      "Epoch 597 Batch  354/718   train_loss = 2.478\n",
      "Epoch 597 Batch  454/718   train_loss = 2.396\n",
      "Epoch 597 Batch  554/718   train_loss = 2.515\n",
      "Epoch 597 Batch  654/718   train_loss = 2.530\n",
      "Epoch 598 Batch   36/718   train_loss = 2.418\n",
      "Epoch 598 Batch  136/718   train_loss = 2.376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 598 Batch  236/718   train_loss = 2.353\n",
      "Epoch 598 Batch  336/718   train_loss = 2.471\n",
      "Epoch 598 Batch  436/718   train_loss = 2.438\n",
      "Epoch 598 Batch  536/718   train_loss = 2.488\n",
      "Epoch 598 Batch  636/718   train_loss = 2.488\n",
      "Epoch 599 Batch   18/718   train_loss = 2.421\n",
      "Epoch 599 Batch  118/718   train_loss = 2.410\n",
      "Epoch 599 Batch  218/718   train_loss = 2.433\n",
      "Epoch 599 Batch  318/718   train_loss = 2.463\n",
      "Epoch 599 Batch  418/718   train_loss = 2.578\n",
      "Epoch 599 Batch  518/718   train_loss = 2.437\n",
      "Epoch 599 Batch  618/718   train_loss = 2.419\n",
      "Epoch 600 Batch    0/718   train_loss = 2.482\n",
      "Epoch 600 Batch  100/718   train_loss = 2.452\n",
      "Epoch 600 Batch  200/718   train_loss = 2.379\n",
      "Epoch 600 Batch  300/718   train_loss = 2.359\n",
      "Epoch 600 Batch  400/718   train_loss = 2.495\n",
      "Epoch 600 Batch  500/718   train_loss = 2.600\n",
      "Epoch 600 Batch  600/718   train_loss = 2.524\n",
      "Epoch 600 Batch  700/718   train_loss = 2.386\n",
      "Epoch 601 Batch   82/718   train_loss = 2.356\n",
      "Epoch 601 Batch  182/718   train_loss = 2.490\n",
      "Epoch 601 Batch  282/718   train_loss = 2.500\n",
      "Epoch 601 Batch  382/718   train_loss = 2.386\n",
      "Epoch 601 Batch  482/718   train_loss = 2.572\n",
      "Epoch 601 Batch  582/718   train_loss = 2.361\n",
      "Epoch 601 Batch  682/718   train_loss = 2.441\n",
      "Epoch 602 Batch   64/718   train_loss = 2.401\n",
      "Epoch 602 Batch  164/718   train_loss = 2.311\n",
      "Epoch 602 Batch  264/718   train_loss = 2.434\n",
      "Epoch 602 Batch  364/718   train_loss = 2.558\n",
      "Epoch 602 Batch  464/718   train_loss = 2.325\n",
      "Epoch 602 Batch  564/718   train_loss = 2.459\n",
      "Epoch 602 Batch  664/718   train_loss = 2.527\n",
      "Epoch 603 Batch   46/718   train_loss = 2.341\n",
      "Epoch 603 Batch  146/718   train_loss = 2.347\n",
      "Epoch 603 Batch  246/718   train_loss = 2.510\n",
      "Epoch 603 Batch  346/718   train_loss = 2.355\n",
      "Epoch 603 Batch  446/718   train_loss = 2.408\n",
      "Epoch 603 Batch  546/718   train_loss = 2.333\n",
      "Epoch 603 Batch  646/718   train_loss = 2.490\n",
      "Epoch 604 Batch   28/718   train_loss = 2.380\n",
      "Epoch 604 Batch  128/718   train_loss = 2.409\n",
      "Epoch 604 Batch  228/718   train_loss = 2.488\n",
      "Epoch 604 Batch  328/718   train_loss = 2.334\n",
      "Epoch 604 Batch  428/718   train_loss = 2.330\n",
      "Epoch 604 Batch  528/718   train_loss = 2.511\n",
      "Epoch 604 Batch  628/718   train_loss = 2.524\n",
      "Epoch 605 Batch   10/718   train_loss = 2.363\n",
      "Epoch 605 Batch  110/718   train_loss = 2.422\n",
      "Epoch 605 Batch  210/718   train_loss = 2.362\n",
      "Epoch 605 Batch  310/718   train_loss = 2.536\n",
      "Epoch 605 Batch  410/718   train_loss = 2.294\n",
      "Epoch 605 Batch  510/718   train_loss = 2.447\n",
      "Epoch 605 Batch  610/718   train_loss = 2.441\n",
      "Epoch 605 Batch  710/718   train_loss = 2.373\n",
      "Epoch 606 Batch   92/718   train_loss = 2.342\n",
      "Epoch 606 Batch  192/718   train_loss = 2.421\n",
      "Epoch 606 Batch  292/718   train_loss = 2.307\n",
      "Epoch 606 Batch  392/718   train_loss = 2.406\n",
      "Epoch 606 Batch  492/718   train_loss = 2.464\n",
      "Epoch 606 Batch  592/718   train_loss = 2.492\n",
      "Epoch 606 Batch  692/718   train_loss = 2.445\n",
      "Epoch 607 Batch   74/718   train_loss = 2.474\n",
      "Epoch 607 Batch  174/718   train_loss = 2.529\n",
      "Epoch 607 Batch  274/718   train_loss = 2.465\n",
      "Epoch 607 Batch  374/718   train_loss = 2.364\n",
      "Epoch 607 Batch  474/718   train_loss = 2.465\n",
      "Epoch 607 Batch  574/718   train_loss = 2.330\n",
      "Epoch 607 Batch  674/718   train_loss = 2.369\n",
      "Epoch 608 Batch   56/718   train_loss = 2.519\n",
      "Epoch 608 Batch  156/718   train_loss = 2.360\n",
      "Epoch 608 Batch  256/718   train_loss = 2.342\n",
      "Epoch 608 Batch  356/718   train_loss = 2.340\n",
      "Epoch 608 Batch  456/718   train_loss = 2.397\n",
      "Epoch 608 Batch  556/718   train_loss = 2.368\n",
      "Epoch 608 Batch  656/718   train_loss = 2.384\n",
      "Epoch 609 Batch   38/718   train_loss = 2.482\n",
      "Epoch 609 Batch  138/718   train_loss = 2.468\n",
      "Epoch 609 Batch  238/718   train_loss = 2.487\n",
      "Epoch 609 Batch  338/718   train_loss = 2.480\n",
      "Epoch 609 Batch  438/718   train_loss = 2.299\n",
      "Epoch 609 Batch  538/718   train_loss = 2.350\n",
      "Epoch 609 Batch  638/718   train_loss = 2.372\n",
      "Epoch 610 Batch   20/718   train_loss = 2.430\n",
      "Epoch 610 Batch  120/718   train_loss = 2.388\n",
      "Epoch 610 Batch  220/718   train_loss = 2.427\n",
      "Epoch 610 Batch  320/718   train_loss = 2.471\n",
      "Epoch 610 Batch  420/718   train_loss = 2.571\n",
      "Epoch 610 Batch  520/718   train_loss = 2.473\n",
      "Epoch 610 Batch  620/718   train_loss = 2.573\n",
      "Epoch 611 Batch    2/718   train_loss = 2.429\n",
      "Epoch 611 Batch  102/718   train_loss = 2.340\n",
      "Epoch 611 Batch  202/718   train_loss = 2.438\n",
      "Epoch 611 Batch  302/718   train_loss = 2.517\n",
      "Epoch 611 Batch  402/718   train_loss = 2.486\n",
      "Epoch 611 Batch  502/718   train_loss = 2.482\n",
      "Epoch 611 Batch  602/718   train_loss = 2.397\n",
      "Epoch 611 Batch  702/718   train_loss = 2.407\n",
      "Epoch 612 Batch   84/718   train_loss = 2.349\n",
      "Epoch 612 Batch  184/718   train_loss = 2.410\n",
      "Epoch 612 Batch  284/718   train_loss = 2.394\n",
      "Epoch 612 Batch  384/718   train_loss = 2.373\n",
      "Epoch 612 Batch  484/718   train_loss = 2.395\n",
      "Epoch 612 Batch  584/718   train_loss = 2.366\n",
      "Epoch 612 Batch  684/718   train_loss = 2.421\n",
      "Epoch 613 Batch   66/718   train_loss = 2.348\n",
      "Epoch 613 Batch  166/718   train_loss = 2.365\n",
      "Epoch 613 Batch  266/718   train_loss = 2.465\n",
      "Epoch 613 Batch  366/718   train_loss = 2.543\n",
      "Epoch 613 Batch  466/718   train_loss = 2.426\n",
      "Epoch 613 Batch  566/718   train_loss = 2.373\n",
      "Epoch 613 Batch  666/718   train_loss = 2.367\n",
      "Epoch 614 Batch   48/718   train_loss = 2.303\n",
      "Epoch 614 Batch  148/718   train_loss = 2.419\n",
      "Epoch 614 Batch  248/718   train_loss = 2.424\n",
      "Epoch 614 Batch  348/718   train_loss = 2.367\n",
      "Epoch 614 Batch  448/718   train_loss = 2.424\n",
      "Epoch 614 Batch  548/718   train_loss = 2.489\n",
      "Epoch 614 Batch  648/718   train_loss = 2.411\n",
      "Epoch 615 Batch   30/718   train_loss = 2.419\n",
      "Epoch 615 Batch  130/718   train_loss = 2.333\n",
      "Epoch 615 Batch  230/718   train_loss = 2.441\n",
      "Epoch 615 Batch  330/718   train_loss = 2.509\n",
      "Epoch 615 Batch  430/718   train_loss = 2.538\n",
      "Epoch 615 Batch  530/718   train_loss = 2.388\n",
      "Epoch 615 Batch  630/718   train_loss = 2.349\n",
      "Epoch 616 Batch   12/718   train_loss = 2.290\n",
      "Epoch 616 Batch  112/718   train_loss = 2.433\n",
      "Epoch 616 Batch  212/718   train_loss = 2.496\n",
      "Epoch 616 Batch  312/718   train_loss = 2.445\n",
      "Epoch 616 Batch  412/718   train_loss = 2.341\n",
      "Epoch 616 Batch  512/718   train_loss = 2.411\n",
      "Epoch 616 Batch  612/718   train_loss = 2.479\n",
      "Epoch 616 Batch  712/718   train_loss = 2.472\n",
      "Epoch 617 Batch   94/718   train_loss = 2.285\n",
      "Epoch 617 Batch  194/718   train_loss = 2.322\n",
      "Epoch 617 Batch  294/718   train_loss = 2.443\n",
      "Epoch 617 Batch  394/718   train_loss = 2.404\n",
      "Epoch 617 Batch  494/718   train_loss = 2.588\n",
      "Epoch 617 Batch  594/718   train_loss = 2.453\n",
      "Epoch 617 Batch  694/718   train_loss = 2.541\n",
      "Epoch 618 Batch   76/718   train_loss = 2.351\n",
      "Epoch 618 Batch  176/718   train_loss = 2.438\n",
      "Epoch 618 Batch  276/718   train_loss = 2.409\n",
      "Epoch 618 Batch  376/718   train_loss = 2.435\n",
      "Epoch 618 Batch  476/718   train_loss = 2.454\n",
      "Epoch 618 Batch  576/718   train_loss = 2.503\n",
      "Epoch 618 Batch  676/718   train_loss = 2.405\n",
      "Epoch 619 Batch   58/718   train_loss = 2.408\n",
      "Epoch 619 Batch  158/718   train_loss = 2.454\n",
      "Epoch 619 Batch  258/718   train_loss = 2.390\n",
      "Epoch 619 Batch  358/718   train_loss = 2.345\n",
      "Epoch 619 Batch  458/718   train_loss = 2.508\n",
      "Epoch 619 Batch  558/718   train_loss = 2.528\n",
      "Epoch 619 Batch  658/718   train_loss = 2.337\n",
      "Epoch 620 Batch   40/718   train_loss = 2.435\n",
      "Epoch 620 Batch  140/718   train_loss = 2.381\n",
      "Epoch 620 Batch  240/718   train_loss = 2.321\n",
      "Epoch 620 Batch  340/718   train_loss = 2.461\n",
      "Epoch 620 Batch  440/718   train_loss = 2.380\n",
      "Epoch 620 Batch  540/718   train_loss = 2.435\n",
      "Epoch 620 Batch  640/718   train_loss = 2.403\n",
      "Epoch 621 Batch   22/718   train_loss = 2.224\n",
      "Epoch 621 Batch  122/718   train_loss = 2.428\n",
      "Epoch 621 Batch  222/718   train_loss = 2.400\n",
      "Epoch 621 Batch  322/718   train_loss = 2.457\n",
      "Epoch 621 Batch  422/718   train_loss = 2.459\n",
      "Epoch 621 Batch  522/718   train_loss = 2.422\n",
      "Epoch 621 Batch  622/718   train_loss = 2.495\n",
      "Epoch 622 Batch    4/718   train_loss = 2.295\n",
      "Epoch 622 Batch  104/718   train_loss = 2.422\n",
      "Epoch 622 Batch  204/718   train_loss = 2.278\n",
      "Epoch 622 Batch  304/718   train_loss = 2.380\n",
      "Epoch 622 Batch  404/718   train_loss = 2.387\n",
      "Epoch 622 Batch  504/718   train_loss = 2.553\n",
      "Epoch 622 Batch  604/718   train_loss = 2.311\n",
      "Epoch 622 Batch  704/718   train_loss = 2.423\n",
      "Epoch 623 Batch   86/718   train_loss = 2.395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 623 Batch  186/718   train_loss = 2.374\n",
      "Epoch 623 Batch  286/718   train_loss = 2.464\n",
      "Epoch 623 Batch  386/718   train_loss = 2.426\n",
      "Epoch 623 Batch  486/718   train_loss = 2.423\n",
      "Epoch 623 Batch  586/718   train_loss = 2.392\n",
      "Epoch 623 Batch  686/718   train_loss = 2.401\n",
      "Epoch 624 Batch   68/718   train_loss = 2.405\n",
      "Epoch 624 Batch  168/718   train_loss = 2.443\n",
      "Epoch 624 Batch  268/718   train_loss = 2.394\n",
      "Epoch 624 Batch  368/718   train_loss = 2.349\n",
      "Epoch 624 Batch  468/718   train_loss = 2.625\n",
      "Epoch 624 Batch  568/718   train_loss = 2.566\n",
      "Epoch 624 Batch  668/718   train_loss = 2.406\n",
      "Epoch 625 Batch   50/718   train_loss = 2.495\n",
      "Epoch 625 Batch  150/718   train_loss = 2.369\n",
      "Epoch 625 Batch  250/718   train_loss = 2.394\n",
      "Epoch 625 Batch  350/718   train_loss = 2.401\n",
      "Epoch 625 Batch  450/718   train_loss = 2.423\n",
      "Epoch 625 Batch  550/718   train_loss = 2.343\n",
      "Epoch 625 Batch  650/718   train_loss = 2.346\n",
      "Epoch 626 Batch   32/718   train_loss = 2.343\n",
      "Epoch 626 Batch  132/718   train_loss = 2.417\n",
      "Epoch 626 Batch  232/718   train_loss = 2.369\n",
      "Epoch 626 Batch  332/718   train_loss = 2.321\n",
      "Epoch 626 Batch  432/718   train_loss = 2.517\n",
      "Epoch 626 Batch  532/718   train_loss = 2.547\n",
      "Epoch 626 Batch  632/718   train_loss = 2.293\n",
      "Epoch 627 Batch   14/718   train_loss = 2.419\n",
      "Epoch 627 Batch  114/718   train_loss = 2.351\n",
      "Epoch 627 Batch  214/718   train_loss = 2.478\n",
      "Epoch 627 Batch  314/718   train_loss = 2.429\n",
      "Epoch 627 Batch  414/718   train_loss = 2.320\n",
      "Epoch 627 Batch  514/718   train_loss = 2.517\n",
      "Epoch 627 Batch  614/718   train_loss = 2.355\n",
      "Epoch 627 Batch  714/718   train_loss = 2.407\n",
      "Epoch 628 Batch   96/718   train_loss = 2.435\n",
      "Epoch 628 Batch  196/718   train_loss = 2.349\n",
      "Epoch 628 Batch  296/718   train_loss = 2.329\n",
      "Epoch 628 Batch  396/718   train_loss = 2.458\n",
      "Epoch 628 Batch  496/718   train_loss = 2.503\n",
      "Epoch 628 Batch  596/718   train_loss = 2.391\n",
      "Epoch 628 Batch  696/718   train_loss = 2.392\n",
      "Epoch 629 Batch   78/718   train_loss = 2.436\n",
      "Epoch 629 Batch  178/718   train_loss = 2.432\n",
      "Epoch 629 Batch  278/718   train_loss = 2.363\n",
      "Epoch 629 Batch  378/718   train_loss = 2.351\n",
      "Epoch 629 Batch  478/718   train_loss = 2.336\n",
      "Epoch 629 Batch  578/718   train_loss = 2.339\n",
      "Epoch 629 Batch  678/718   train_loss = 2.458\n",
      "Epoch 630 Batch   60/718   train_loss = 2.448\n",
      "Epoch 630 Batch  160/718   train_loss = 2.421\n",
      "Epoch 630 Batch  260/718   train_loss = 2.439\n",
      "Epoch 630 Batch  360/718   train_loss = 2.379\n",
      "Epoch 630 Batch  460/718   train_loss = 2.341\n",
      "Epoch 630 Batch  560/718   train_loss = 2.431\n",
      "Epoch 630 Batch  660/718   train_loss = 2.516\n",
      "Epoch 631 Batch   42/718   train_loss = 2.455\n",
      "Epoch 631 Batch  142/718   train_loss = 2.452\n",
      "Epoch 631 Batch  242/718   train_loss = 2.307\n",
      "Epoch 631 Batch  342/718   train_loss = 2.424\n",
      "Epoch 631 Batch  442/718   train_loss = 2.388\n",
      "Epoch 631 Batch  542/718   train_loss = 2.308\n",
      "Epoch 631 Batch  642/718   train_loss = 2.514\n",
      "Epoch 632 Batch   24/718   train_loss = 2.382\n",
      "Epoch 632 Batch  124/718   train_loss = 2.304\n",
      "Epoch 632 Batch  224/718   train_loss = 2.388\n",
      "Epoch 632 Batch  324/718   train_loss = 2.413\n",
      "Epoch 632 Batch  424/718   train_loss = 2.404\n",
      "Epoch 632 Batch  524/718   train_loss = 2.443\n",
      "Epoch 632 Batch  624/718   train_loss = 2.418\n",
      "Epoch 633 Batch    6/718   train_loss = 2.335\n",
      "Epoch 633 Batch  106/718   train_loss = 2.382\n",
      "Epoch 633 Batch  206/718   train_loss = 2.300\n",
      "Epoch 633 Batch  306/718   train_loss = 2.369\n",
      "Epoch 633 Batch  406/718   train_loss = 2.394\n",
      "Epoch 633 Batch  506/718   train_loss = 2.375\n",
      "Epoch 633 Batch  606/718   train_loss = 2.327\n",
      "Epoch 633 Batch  706/718   train_loss = 2.415\n",
      "Epoch 634 Batch   88/718   train_loss = 2.503\n",
      "Epoch 634 Batch  188/718   train_loss = 2.360\n",
      "Epoch 634 Batch  288/718   train_loss = 2.346\n",
      "Epoch 634 Batch  388/718   train_loss = 2.293\n",
      "Epoch 634 Batch  488/718   train_loss = 2.303\n",
      "Epoch 634 Batch  588/718   train_loss = 2.476\n",
      "Epoch 634 Batch  688/718   train_loss = 2.397\n",
      "Epoch 635 Batch   70/718   train_loss = 2.393\n",
      "Epoch 635 Batch  170/718   train_loss = 2.447\n",
      "Epoch 635 Batch  270/718   train_loss = 2.448\n",
      "Epoch 635 Batch  370/718   train_loss = 2.411\n",
      "Epoch 635 Batch  470/718   train_loss = 2.570\n",
      "Epoch 635 Batch  570/718   train_loss = 2.448\n",
      "Epoch 635 Batch  670/718   train_loss = 2.398\n",
      "Epoch 636 Batch   52/718   train_loss = 2.338\n",
      "Epoch 636 Batch  152/718   train_loss = 2.380\n",
      "Epoch 636 Batch  252/718   train_loss = 2.443\n",
      "Epoch 636 Batch  352/718   train_loss = 2.395\n",
      "Epoch 636 Batch  452/718   train_loss = 2.458\n",
      "Epoch 636 Batch  552/718   train_loss = 2.453\n",
      "Epoch 636 Batch  652/718   train_loss = 2.332\n",
      "Epoch 637 Batch   34/718   train_loss = 2.386\n",
      "Epoch 637 Batch  134/718   train_loss = 2.350\n",
      "Epoch 637 Batch  234/718   train_loss = 2.492\n",
      "Epoch 637 Batch  334/718   train_loss = 2.498\n",
      "Epoch 637 Batch  434/718   train_loss = 2.447\n",
      "Epoch 637 Batch  534/718   train_loss = 2.390\n",
      "Epoch 637 Batch  634/718   train_loss = 2.520\n",
      "Epoch 638 Batch   16/718   train_loss = 2.401\n",
      "Epoch 638 Batch  116/718   train_loss = 2.345\n",
      "Epoch 638 Batch  216/718   train_loss = 2.376\n",
      "Epoch 638 Batch  316/718   train_loss = 2.330\n",
      "Epoch 638 Batch  416/718   train_loss = 2.414\n",
      "Epoch 638 Batch  516/718   train_loss = 2.404\n",
      "Epoch 638 Batch  616/718   train_loss = 2.492\n",
      "Epoch 638 Batch  716/718   train_loss = 2.459\n",
      "Epoch 639 Batch   98/718   train_loss = 2.326\n",
      "Epoch 639 Batch  198/718   train_loss = 2.426\n",
      "Epoch 639 Batch  298/718   train_loss = 2.347\n",
      "Epoch 639 Batch  398/718   train_loss = 2.387\n",
      "Epoch 639 Batch  498/718   train_loss = 2.391\n",
      "Epoch 639 Batch  598/718   train_loss = 2.443\n",
      "Epoch 639 Batch  698/718   train_loss = 2.474\n",
      "Epoch 640 Batch   80/718   train_loss = 2.431\n",
      "Epoch 640 Batch  180/718   train_loss = 2.354\n",
      "Epoch 640 Batch  280/718   train_loss = 2.472\n",
      "Epoch 640 Batch  380/718   train_loss = 2.400\n",
      "Epoch 640 Batch  480/718   train_loss = 2.473\n",
      "Epoch 640 Batch  580/718   train_loss = 2.342\n",
      "Epoch 640 Batch  680/718   train_loss = 2.477\n",
      "Epoch 641 Batch   62/718   train_loss = 2.352\n",
      "Epoch 641 Batch  162/718   train_loss = 2.403\n",
      "Epoch 641 Batch  262/718   train_loss = 2.480\n",
      "Epoch 641 Batch  362/718   train_loss = 2.489\n",
      "Epoch 641 Batch  462/718   train_loss = 2.451\n",
      "Epoch 641 Batch  562/718   train_loss = 2.373\n",
      "Epoch 641 Batch  662/718   train_loss = 2.402\n",
      "Epoch 642 Batch   44/718   train_loss = 2.396\n",
      "Epoch 642 Batch  144/718   train_loss = 2.487\n",
      "Epoch 642 Batch  244/718   train_loss = 2.492\n",
      "Epoch 642 Batch  344/718   train_loss = 2.411\n",
      "Epoch 642 Batch  444/718   train_loss = 2.349\n",
      "Epoch 642 Batch  544/718   train_loss = 2.396\n",
      "Epoch 642 Batch  644/718   train_loss = 2.506\n",
      "Epoch 643 Batch   26/718   train_loss = 2.507\n",
      "Epoch 643 Batch  126/718   train_loss = 2.408\n",
      "Epoch 643 Batch  226/718   train_loss = 2.394\n",
      "Epoch 643 Batch  326/718   train_loss = 2.390\n",
      "Epoch 643 Batch  426/718   train_loss = 2.389\n",
      "Epoch 643 Batch  526/718   train_loss = 2.424\n",
      "Epoch 643 Batch  626/718   train_loss = 2.501\n",
      "Epoch 644 Batch    8/718   train_loss = 2.327\n",
      "Epoch 644 Batch  108/718   train_loss = 2.318\n",
      "Epoch 644 Batch  208/718   train_loss = 2.435\n",
      "Epoch 644 Batch  308/718   train_loss = 2.438\n",
      "Epoch 644 Batch  408/718   train_loss = 2.525\n",
      "Epoch 644 Batch  508/718   train_loss = 2.497\n",
      "Epoch 644 Batch  608/718   train_loss = 2.578\n",
      "Epoch 644 Batch  708/718   train_loss = 2.364\n",
      "Epoch 645 Batch   90/718   train_loss = 2.309\n",
      "Epoch 645 Batch  190/718   train_loss = 2.485\n",
      "Epoch 645 Batch  290/718   train_loss = 2.327\n",
      "Epoch 645 Batch  390/718   train_loss = 2.369\n",
      "Epoch 645 Batch  490/718   train_loss = 2.393\n",
      "Epoch 645 Batch  590/718   train_loss = 2.405\n",
      "Epoch 645 Batch  690/718   train_loss = 2.481\n",
      "Epoch 646 Batch   72/718   train_loss = 2.445\n",
      "Epoch 646 Batch  172/718   train_loss = 2.462\n",
      "Epoch 646 Batch  272/718   train_loss = 2.443\n",
      "Epoch 646 Batch  372/718   train_loss = 2.483\n",
      "Epoch 646 Batch  472/718   train_loss = 2.495\n",
      "Epoch 646 Batch  572/718   train_loss = 2.465\n",
      "Epoch 646 Batch  672/718   train_loss = 2.345\n",
      "Epoch 647 Batch   54/718   train_loss = 2.290\n",
      "Epoch 647 Batch  154/718   train_loss = 2.526\n",
      "Epoch 647 Batch  254/718   train_loss = 2.350\n",
      "Epoch 647 Batch  354/718   train_loss = 2.402\n",
      "Epoch 647 Batch  454/718   train_loss = 2.307\n",
      "Epoch 647 Batch  554/718   train_loss = 2.512\n",
      "Epoch 647 Batch  654/718   train_loss = 2.460\n",
      "Epoch 648 Batch   36/718   train_loss = 2.377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 648 Batch  136/718   train_loss = 2.334\n",
      "Epoch 648 Batch  236/718   train_loss = 2.362\n",
      "Epoch 648 Batch  336/718   train_loss = 2.465\n",
      "Epoch 648 Batch  436/718   train_loss = 2.408\n",
      "Epoch 648 Batch  536/718   train_loss = 2.491\n",
      "Epoch 648 Batch  636/718   train_loss = 2.465\n",
      "Epoch 649 Batch   18/718   train_loss = 2.409\n",
      "Epoch 649 Batch  118/718   train_loss = 2.422\n",
      "Epoch 649 Batch  218/718   train_loss = 2.452\n",
      "Epoch 649 Batch  318/718   train_loss = 2.453\n",
      "Epoch 649 Batch  418/718   train_loss = 2.573\n",
      "Epoch 649 Batch  518/718   train_loss = 2.475\n",
      "Epoch 649 Batch  618/718   train_loss = 2.392\n",
      "Epoch 650 Batch    0/718   train_loss = 2.453\n",
      "Epoch 650 Batch  100/718   train_loss = 2.437\n",
      "Epoch 650 Batch  200/718   train_loss = 2.316\n",
      "Epoch 650 Batch  300/718   train_loss = 2.384\n",
      "Epoch 650 Batch  400/718   train_loss = 2.456\n",
      "Epoch 650 Batch  500/718   train_loss = 2.573\n",
      "Epoch 650 Batch  600/718   train_loss = 2.529\n",
      "Epoch 650 Batch  700/718   train_loss = 2.365\n",
      "Epoch 651 Batch   82/718   train_loss = 2.344\n",
      "Epoch 651 Batch  182/718   train_loss = 2.487\n",
      "Epoch 651 Batch  282/718   train_loss = 2.450\n",
      "Epoch 651 Batch  382/718   train_loss = 2.372\n",
      "Epoch 651 Batch  482/718   train_loss = 2.535\n",
      "Epoch 651 Batch  582/718   train_loss = 2.349\n",
      "Epoch 651 Batch  682/718   train_loss = 2.398\n",
      "Epoch 652 Batch   64/718   train_loss = 2.416\n",
      "Epoch 652 Batch  164/718   train_loss = 2.260\n",
      "Epoch 652 Batch  264/718   train_loss = 2.426\n",
      "Epoch 652 Batch  364/718   train_loss = 2.539\n",
      "Epoch 652 Batch  464/718   train_loss = 2.345\n",
      "Epoch 652 Batch  564/718   train_loss = 2.464\n",
      "Epoch 652 Batch  664/718   train_loss = 2.483\n",
      "Epoch 653 Batch   46/718   train_loss = 2.350\n",
      "Epoch 653 Batch  146/718   train_loss = 2.298\n",
      "Epoch 653 Batch  246/718   train_loss = 2.498\n",
      "Epoch 653 Batch  346/718   train_loss = 2.319\n",
      "Epoch 653 Batch  446/718   train_loss = 2.336\n",
      "Epoch 653 Batch  546/718   train_loss = 2.327\n",
      "Epoch 653 Batch  646/718   train_loss = 2.462\n",
      "Epoch 654 Batch   28/718   train_loss = 2.379\n",
      "Epoch 654 Batch  128/718   train_loss = 2.394\n",
      "Epoch 654 Batch  228/718   train_loss = 2.473\n",
      "Epoch 654 Batch  328/718   train_loss = 2.373\n",
      "Epoch 654 Batch  428/718   train_loss = 2.271\n",
      "Epoch 654 Batch  528/718   train_loss = 2.453\n",
      "Epoch 654 Batch  628/718   train_loss = 2.535\n",
      "Epoch 655 Batch   10/718   train_loss = 2.339\n",
      "Epoch 655 Batch  110/718   train_loss = 2.413\n",
      "Epoch 655 Batch  210/718   train_loss = 2.354\n",
      "Epoch 655 Batch  310/718   train_loss = 2.493\n",
      "Epoch 655 Batch  410/718   train_loss = 2.283\n",
      "Epoch 655 Batch  510/718   train_loss = 2.469\n",
      "Epoch 655 Batch  610/718   train_loss = 2.461\n",
      "Epoch 655 Batch  710/718   train_loss = 2.373\n",
      "Epoch 656 Batch   92/718   train_loss = 2.346\n",
      "Epoch 656 Batch  192/718   train_loss = 2.385\n",
      "Epoch 656 Batch  292/718   train_loss = 2.283\n",
      "Epoch 656 Batch  392/718   train_loss = 2.390\n",
      "Epoch 656 Batch  492/718   train_loss = 2.417\n",
      "Epoch 656 Batch  592/718   train_loss = 2.465\n",
      "Epoch 656 Batch  692/718   train_loss = 2.393\n",
      "Epoch 657 Batch   74/718   train_loss = 2.499\n",
      "Epoch 657 Batch  174/718   train_loss = 2.506\n",
      "Epoch 657 Batch  274/718   train_loss = 2.443\n",
      "Epoch 657 Batch  374/718   train_loss = 2.358\n",
      "Epoch 657 Batch  474/718   train_loss = 2.425\n",
      "Epoch 657 Batch  574/718   train_loss = 2.279\n",
      "Epoch 657 Batch  674/718   train_loss = 2.346\n",
      "Epoch 658 Batch   56/718   train_loss = 2.530\n",
      "Epoch 658 Batch  156/718   train_loss = 2.356\n",
      "Epoch 658 Batch  256/718   train_loss = 2.291\n",
      "Epoch 658 Batch  356/718   train_loss = 2.331\n",
      "Epoch 658 Batch  456/718   train_loss = 2.365\n",
      "Epoch 658 Batch  556/718   train_loss = 2.336\n",
      "Epoch 658 Batch  656/718   train_loss = 2.346\n",
      "Epoch 659 Batch   38/718   train_loss = 2.441\n",
      "Epoch 659 Batch  138/718   train_loss = 2.449\n",
      "Epoch 659 Batch  238/718   train_loss = 2.478\n",
      "Epoch 659 Batch  338/718   train_loss = 2.417\n",
      "Epoch 659 Batch  438/718   train_loss = 2.311\n",
      "Epoch 659 Batch  538/718   train_loss = 2.380\n",
      "Epoch 659 Batch  638/718   train_loss = 2.343\n",
      "Epoch 660 Batch   20/718   train_loss = 2.426\n",
      "Epoch 660 Batch  120/718   train_loss = 2.363\n",
      "Epoch 660 Batch  220/718   train_loss = 2.401\n",
      "Epoch 660 Batch  320/718   train_loss = 2.443\n",
      "Epoch 660 Batch  420/718   train_loss = 2.536\n",
      "Epoch 660 Batch  520/718   train_loss = 2.470\n",
      "Epoch 660 Batch  620/718   train_loss = 2.580\n",
      "Epoch 661 Batch    2/718   train_loss = 2.445\n",
      "Epoch 661 Batch  102/718   train_loss = 2.286\n",
      "Epoch 661 Batch  202/718   train_loss = 2.389\n",
      "Epoch 661 Batch  302/718   train_loss = 2.491\n",
      "Epoch 661 Batch  402/718   train_loss = 2.464\n",
      "Epoch 661 Batch  502/718   train_loss = 2.442\n",
      "Epoch 661 Batch  602/718   train_loss = 2.393\n",
      "Epoch 661 Batch  702/718   train_loss = 2.340\n",
      "Epoch 662 Batch   84/718   train_loss = 2.305\n",
      "Epoch 662 Batch  184/718   train_loss = 2.408\n",
      "Epoch 662 Batch  284/718   train_loss = 2.376\n",
      "Epoch 662 Batch  384/718   train_loss = 2.379\n",
      "Epoch 662 Batch  484/718   train_loss = 2.382\n",
      "Epoch 662 Batch  584/718   train_loss = 2.346\n",
      "Epoch 662 Batch  684/718   train_loss = 2.415\n",
      "Epoch 663 Batch   66/718   train_loss = 2.331\n",
      "Epoch 663 Batch  166/718   train_loss = 2.369\n",
      "Epoch 663 Batch  266/718   train_loss = 2.413\n",
      "Epoch 663 Batch  366/718   train_loss = 2.546\n",
      "Epoch 663 Batch  466/718   train_loss = 2.391\n",
      "Epoch 663 Batch  566/718   train_loss = 2.343\n",
      "Epoch 663 Batch  666/718   train_loss = 2.339\n",
      "Epoch 664 Batch   48/718   train_loss = 2.316\n",
      "Epoch 664 Batch  148/718   train_loss = 2.402\n",
      "Epoch 664 Batch  248/718   train_loss = 2.447\n",
      "Epoch 664 Batch  348/718   train_loss = 2.325\n",
      "Epoch 664 Batch  448/718   train_loss = 2.411\n",
      "Epoch 664 Batch  548/718   train_loss = 2.460\n",
      "Epoch 664 Batch  648/718   train_loss = 2.391\n",
      "Epoch 665 Batch   30/718   train_loss = 2.367\n",
      "Epoch 665 Batch  130/718   train_loss = 2.333\n",
      "Epoch 665 Batch  230/718   train_loss = 2.459\n",
      "Epoch 665 Batch  330/718   train_loss = 2.478\n",
      "Epoch 665 Batch  430/718   train_loss = 2.489\n",
      "Epoch 665 Batch  530/718   train_loss = 2.368\n",
      "Epoch 665 Batch  630/718   train_loss = 2.349\n",
      "Epoch 666 Batch   12/718   train_loss = 2.300\n",
      "Epoch 666 Batch  112/718   train_loss = 2.425\n",
      "Epoch 666 Batch  212/718   train_loss = 2.454\n",
      "Epoch 666 Batch  312/718   train_loss = 2.406\n",
      "Epoch 666 Batch  412/718   train_loss = 2.319\n",
      "Epoch 666 Batch  512/718   train_loss = 2.389\n",
      "Epoch 666 Batch  612/718   train_loss = 2.501\n",
      "Epoch 666 Batch  712/718   train_loss = 2.451\n",
      "Epoch 667 Batch   94/718   train_loss = 2.269\n",
      "Epoch 667 Batch  194/718   train_loss = 2.339\n",
      "Epoch 667 Batch  294/718   train_loss = 2.462\n",
      "Epoch 667 Batch  394/718   train_loss = 2.373\n",
      "Epoch 667 Batch  494/718   train_loss = 2.515\n",
      "Epoch 667 Batch  594/718   train_loss = 2.496\n",
      "Epoch 667 Batch  694/718   train_loss = 2.529\n",
      "Epoch 668 Batch   76/718   train_loss = 2.352\n",
      "Epoch 668 Batch  176/718   train_loss = 2.462\n",
      "Epoch 668 Batch  276/718   train_loss = 2.399\n",
      "Epoch 668 Batch  376/718   train_loss = 2.400\n",
      "Epoch 668 Batch  476/718   train_loss = 2.414\n",
      "Epoch 668 Batch  576/718   train_loss = 2.439\n",
      "Epoch 668 Batch  676/718   train_loss = 2.385\n",
      "Epoch 669 Batch   58/718   train_loss = 2.422\n",
      "Epoch 669 Batch  158/718   train_loss = 2.377\n",
      "Epoch 669 Batch  258/718   train_loss = 2.403\n",
      "Epoch 669 Batch  358/718   train_loss = 2.339\n",
      "Epoch 669 Batch  458/718   train_loss = 2.474\n",
      "Epoch 669 Batch  558/718   train_loss = 2.515\n",
      "Epoch 669 Batch  658/718   train_loss = 2.335\n",
      "Epoch 670 Batch   40/718   train_loss = 2.404\n",
      "Epoch 670 Batch  140/718   train_loss = 2.348\n",
      "Epoch 670 Batch  240/718   train_loss = 2.295\n",
      "Epoch 670 Batch  340/718   train_loss = 2.427\n",
      "Epoch 670 Batch  440/718   train_loss = 2.349\n",
      "Epoch 670 Batch  540/718   train_loss = 2.494\n",
      "Epoch 670 Batch  640/718   train_loss = 2.365\n",
      "Epoch 671 Batch   22/718   train_loss = 2.233\n",
      "Epoch 671 Batch  122/718   train_loss = 2.380\n",
      "Epoch 671 Batch  222/718   train_loss = 2.379\n",
      "Epoch 671 Batch  322/718   train_loss = 2.455\n",
      "Epoch 671 Batch  422/718   train_loss = 2.452\n",
      "Epoch 671 Batch  522/718   train_loss = 2.357\n",
      "Epoch 671 Batch  622/718   train_loss = 2.473\n",
      "Epoch 672 Batch    4/718   train_loss = 2.350\n",
      "Epoch 672 Batch  104/718   train_loss = 2.412\n",
      "Epoch 672 Batch  204/718   train_loss = 2.301\n",
      "Epoch 672 Batch  304/718   train_loss = 2.371\n",
      "Epoch 672 Batch  404/718   train_loss = 2.348\n",
      "Epoch 672 Batch  504/718   train_loss = 2.540\n",
      "Epoch 672 Batch  604/718   train_loss = 2.352\n",
      "Epoch 672 Batch  704/718   train_loss = 2.359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 673 Batch   86/718   train_loss = 2.409\n",
      "Epoch 673 Batch  186/718   train_loss = 2.387\n",
      "Epoch 673 Batch  286/718   train_loss = 2.496\n",
      "Epoch 673 Batch  386/718   train_loss = 2.375\n",
      "Epoch 673 Batch  486/718   train_loss = 2.398\n",
      "Epoch 673 Batch  586/718   train_loss = 2.370\n",
      "Epoch 673 Batch  686/718   train_loss = 2.360\n",
      "Epoch 674 Batch   68/718   train_loss = 2.442\n",
      "Epoch 674 Batch  168/718   train_loss = 2.444\n",
      "Epoch 674 Batch  268/718   train_loss = 2.379\n",
      "Epoch 674 Batch  368/718   train_loss = 2.351\n",
      "Epoch 674 Batch  468/718   train_loss = 2.596\n",
      "Epoch 674 Batch  568/718   train_loss = 2.540\n",
      "Epoch 674 Batch  668/718   train_loss = 2.387\n",
      "Epoch 675 Batch   50/718   train_loss = 2.490\n",
      "Epoch 675 Batch  150/718   train_loss = 2.409\n",
      "Epoch 675 Batch  250/718   train_loss = 2.340\n",
      "Epoch 675 Batch  350/718   train_loss = 2.398\n",
      "Epoch 675 Batch  450/718   train_loss = 2.468\n",
      "Epoch 675 Batch  550/718   train_loss = 2.292\n",
      "Epoch 675 Batch  650/718   train_loss = 2.328\n",
      "Epoch 676 Batch   32/718   train_loss = 2.311\n",
      "Epoch 676 Batch  132/718   train_loss = 2.366\n",
      "Epoch 676 Batch  232/718   train_loss = 2.373\n",
      "Epoch 676 Batch  332/718   train_loss = 2.304\n",
      "Epoch 676 Batch  432/718   train_loss = 2.508\n",
      "Epoch 676 Batch  532/718   train_loss = 2.476\n",
      "Epoch 676 Batch  632/718   train_loss = 2.244\n",
      "Epoch 677 Batch   14/718   train_loss = 2.428\n",
      "Epoch 677 Batch  114/718   train_loss = 2.326\n",
      "Epoch 677 Batch  214/718   train_loss = 2.485\n",
      "Epoch 677 Batch  314/718   train_loss = 2.409\n",
      "Epoch 677 Batch  414/718   train_loss = 2.294\n",
      "Epoch 677 Batch  514/718   train_loss = 2.497\n",
      "Epoch 677 Batch  614/718   train_loss = 2.343\n",
      "Epoch 677 Batch  714/718   train_loss = 2.385\n",
      "Epoch 678 Batch   96/718   train_loss = 2.435\n",
      "Epoch 678 Batch  196/718   train_loss = 2.380\n",
      "Epoch 678 Batch  296/718   train_loss = 2.342\n",
      "Epoch 678 Batch  396/718   train_loss = 2.416\n",
      "Epoch 678 Batch  496/718   train_loss = 2.484\n",
      "Epoch 678 Batch  596/718   train_loss = 2.364\n",
      "Epoch 678 Batch  696/718   train_loss = 2.380\n",
      "Epoch 679 Batch   78/718   train_loss = 2.433\n",
      "Epoch 679 Batch  178/718   train_loss = 2.437\n",
      "Epoch 679 Batch  278/718   train_loss = 2.316\n",
      "Epoch 679 Batch  378/718   train_loss = 2.305\n",
      "Epoch 679 Batch  478/718   train_loss = 2.295\n",
      "Epoch 679 Batch  578/718   train_loss = 2.334\n",
      "Epoch 679 Batch  678/718   train_loss = 2.425\n",
      "Epoch 680 Batch   60/718   train_loss = 2.469\n",
      "Epoch 680 Batch  160/718   train_loss = 2.431\n",
      "Epoch 680 Batch  260/718   train_loss = 2.420\n",
      "Epoch 680 Batch  360/718   train_loss = 2.372\n",
      "Epoch 680 Batch  460/718   train_loss = 2.375\n",
      "Epoch 680 Batch  560/718   train_loss = 2.378\n",
      "Epoch 680 Batch  660/718   train_loss = 2.500\n",
      "Epoch 681 Batch   42/718   train_loss = 2.412\n",
      "Epoch 681 Batch  142/718   train_loss = 2.428\n",
      "Epoch 681 Batch  242/718   train_loss = 2.273\n",
      "Epoch 681 Batch  342/718   train_loss = 2.402\n",
      "Epoch 681 Batch  442/718   train_loss = 2.403\n",
      "Epoch 681 Batch  542/718   train_loss = 2.306\n",
      "Epoch 681 Batch  642/718   train_loss = 2.522\n",
      "Epoch 682 Batch   24/718   train_loss = 2.420\n",
      "Epoch 682 Batch  124/718   train_loss = 2.246\n",
      "Epoch 682 Batch  224/718   train_loss = 2.364\n",
      "Epoch 682 Batch  324/718   train_loss = 2.394\n",
      "Epoch 682 Batch  424/718   train_loss = 2.373\n",
      "Epoch 682 Batch  524/718   train_loss = 2.422\n",
      "Epoch 682 Batch  624/718   train_loss = 2.404\n",
      "Epoch 683 Batch    6/718   train_loss = 2.359\n",
      "Epoch 683 Batch  106/718   train_loss = 2.388\n",
      "Epoch 683 Batch  206/718   train_loss = 2.246\n",
      "Epoch 683 Batch  306/718   train_loss = 2.341\n",
      "Epoch 683 Batch  406/718   train_loss = 2.385\n",
      "Epoch 683 Batch  506/718   train_loss = 2.320\n",
      "Epoch 683 Batch  606/718   train_loss = 2.352\n",
      "Epoch 683 Batch  706/718   train_loss = 2.384\n",
      "Epoch 684 Batch   88/718   train_loss = 2.478\n",
      "Epoch 684 Batch  188/718   train_loss = 2.337\n",
      "Epoch 684 Batch  288/718   train_loss = 2.372\n",
      "Epoch 684 Batch  388/718   train_loss = 2.296\n",
      "Epoch 684 Batch  488/718   train_loss = 2.331\n",
      "Epoch 684 Batch  588/718   train_loss = 2.478\n",
      "Epoch 684 Batch  688/718   train_loss = 2.377\n",
      "Epoch 685 Batch   70/718   train_loss = 2.399\n",
      "Epoch 685 Batch  170/718   train_loss = 2.399\n",
      "Epoch 685 Batch  270/718   train_loss = 2.443\n",
      "Epoch 685 Batch  370/718   train_loss = 2.392\n",
      "Epoch 685 Batch  470/718   train_loss = 2.549\n",
      "Epoch 685 Batch  570/718   train_loss = 2.443\n",
      "Epoch 685 Batch  670/718   train_loss = 2.383\n",
      "Epoch 686 Batch   52/718   train_loss = 2.351\n",
      "Epoch 686 Batch  152/718   train_loss = 2.372\n",
      "Epoch 686 Batch  252/718   train_loss = 2.456\n",
      "Epoch 686 Batch  352/718   train_loss = 2.344\n",
      "Epoch 686 Batch  452/718   train_loss = 2.398\n",
      "Epoch 686 Batch  552/718   train_loss = 2.438\n",
      "Epoch 686 Batch  652/718   train_loss = 2.344\n",
      "Epoch 687 Batch   34/718   train_loss = 2.349\n",
      "Epoch 687 Batch  134/718   train_loss = 2.323\n",
      "Epoch 687 Batch  234/718   train_loss = 2.442\n",
      "Epoch 687 Batch  334/718   train_loss = 2.462\n",
      "Epoch 687 Batch  434/718   train_loss = 2.447\n",
      "Epoch 687 Batch  534/718   train_loss = 2.372\n",
      "Epoch 687 Batch  634/718   train_loss = 2.468\n",
      "Epoch 688 Batch   16/718   train_loss = 2.346\n",
      "Epoch 688 Batch  116/718   train_loss = 2.360\n",
      "Epoch 688 Batch  216/718   train_loss = 2.375\n",
      "Epoch 688 Batch  316/718   train_loss = 2.329\n",
      "Epoch 688 Batch  416/718   train_loss = 2.400\n",
      "Epoch 688 Batch  516/718   train_loss = 2.385\n",
      "Epoch 688 Batch  616/718   train_loss = 2.496\n",
      "Epoch 688 Batch  716/718   train_loss = 2.489\n",
      "Epoch 689 Batch   98/718   train_loss = 2.334\n",
      "Epoch 689 Batch  198/718   train_loss = 2.378\n",
      "Epoch 689 Batch  298/718   train_loss = 2.345\n",
      "Epoch 689 Batch  398/718   train_loss = 2.368\n",
      "Epoch 689 Batch  498/718   train_loss = 2.382\n",
      "Epoch 689 Batch  598/718   train_loss = 2.436\n",
      "Epoch 689 Batch  698/718   train_loss = 2.415\n",
      "Epoch 690 Batch   80/718   train_loss = 2.365\n",
      "Epoch 690 Batch  180/718   train_loss = 2.365\n",
      "Epoch 690 Batch  280/718   train_loss = 2.455\n",
      "Epoch 690 Batch  380/718   train_loss = 2.397\n",
      "Epoch 690 Batch  480/718   train_loss = 2.446\n",
      "Epoch 690 Batch  580/718   train_loss = 2.308\n",
      "Epoch 690 Batch  680/718   train_loss = 2.422\n",
      "Epoch 691 Batch   62/718   train_loss = 2.338\n",
      "Epoch 691 Batch  162/718   train_loss = 2.389\n",
      "Epoch 691 Batch  262/718   train_loss = 2.468\n",
      "Epoch 691 Batch  362/718   train_loss = 2.450\n",
      "Epoch 691 Batch  462/718   train_loss = 2.404\n",
      "Epoch 691 Batch  562/718   train_loss = 2.357\n",
      "Epoch 691 Batch  662/718   train_loss = 2.417\n",
      "Epoch 692 Batch   44/718   train_loss = 2.431\n",
      "Epoch 692 Batch  144/718   train_loss = 2.497\n",
      "Epoch 692 Batch  244/718   train_loss = 2.503\n",
      "Epoch 692 Batch  344/718   train_loss = 2.418\n",
      "Epoch 692 Batch  444/718   train_loss = 2.322\n",
      "Epoch 692 Batch  544/718   train_loss = 2.319\n",
      "Epoch 692 Batch  644/718   train_loss = 2.526\n",
      "Epoch 693 Batch   26/718   train_loss = 2.494\n",
      "Epoch 693 Batch  126/718   train_loss = 2.375\n",
      "Epoch 693 Batch  226/718   train_loss = 2.350\n",
      "Epoch 693 Batch  326/718   train_loss = 2.362\n",
      "Epoch 693 Batch  426/718   train_loss = 2.390\n",
      "Epoch 693 Batch  526/718   train_loss = 2.374\n",
      "Epoch 693 Batch  626/718   train_loss = 2.502\n",
      "Epoch 694 Batch    8/718   train_loss = 2.296\n",
      "Epoch 694 Batch  108/718   train_loss = 2.259\n",
      "Epoch 694 Batch  208/718   train_loss = 2.395\n",
      "Epoch 694 Batch  308/718   train_loss = 2.453\n",
      "Epoch 694 Batch  408/718   train_loss = 2.468\n",
      "Epoch 694 Batch  508/718   train_loss = 2.513\n",
      "Epoch 694 Batch  608/718   train_loss = 2.573\n",
      "Epoch 694 Batch  708/718   train_loss = 2.370\n",
      "Epoch 695 Batch   90/718   train_loss = 2.296\n",
      "Epoch 695 Batch  190/718   train_loss = 2.480\n",
      "Epoch 695 Batch  290/718   train_loss = 2.275\n",
      "Epoch 695 Batch  390/718   train_loss = 2.360\n",
      "Epoch 695 Batch  490/718   train_loss = 2.396\n",
      "Epoch 695 Batch  590/718   train_loss = 2.393\n",
      "Epoch 695 Batch  690/718   train_loss = 2.491\n",
      "Epoch 696 Batch   72/718   train_loss = 2.440\n",
      "Epoch 696 Batch  172/718   train_loss = 2.443\n",
      "Epoch 696 Batch  272/718   train_loss = 2.406\n",
      "Epoch 696 Batch  372/718   train_loss = 2.435\n",
      "Epoch 696 Batch  472/718   train_loss = 2.494\n",
      "Epoch 696 Batch  572/718   train_loss = 2.412\n",
      "Epoch 696 Batch  672/718   train_loss = 2.308\n",
      "Epoch 697 Batch   54/718   train_loss = 2.272\n",
      "Epoch 697 Batch  154/718   train_loss = 2.509\n",
      "Epoch 697 Batch  254/718   train_loss = 2.358\n",
      "Epoch 697 Batch  354/718   train_loss = 2.409\n",
      "Epoch 697 Batch  454/718   train_loss = 2.333\n",
      "Epoch 697 Batch  554/718   train_loss = 2.482\n",
      "Epoch 697 Batch  654/718   train_loss = 2.490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 698 Batch   36/718   train_loss = 2.397\n",
      "Epoch 698 Batch  136/718   train_loss = 2.359\n",
      "Epoch 698 Batch  236/718   train_loss = 2.372\n",
      "Epoch 698 Batch  336/718   train_loss = 2.391\n",
      "Epoch 698 Batch  436/718   train_loss = 2.384\n",
      "Epoch 698 Batch  536/718   train_loss = 2.466\n",
      "Epoch 698 Batch  636/718   train_loss = 2.499\n",
      "Epoch 699 Batch   18/718   train_loss = 2.388\n",
      "Epoch 699 Batch  118/718   train_loss = 2.369\n",
      "Epoch 699 Batch  218/718   train_loss = 2.459\n",
      "Epoch 699 Batch  318/718   train_loss = 2.373\n",
      "Epoch 699 Batch  418/718   train_loss = 2.558\n",
      "Epoch 699 Batch  518/718   train_loss = 2.434\n",
      "Epoch 699 Batch  618/718   train_loss = 2.365\n",
      "Epoch 700 Batch    0/718   train_loss = 2.461\n",
      "Epoch 700 Batch  100/718   train_loss = 2.419\n",
      "Epoch 700 Batch  200/718   train_loss = 2.327\n",
      "Epoch 700 Batch  300/718   train_loss = 2.366\n",
      "Epoch 700 Batch  400/718   train_loss = 2.452\n",
      "Epoch 700 Batch  500/718   train_loss = 2.551\n",
      "Epoch 700 Batch  600/718   train_loss = 2.486\n",
      "Epoch 700 Batch  700/718   train_loss = 2.322\n",
      "Epoch 701 Batch   82/718   train_loss = 2.300\n",
      "Epoch 701 Batch  182/718   train_loss = 2.481\n",
      "Epoch 701 Batch  282/718   train_loss = 2.457\n",
      "Epoch 701 Batch  382/718   train_loss = 2.341\n",
      "Epoch 701 Batch  482/718   train_loss = 2.525\n",
      "Epoch 701 Batch  582/718   train_loss = 2.328\n",
      "Epoch 701 Batch  682/718   train_loss = 2.378\n",
      "Epoch 702 Batch   64/718   train_loss = 2.394\n",
      "Epoch 702 Batch  164/718   train_loss = 2.283\n",
      "Epoch 702 Batch  264/718   train_loss = 2.382\n",
      "Epoch 702 Batch  364/718   train_loss = 2.478\n",
      "Epoch 702 Batch  464/718   train_loss = 2.308\n",
      "Epoch 702 Batch  564/718   train_loss = 2.451\n",
      "Epoch 702 Batch  664/718   train_loss = 2.482\n",
      "Epoch 703 Batch   46/718   train_loss = 2.354\n",
      "Epoch 703 Batch  146/718   train_loss = 2.272\n",
      "Epoch 703 Batch  246/718   train_loss = 2.488\n",
      "Epoch 703 Batch  346/718   train_loss = 2.327\n",
      "Epoch 703 Batch  446/718   train_loss = 2.388\n",
      "Epoch 703 Batch  546/718   train_loss = 2.328\n",
      "Epoch 703 Batch  646/718   train_loss = 2.441\n",
      "Epoch 704 Batch   28/718   train_loss = 2.359\n",
      "Epoch 704 Batch  128/718   train_loss = 2.373\n",
      "Epoch 704 Batch  228/718   train_loss = 2.481\n",
      "Epoch 704 Batch  328/718   train_loss = 2.386\n",
      "Epoch 704 Batch  428/718   train_loss = 2.292\n",
      "Epoch 704 Batch  528/718   train_loss = 2.437\n",
      "Epoch 704 Batch  628/718   train_loss = 2.504\n",
      "Epoch 705 Batch   10/718   train_loss = 2.364\n",
      "Epoch 705 Batch  110/718   train_loss = 2.390\n",
      "Epoch 705 Batch  210/718   train_loss = 2.324\n",
      "Epoch 705 Batch  310/718   train_loss = 2.461\n",
      "Epoch 705 Batch  410/718   train_loss = 2.246\n",
      "Epoch 705 Batch  510/718   train_loss = 2.465\n",
      "Epoch 705 Batch  610/718   train_loss = 2.446\n",
      "Epoch 705 Batch  710/718   train_loss = 2.357\n",
      "Epoch 706 Batch   92/718   train_loss = 2.319\n",
      "Epoch 706 Batch  192/718   train_loss = 2.377\n",
      "Epoch 706 Batch  292/718   train_loss = 2.284\n",
      "Epoch 706 Batch  392/718   train_loss = 2.342\n",
      "Epoch 706 Batch  492/718   train_loss = 2.388\n",
      "Epoch 706 Batch  592/718   train_loss = 2.471\n",
      "Epoch 706 Batch  692/718   train_loss = 2.413\n",
      "Epoch 707 Batch   74/718   train_loss = 2.459\n",
      "Epoch 707 Batch  174/718   train_loss = 2.509\n",
      "Epoch 707 Batch  274/718   train_loss = 2.415\n",
      "Epoch 707 Batch  374/718   train_loss = 2.299\n",
      "Epoch 707 Batch  474/718   train_loss = 2.450\n",
      "Epoch 707 Batch  574/718   train_loss = 2.293\n",
      "Epoch 707 Batch  674/718   train_loss = 2.345\n",
      "Epoch 708 Batch   56/718   train_loss = 2.463\n",
      "Epoch 708 Batch  156/718   train_loss = 2.328\n",
      "Epoch 708 Batch  256/718   train_loss = 2.321\n",
      "Epoch 708 Batch  356/718   train_loss = 2.336\n",
      "Epoch 708 Batch  456/718   train_loss = 2.363\n",
      "Epoch 708 Batch  556/718   train_loss = 2.319\n",
      "Epoch 708 Batch  656/718   train_loss = 2.353\n",
      "Epoch 709 Batch   38/718   train_loss = 2.413\n",
      "Epoch 709 Batch  138/718   train_loss = 2.415\n",
      "Epoch 709 Batch  238/718   train_loss = 2.425\n",
      "Epoch 709 Batch  338/718   train_loss = 2.425\n",
      "Epoch 709 Batch  438/718   train_loss = 2.320\n",
      "Epoch 709 Batch  538/718   train_loss = 2.374\n",
      "Epoch 709 Batch  638/718   train_loss = 2.350\n",
      "Epoch 710 Batch   20/718   train_loss = 2.399\n",
      "Epoch 710 Batch  120/718   train_loss = 2.384\n",
      "Epoch 710 Batch  220/718   train_loss = 2.368\n",
      "Epoch 710 Batch  320/718   train_loss = 2.409\n",
      "Epoch 710 Batch  420/718   train_loss = 2.546\n",
      "Epoch 710 Batch  520/718   train_loss = 2.437\n",
      "Epoch 710 Batch  620/718   train_loss = 2.509\n",
      "Epoch 711 Batch    2/718   train_loss = 2.445\n",
      "Epoch 711 Batch  102/718   train_loss = 2.306\n",
      "Epoch 711 Batch  202/718   train_loss = 2.392\n",
      "Epoch 711 Batch  302/718   train_loss = 2.478\n",
      "Epoch 711 Batch  402/718   train_loss = 2.453\n",
      "Epoch 711 Batch  502/718   train_loss = 2.404\n",
      "Epoch 711 Batch  602/718   train_loss = 2.350\n",
      "Epoch 711 Batch  702/718   train_loss = 2.317\n",
      "Epoch 712 Batch   84/718   train_loss = 2.297\n",
      "Epoch 712 Batch  184/718   train_loss = 2.375\n",
      "Epoch 712 Batch  284/718   train_loss = 2.388\n",
      "Epoch 712 Batch  384/718   train_loss = 2.357\n",
      "Epoch 712 Batch  484/718   train_loss = 2.394\n",
      "Epoch 712 Batch  584/718   train_loss = 2.332\n",
      "Epoch 712 Batch  684/718   train_loss = 2.369\n",
      "Epoch 713 Batch   66/718   train_loss = 2.298\n",
      "Epoch 713 Batch  166/718   train_loss = 2.351\n",
      "Epoch 713 Batch  266/718   train_loss = 2.412\n",
      "Epoch 713 Batch  366/718   train_loss = 2.486\n",
      "Epoch 713 Batch  466/718   train_loss = 2.373\n",
      "Epoch 713 Batch  566/718   train_loss = 2.395\n",
      "Epoch 713 Batch  666/718   train_loss = 2.352\n",
      "Epoch 714 Batch   48/718   train_loss = 2.304\n",
      "Epoch 714 Batch  148/718   train_loss = 2.409\n",
      "Epoch 714 Batch  248/718   train_loss = 2.422\n",
      "Epoch 714 Batch  348/718   train_loss = 2.336\n",
      "Epoch 714 Batch  448/718   train_loss = 2.422\n",
      "Epoch 714 Batch  548/718   train_loss = 2.432\n",
      "Epoch 714 Batch  648/718   train_loss = 2.400\n",
      "Epoch 715 Batch   30/718   train_loss = 2.401\n",
      "Epoch 715 Batch  130/718   train_loss = 2.316\n",
      "Epoch 715 Batch  230/718   train_loss = 2.440\n",
      "Epoch 715 Batch  330/718   train_loss = 2.481\n",
      "Epoch 715 Batch  430/718   train_loss = 2.470\n",
      "Epoch 715 Batch  530/718   train_loss = 2.398\n",
      "Epoch 715 Batch  630/718   train_loss = 2.344\n",
      "Epoch 716 Batch   12/718   train_loss = 2.326\n",
      "Epoch 716 Batch  112/718   train_loss = 2.429\n",
      "Epoch 716 Batch  212/718   train_loss = 2.479\n",
      "Epoch 716 Batch  312/718   train_loss = 2.427\n",
      "Epoch 716 Batch  412/718   train_loss = 2.320\n",
      "Epoch 716 Batch  512/718   train_loss = 2.361\n",
      "Epoch 716 Batch  612/718   train_loss = 2.462\n",
      "Epoch 716 Batch  712/718   train_loss = 2.454\n",
      "Epoch 717 Batch   94/718   train_loss = 2.254\n",
      "Epoch 717 Batch  194/718   train_loss = 2.278\n",
      "Epoch 717 Batch  294/718   train_loss = 2.417\n",
      "Epoch 717 Batch  394/718   train_loss = 2.334\n",
      "Epoch 717 Batch  494/718   train_loss = 2.496\n",
      "Epoch 717 Batch  594/718   train_loss = 2.441\n",
      "Epoch 717 Batch  694/718   train_loss = 2.491\n",
      "Epoch 718 Batch   76/718   train_loss = 2.305\n",
      "Epoch 718 Batch  176/718   train_loss = 2.431\n",
      "Epoch 718 Batch  276/718   train_loss = 2.419\n",
      "Epoch 718 Batch  376/718   train_loss = 2.407\n",
      "Epoch 718 Batch  476/718   train_loss = 2.401\n",
      "Epoch 718 Batch  576/718   train_loss = 2.442\n",
      "Epoch 718 Batch  676/718   train_loss = 2.347\n",
      "Epoch 719 Batch   58/718   train_loss = 2.391\n",
      "Epoch 719 Batch  158/718   train_loss = 2.405\n",
      "Epoch 719 Batch  258/718   train_loss = 2.367\n",
      "Epoch 719 Batch  358/718   train_loss = 2.353\n",
      "Epoch 719 Batch  458/718   train_loss = 2.458\n",
      "Epoch 719 Batch  558/718   train_loss = 2.480\n",
      "Epoch 719 Batch  658/718   train_loss = 2.297\n",
      "Epoch 720 Batch   40/718   train_loss = 2.425\n",
      "Epoch 720 Batch  140/718   train_loss = 2.334\n",
      "Epoch 720 Batch  240/718   train_loss = 2.283\n",
      "Epoch 720 Batch  340/718   train_loss = 2.402\n",
      "Epoch 720 Batch  440/718   train_loss = 2.360\n",
      "Epoch 720 Batch  540/718   train_loss = 2.436\n",
      "Epoch 720 Batch  640/718   train_loss = 2.374\n",
      "Epoch 721 Batch   22/718   train_loss = 2.210\n",
      "Epoch 721 Batch  122/718   train_loss = 2.380\n",
      "Epoch 721 Batch  222/718   train_loss = 2.368\n",
      "Epoch 721 Batch  322/718   train_loss = 2.391\n",
      "Epoch 721 Batch  422/718   train_loss = 2.431\n",
      "Epoch 721 Batch  522/718   train_loss = 2.401\n",
      "Epoch 721 Batch  622/718   train_loss = 2.432\n",
      "Epoch 722 Batch    4/718   train_loss = 2.334\n",
      "Epoch 722 Batch  104/718   train_loss = 2.384\n",
      "Epoch 722 Batch  204/718   train_loss = 2.251\n",
      "Epoch 722 Batch  304/718   train_loss = 2.360\n",
      "Epoch 722 Batch  404/718   train_loss = 2.381\n",
      "Epoch 722 Batch  504/718   train_loss = 2.509\n",
      "Epoch 722 Batch  604/718   train_loss = 2.323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 722 Batch  704/718   train_loss = 2.379\n",
      "Epoch 723 Batch   86/718   train_loss = 2.401\n",
      "Epoch 723 Batch  186/718   train_loss = 2.367\n",
      "Epoch 723 Batch  286/718   train_loss = 2.472\n",
      "Epoch 723 Batch  386/718   train_loss = 2.401\n",
      "Epoch 723 Batch  486/718   train_loss = 2.382\n",
      "Epoch 723 Batch  586/718   train_loss = 2.359\n",
      "Epoch 723 Batch  686/718   train_loss = 2.372\n",
      "Epoch 724 Batch   68/718   train_loss = 2.386\n",
      "Epoch 724 Batch  168/718   train_loss = 2.453\n",
      "Epoch 724 Batch  268/718   train_loss = 2.340\n",
      "Epoch 724 Batch  368/718   train_loss = 2.287\n",
      "Epoch 724 Batch  468/718   train_loss = 2.544\n",
      "Epoch 724 Batch  568/718   train_loss = 2.506\n",
      "Epoch 724 Batch  668/718   train_loss = 2.384\n",
      "Epoch 725 Batch   50/718   train_loss = 2.415\n",
      "Epoch 725 Batch  150/718   train_loss = 2.359\n",
      "Epoch 725 Batch  250/718   train_loss = 2.341\n",
      "Epoch 725 Batch  350/718   train_loss = 2.369\n",
      "Epoch 725 Batch  450/718   train_loss = 2.433\n",
      "Epoch 725 Batch  550/718   train_loss = 2.298\n",
      "Epoch 725 Batch  650/718   train_loss = 2.316\n",
      "Epoch 726 Batch   32/718   train_loss = 2.338\n",
      "Epoch 726 Batch  132/718   train_loss = 2.360\n",
      "Epoch 726 Batch  232/718   train_loss = 2.370\n",
      "Epoch 726 Batch  332/718   train_loss = 2.295\n",
      "Epoch 726 Batch  432/718   train_loss = 2.473\n",
      "Epoch 726 Batch  532/718   train_loss = 2.499\n",
      "Epoch 726 Batch  632/718   train_loss = 2.229\n",
      "Epoch 727 Batch   14/718   train_loss = 2.411\n",
      "Epoch 727 Batch  114/718   train_loss = 2.311\n",
      "Epoch 727 Batch  214/718   train_loss = 2.433\n",
      "Epoch 727 Batch  314/718   train_loss = 2.436\n",
      "Epoch 727 Batch  414/718   train_loss = 2.260\n",
      "Epoch 727 Batch  514/718   train_loss = 2.485\n",
      "Epoch 727 Batch  614/718   train_loss = 2.377\n",
      "Epoch 727 Batch  714/718   train_loss = 2.399\n",
      "Epoch 728 Batch   96/718   train_loss = 2.354\n",
      "Epoch 728 Batch  196/718   train_loss = 2.304\n",
      "Epoch 728 Batch  296/718   train_loss = 2.329\n",
      "Epoch 728 Batch  396/718   train_loss = 2.394\n",
      "Epoch 728 Batch  496/718   train_loss = 2.469\n",
      "Epoch 728 Batch  596/718   train_loss = 2.380\n",
      "Epoch 728 Batch  696/718   train_loss = 2.389\n",
      "Epoch 729 Batch   78/718   train_loss = 2.425\n",
      "Epoch 729 Batch  178/718   train_loss = 2.407\n",
      "Epoch 729 Batch  278/718   train_loss = 2.341\n",
      "Epoch 729 Batch  378/718   train_loss = 2.293\n",
      "Epoch 729 Batch  478/718   train_loss = 2.343\n",
      "Epoch 729 Batch  578/718   train_loss = 2.313\n",
      "Epoch 729 Batch  678/718   train_loss = 2.420\n",
      "Epoch 730 Batch   60/718   train_loss = 2.435\n",
      "Epoch 730 Batch  160/718   train_loss = 2.379\n",
      "Epoch 730 Batch  260/718   train_loss = 2.430\n",
      "Epoch 730 Batch  360/718   train_loss = 2.393\n",
      "Epoch 730 Batch  460/718   train_loss = 2.315\n",
      "Epoch 730 Batch  560/718   train_loss = 2.406\n",
      "Epoch 730 Batch  660/718   train_loss = 2.509\n",
      "Epoch 731 Batch   42/718   train_loss = 2.455\n",
      "Epoch 731 Batch  142/718   train_loss = 2.452\n",
      "Epoch 731 Batch  242/718   train_loss = 2.276\n",
      "Epoch 731 Batch  342/718   train_loss = 2.364\n",
      "Epoch 731 Batch  442/718   train_loss = 2.354\n",
      "Epoch 731 Batch  542/718   train_loss = 2.266\n",
      "Epoch 731 Batch  642/718   train_loss = 2.491\n",
      "Epoch 732 Batch   24/718   train_loss = 2.375\n",
      "Epoch 732 Batch  124/718   train_loss = 2.267\n",
      "Epoch 732 Batch  224/718   train_loss = 2.350\n",
      "Epoch 732 Batch  324/718   train_loss = 2.325\n",
      "Epoch 732 Batch  424/718   train_loss = 2.395\n",
      "Epoch 732 Batch  524/718   train_loss = 2.402\n",
      "Epoch 732 Batch  624/718   train_loss = 2.380\n",
      "Epoch 733 Batch    6/718   train_loss = 2.291\n",
      "Epoch 733 Batch  106/718   train_loss = 2.389\n",
      "Epoch 733 Batch  206/718   train_loss = 2.237\n",
      "Epoch 733 Batch  306/718   train_loss = 2.313\n",
      "Epoch 733 Batch  406/718   train_loss = 2.365\n",
      "Epoch 733 Batch  506/718   train_loss = 2.315\n",
      "Epoch 733 Batch  606/718   train_loss = 2.352\n",
      "Epoch 733 Batch  706/718   train_loss = 2.366\n",
      "Epoch 734 Batch   88/718   train_loss = 2.469\n",
      "Epoch 734 Batch  188/718   train_loss = 2.320\n",
      "Epoch 734 Batch  288/718   train_loss = 2.328\n",
      "Epoch 734 Batch  388/718   train_loss = 2.263\n",
      "Epoch 734 Batch  488/718   train_loss = 2.259\n",
      "Epoch 734 Batch  588/718   train_loss = 2.452\n",
      "Epoch 734 Batch  688/718   train_loss = 2.345\n",
      "Epoch 735 Batch   70/718   train_loss = 2.396\n",
      "Epoch 735 Batch  170/718   train_loss = 2.414\n",
      "Epoch 735 Batch  270/718   train_loss = 2.443\n",
      "Epoch 735 Batch  370/718   train_loss = 2.392\n",
      "Epoch 735 Batch  470/718   train_loss = 2.507\n",
      "Epoch 735 Batch  570/718   train_loss = 2.438\n",
      "Epoch 735 Batch  670/718   train_loss = 2.369\n",
      "Epoch 736 Batch   52/718   train_loss = 2.330\n",
      "Epoch 736 Batch  152/718   train_loss = 2.391\n",
      "Epoch 736 Batch  252/718   train_loss = 2.426\n",
      "Epoch 736 Batch  352/718   train_loss = 2.334\n",
      "Epoch 736 Batch  452/718   train_loss = 2.419\n",
      "Epoch 736 Batch  552/718   train_loss = 2.451\n",
      "Epoch 736 Batch  652/718   train_loss = 2.313\n",
      "Epoch 737 Batch   34/718   train_loss = 2.368\n",
      "Epoch 737 Batch  134/718   train_loss = 2.314\n",
      "Epoch 737 Batch  234/718   train_loss = 2.455\n",
      "Epoch 737 Batch  334/718   train_loss = 2.461\n",
      "Epoch 737 Batch  434/718   train_loss = 2.406\n",
      "Epoch 737 Batch  534/718   train_loss = 2.365\n",
      "Epoch 737 Batch  634/718   train_loss = 2.500\n",
      "Epoch 738 Batch   16/718   train_loss = 2.294\n",
      "Epoch 738 Batch  116/718   train_loss = 2.305\n",
      "Epoch 738 Batch  216/718   train_loss = 2.341\n",
      "Epoch 738 Batch  316/718   train_loss = 2.302\n",
      "Epoch 738 Batch  416/718   train_loss = 2.370\n",
      "Epoch 738 Batch  516/718   train_loss = 2.387\n",
      "Epoch 738 Batch  616/718   train_loss = 2.470\n",
      "Epoch 738 Batch  716/718   train_loss = 2.464\n",
      "Epoch 739 Batch   98/718   train_loss = 2.333\n",
      "Epoch 739 Batch  198/718   train_loss = 2.390\n",
      "Epoch 739 Batch  298/718   train_loss = 2.304\n",
      "Epoch 739 Batch  398/718   train_loss = 2.368\n",
      "Epoch 739 Batch  498/718   train_loss = 2.338\n",
      "Epoch 739 Batch  598/718   train_loss = 2.458\n",
      "Epoch 739 Batch  698/718   train_loss = 2.450\n",
      "Epoch 740 Batch   80/718   train_loss = 2.367\n",
      "Epoch 740 Batch  180/718   train_loss = 2.313\n",
      "Epoch 740 Batch  280/718   train_loss = 2.478\n",
      "Epoch 740 Batch  380/718   train_loss = 2.389\n",
      "Epoch 740 Batch  480/718   train_loss = 2.423\n",
      "Epoch 740 Batch  580/718   train_loss = 2.353\n",
      "Epoch 740 Batch  680/718   train_loss = 2.433\n",
      "Epoch 741 Batch   62/718   train_loss = 2.317\n",
      "Epoch 741 Batch  162/718   train_loss = 2.386\n",
      "Epoch 741 Batch  262/718   train_loss = 2.438\n",
      "Epoch 741 Batch  362/718   train_loss = 2.436\n",
      "Epoch 741 Batch  462/718   train_loss = 2.396\n",
      "Epoch 741 Batch  562/718   train_loss = 2.339\n",
      "Epoch 741 Batch  662/718   train_loss = 2.383\n",
      "Epoch 742 Batch   44/718   train_loss = 2.351\n",
      "Epoch 742 Batch  144/718   train_loss = 2.472\n",
      "Epoch 742 Batch  244/718   train_loss = 2.469\n",
      "Epoch 742 Batch  344/718   train_loss = 2.395\n",
      "Epoch 742 Batch  444/718   train_loss = 2.281\n",
      "Epoch 742 Batch  544/718   train_loss = 2.346\n",
      "Epoch 742 Batch  644/718   train_loss = 2.454\n",
      "Epoch 743 Batch   26/718   train_loss = 2.481\n",
      "Epoch 743 Batch  126/718   train_loss = 2.332\n",
      "Epoch 743 Batch  226/718   train_loss = 2.328\n",
      "Epoch 743 Batch  326/718   train_loss = 2.303\n",
      "Epoch 743 Batch  426/718   train_loss = 2.324\n",
      "Epoch 743 Batch  526/718   train_loss = 2.366\n",
      "Epoch 743 Batch  626/718   train_loss = 2.483\n",
      "Epoch 744 Batch    8/718   train_loss = 2.296\n",
      "Epoch 744 Batch  108/718   train_loss = 2.324\n",
      "Epoch 744 Batch  208/718   train_loss = 2.413\n",
      "Epoch 744 Batch  308/718   train_loss = 2.380\n",
      "Epoch 744 Batch  408/718   train_loss = 2.497\n",
      "Epoch 744 Batch  508/718   train_loss = 2.448\n",
      "Epoch 744 Batch  608/718   train_loss = 2.533\n",
      "Epoch 744 Batch  708/718   train_loss = 2.352\n",
      "Epoch 745 Batch   90/718   train_loss = 2.311\n",
      "Epoch 745 Batch  190/718   train_loss = 2.469\n",
      "Epoch 745 Batch  290/718   train_loss = 2.288\n",
      "Epoch 745 Batch  390/718   train_loss = 2.339\n",
      "Epoch 745 Batch  490/718   train_loss = 2.414\n",
      "Epoch 745 Batch  590/718   train_loss = 2.392\n",
      "Epoch 745 Batch  690/718   train_loss = 2.494\n",
      "Epoch 746 Batch   72/718   train_loss = 2.414\n",
      "Epoch 746 Batch  172/718   train_loss = 2.416\n",
      "Epoch 746 Batch  272/718   train_loss = 2.428\n",
      "Epoch 746 Batch  372/718   train_loss = 2.441\n",
      "Epoch 746 Batch  472/718   train_loss = 2.465\n",
      "Epoch 746 Batch  572/718   train_loss = 2.415\n",
      "Epoch 746 Batch  672/718   train_loss = 2.323\n",
      "Epoch 747 Batch   54/718   train_loss = 2.282\n",
      "Epoch 747 Batch  154/718   train_loss = 2.505\n",
      "Epoch 747 Batch  254/718   train_loss = 2.346\n",
      "Epoch 747 Batch  354/718   train_loss = 2.433\n",
      "Epoch 747 Batch  454/718   train_loss = 2.314\n",
      "Epoch 747 Batch  554/718   train_loss = 2.477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 747 Batch  654/718   train_loss = 2.481\n",
      "Epoch 748 Batch   36/718   train_loss = 2.372\n",
      "Epoch 748 Batch  136/718   train_loss = 2.353\n",
      "Epoch 748 Batch  236/718   train_loss = 2.293\n",
      "Epoch 748 Batch  336/718   train_loss = 2.414\n",
      "Epoch 748 Batch  436/718   train_loss = 2.454\n",
      "Epoch 748 Batch  536/718   train_loss = 2.472\n",
      "Epoch 748 Batch  636/718   train_loss = 2.468\n",
      "Epoch 749 Batch   18/718   train_loss = 2.380\n",
      "Epoch 749 Batch  118/718   train_loss = 2.356\n",
      "Epoch 749 Batch  218/718   train_loss = 2.441\n",
      "Epoch 749 Batch  318/718   train_loss = 2.383\n",
      "Epoch 749 Batch  418/718   train_loss = 2.539\n",
      "Epoch 749 Batch  518/718   train_loss = 2.434\n",
      "Epoch 749 Batch  618/718   train_loss = 2.389\n",
      "Epoch 750 Batch    0/718   train_loss = 2.455\n",
      "Epoch 750 Batch  100/718   train_loss = 2.396\n",
      "Epoch 750 Batch  200/718   train_loss = 2.341\n",
      "Epoch 750 Batch  300/718   train_loss = 2.350\n",
      "Epoch 750 Batch  400/718   train_loss = 2.454\n",
      "Epoch 750 Batch  500/718   train_loss = 2.538\n",
      "Epoch 750 Batch  600/718   train_loss = 2.442\n",
      "Epoch 750 Batch  700/718   train_loss = 2.327\n",
      "Epoch 751 Batch   82/718   train_loss = 2.349\n",
      "Epoch 751 Batch  182/718   train_loss = 2.452\n",
      "Epoch 751 Batch  282/718   train_loss = 2.500\n",
      "Epoch 751 Batch  382/718   train_loss = 2.288\n",
      "Epoch 751 Batch  482/718   train_loss = 2.484\n",
      "Epoch 751 Batch  582/718   train_loss = 2.322\n",
      "Epoch 751 Batch  682/718   train_loss = 2.412\n",
      "Epoch 752 Batch   64/718   train_loss = 2.365\n",
      "Epoch 752 Batch  164/718   train_loss = 2.262\n",
      "Epoch 752 Batch  264/718   train_loss = 2.383\n",
      "Epoch 752 Batch  364/718   train_loss = 2.491\n",
      "Epoch 752 Batch  464/718   train_loss = 2.324\n",
      "Epoch 752 Batch  564/718   train_loss = 2.422\n",
      "Epoch 752 Batch  664/718   train_loss = 2.487\n",
      "Epoch 753 Batch   46/718   train_loss = 2.317\n",
      "Epoch 753 Batch  146/718   train_loss = 2.229\n",
      "Epoch 753 Batch  246/718   train_loss = 2.488\n",
      "Epoch 753 Batch  346/718   train_loss = 2.325\n",
      "Epoch 753 Batch  446/718   train_loss = 2.343\n",
      "Epoch 753 Batch  546/718   train_loss = 2.284\n",
      "Epoch 753 Batch  646/718   train_loss = 2.432\n",
      "Epoch 754 Batch   28/718   train_loss = 2.316\n",
      "Epoch 754 Batch  128/718   train_loss = 2.357\n",
      "Epoch 754 Batch  228/718   train_loss = 2.431\n",
      "Epoch 754 Batch  328/718   train_loss = 2.320\n",
      "Epoch 754 Batch  428/718   train_loss = 2.277\n",
      "Epoch 754 Batch  528/718   train_loss = 2.464\n",
      "Epoch 754 Batch  628/718   train_loss = 2.480\n",
      "Epoch 755 Batch   10/718   train_loss = 2.334\n",
      "Epoch 755 Batch  110/718   train_loss = 2.393\n",
      "Epoch 755 Batch  210/718   train_loss = 2.350\n",
      "Epoch 755 Batch  310/718   train_loss = 2.457\n",
      "Epoch 755 Batch  410/718   train_loss = 2.250\n",
      "Epoch 755 Batch  510/718   train_loss = 2.394\n",
      "Epoch 755 Batch  610/718   train_loss = 2.400\n",
      "Epoch 755 Batch  710/718   train_loss = 2.305\n",
      "Epoch 756 Batch   92/718   train_loss = 2.322\n",
      "Epoch 756 Batch  192/718   train_loss = 2.386\n",
      "Epoch 756 Batch  292/718   train_loss = 2.242\n",
      "Epoch 756 Batch  392/718   train_loss = 2.315\n",
      "Epoch 756 Batch  492/718   train_loss = 2.422\n",
      "Epoch 756 Batch  592/718   train_loss = 2.454\n",
      "Epoch 756 Batch  692/718   train_loss = 2.416\n",
      "Epoch 757 Batch   74/718   train_loss = 2.421\n",
      "Epoch 757 Batch  174/718   train_loss = 2.506\n",
      "Epoch 757 Batch  274/718   train_loss = 2.405\n",
      "Epoch 757 Batch  374/718   train_loss = 2.292\n",
      "Epoch 757 Batch  474/718   train_loss = 2.430\n",
      "Epoch 757 Batch  574/718   train_loss = 2.265\n",
      "Epoch 757 Batch  674/718   train_loss = 2.370\n",
      "Epoch 758 Batch   56/718   train_loss = 2.495\n",
      "Epoch 758 Batch  156/718   train_loss = 2.358\n",
      "Epoch 758 Batch  256/718   train_loss = 2.311\n",
      "Epoch 758 Batch  356/718   train_loss = 2.296\n",
      "Epoch 758 Batch  456/718   train_loss = 2.360\n",
      "Epoch 758 Batch  556/718   train_loss = 2.282\n",
      "Epoch 758 Batch  656/718   train_loss = 2.314\n",
      "Epoch 759 Batch   38/718   train_loss = 2.406\n",
      "Epoch 759 Batch  138/718   train_loss = 2.423\n",
      "Epoch 759 Batch  238/718   train_loss = 2.416\n",
      "Epoch 759 Batch  338/718   train_loss = 2.416\n",
      "Epoch 759 Batch  438/718   train_loss = 2.287\n",
      "Epoch 759 Batch  538/718   train_loss = 2.338\n",
      "Epoch 759 Batch  638/718   train_loss = 2.331\n",
      "Epoch 760 Batch   20/718   train_loss = 2.376\n",
      "Epoch 760 Batch  120/718   train_loss = 2.401\n",
      "Epoch 760 Batch  220/718   train_loss = 2.387\n",
      "Epoch 760 Batch  320/718   train_loss = 2.417\n",
      "Epoch 760 Batch  420/718   train_loss = 2.495\n",
      "Epoch 760 Batch  520/718   train_loss = 2.439\n",
      "Epoch 760 Batch  620/718   train_loss = 2.502\n",
      "Epoch 761 Batch    2/718   train_loss = 2.416\n",
      "Epoch 761 Batch  102/718   train_loss = 2.279\n",
      "Epoch 761 Batch  202/718   train_loss = 2.363\n",
      "Epoch 761 Batch  302/718   train_loss = 2.466\n",
      "Epoch 761 Batch  402/718   train_loss = 2.402\n",
      "Epoch 761 Batch  502/718   train_loss = 2.395\n",
      "Epoch 761 Batch  602/718   train_loss = 2.344\n",
      "Epoch 761 Batch  702/718   train_loss = 2.333\n",
      "Epoch 762 Batch   84/718   train_loss = 2.330\n",
      "Epoch 762 Batch  184/718   train_loss = 2.342\n",
      "Epoch 762 Batch  284/718   train_loss = 2.377\n",
      "Epoch 762 Batch  384/718   train_loss = 2.380\n",
      "Epoch 762 Batch  484/718   train_loss = 2.364\n",
      "Epoch 762 Batch  584/718   train_loss = 2.320\n",
      "Epoch 762 Batch  684/718   train_loss = 2.373\n",
      "Epoch 763 Batch   66/718   train_loss = 2.304\n",
      "Epoch 763 Batch  166/718   train_loss = 2.367\n",
      "Epoch 763 Batch  266/718   train_loss = 2.412\n",
      "Epoch 763 Batch  366/718   train_loss = 2.484\n",
      "Epoch 763 Batch  466/718   train_loss = 2.369\n",
      "Epoch 763 Batch  566/718   train_loss = 2.321\n",
      "Epoch 763 Batch  666/718   train_loss = 2.288\n",
      "Epoch 764 Batch   48/718   train_loss = 2.245\n",
      "Epoch 764 Batch  148/718   train_loss = 2.394\n",
      "Epoch 764 Batch  248/718   train_loss = 2.432\n",
      "Epoch 764 Batch  348/718   train_loss = 2.341\n",
      "Epoch 764 Batch  448/718   train_loss = 2.383\n",
      "Epoch 764 Batch  548/718   train_loss = 2.413\n",
      "Epoch 764 Batch  648/718   train_loss = 2.368\n",
      "Epoch 765 Batch   30/718   train_loss = 2.376\n",
      "Epoch 765 Batch  130/718   train_loss = 2.303\n",
      "Epoch 765 Batch  230/718   train_loss = 2.426\n",
      "Epoch 765 Batch  330/718   train_loss = 2.431\n",
      "Epoch 765 Batch  430/718   train_loss = 2.509\n",
      "Epoch 765 Batch  530/718   train_loss = 2.407\n",
      "Epoch 765 Batch  630/718   train_loss = 2.325\n",
      "Epoch 766 Batch   12/718   train_loss = 2.249\n",
      "Epoch 766 Batch  112/718   train_loss = 2.382\n",
      "Epoch 766 Batch  212/718   train_loss = 2.442\n",
      "Epoch 766 Batch  312/718   train_loss = 2.380\n",
      "Epoch 766 Batch  412/718   train_loss = 2.275\n",
      "Epoch 766 Batch  512/718   train_loss = 2.367\n",
      "Epoch 766 Batch  612/718   train_loss = 2.424\n",
      "Epoch 766 Batch  712/718   train_loss = 2.443\n",
      "Epoch 767 Batch   94/718   train_loss = 2.258\n",
      "Epoch 767 Batch  194/718   train_loss = 2.268\n",
      "Epoch 767 Batch  294/718   train_loss = 2.445\n",
      "Epoch 767 Batch  394/718   train_loss = 2.384\n",
      "Epoch 767 Batch  494/718   train_loss = 2.512\n",
      "Epoch 767 Batch  594/718   train_loss = 2.427\n",
      "Epoch 767 Batch  694/718   train_loss = 2.467\n",
      "Epoch 768 Batch   76/718   train_loss = 2.331\n",
      "Epoch 768 Batch  176/718   train_loss = 2.433\n",
      "Epoch 768 Batch  276/718   train_loss = 2.412\n",
      "Epoch 768 Batch  376/718   train_loss = 2.371\n",
      "Epoch 768 Batch  476/718   train_loss = 2.432\n",
      "Epoch 768 Batch  576/718   train_loss = 2.392\n",
      "Epoch 768 Batch  676/718   train_loss = 2.382\n",
      "Epoch 769 Batch   58/718   train_loss = 2.402\n",
      "Epoch 769 Batch  158/718   train_loss = 2.439\n",
      "Epoch 769 Batch  258/718   train_loss = 2.396\n",
      "Epoch 769 Batch  358/718   train_loss = 2.319\n",
      "Epoch 769 Batch  458/718   train_loss = 2.423\n",
      "Epoch 769 Batch  558/718   train_loss = 2.467\n",
      "Epoch 769 Batch  658/718   train_loss = 2.266\n",
      "Epoch 770 Batch   40/718   train_loss = 2.465\n",
      "Epoch 770 Batch  140/718   train_loss = 2.339\n",
      "Epoch 770 Batch  240/718   train_loss = 2.294\n",
      "Epoch 770 Batch  340/718   train_loss = 2.407\n",
      "Epoch 770 Batch  440/718   train_loss = 2.361\n",
      "Epoch 770 Batch  540/718   train_loss = 2.403\n",
      "Epoch 770 Batch  640/718   train_loss = 2.353\n",
      "Epoch 771 Batch   22/718   train_loss = 2.212\n",
      "Epoch 771 Batch  122/718   train_loss = 2.384\n",
      "Epoch 771 Batch  222/718   train_loss = 2.352\n",
      "Epoch 771 Batch  322/718   train_loss = 2.355\n",
      "Epoch 771 Batch  422/718   train_loss = 2.460\n",
      "Epoch 771 Batch  522/718   train_loss = 2.351\n",
      "Epoch 771 Batch  622/718   train_loss = 2.418\n",
      "Epoch 772 Batch    4/718   train_loss = 2.316\n",
      "Epoch 772 Batch  104/718   train_loss = 2.363\n",
      "Epoch 772 Batch  204/718   train_loss = 2.254\n",
      "Epoch 772 Batch  304/718   train_loss = 2.316\n",
      "Epoch 772 Batch  404/718   train_loss = 2.328\n",
      "Epoch 772 Batch  504/718   train_loss = 2.486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 772 Batch  604/718   train_loss = 2.291\n",
      "Epoch 772 Batch  704/718   train_loss = 2.354\n",
      "Epoch 773 Batch   86/718   train_loss = 2.401\n",
      "Epoch 773 Batch  186/718   train_loss = 2.342\n",
      "Epoch 773 Batch  286/718   train_loss = 2.481\n",
      "Epoch 773 Batch  386/718   train_loss = 2.350\n",
      "Epoch 773 Batch  486/718   train_loss = 2.364\n",
      "Epoch 773 Batch  586/718   train_loss = 2.375\n",
      "Epoch 773 Batch  686/718   train_loss = 2.394\n",
      "Epoch 774 Batch   68/718   train_loss = 2.375\n",
      "Epoch 774 Batch  168/718   train_loss = 2.459\n",
      "Epoch 774 Batch  268/718   train_loss = 2.344\n",
      "Epoch 774 Batch  368/718   train_loss = 2.337\n",
      "Epoch 774 Batch  468/718   train_loss = 2.588\n",
      "Epoch 774 Batch  568/718   train_loss = 2.507\n",
      "Epoch 774 Batch  668/718   train_loss = 2.381\n",
      "Epoch 775 Batch   50/718   train_loss = 2.415\n",
      "Epoch 775 Batch  150/718   train_loss = 2.348\n",
      "Epoch 775 Batch  250/718   train_loss = 2.327\n",
      "Epoch 775 Batch  350/718   train_loss = 2.343\n",
      "Epoch 775 Batch  450/718   train_loss = 2.378\n",
      "Epoch 775 Batch  550/718   train_loss = 2.331\n",
      "Epoch 775 Batch  650/718   train_loss = 2.312\n",
      "Epoch 776 Batch   32/718   train_loss = 2.264\n",
      "Epoch 776 Batch  132/718   train_loss = 2.380\n",
      "Epoch 776 Batch  232/718   train_loss = 2.344\n",
      "Epoch 776 Batch  332/718   train_loss = 2.261\n",
      "Epoch 776 Batch  432/718   train_loss = 2.482\n",
      "Epoch 776 Batch  532/718   train_loss = 2.484\n",
      "Epoch 776 Batch  632/718   train_loss = 2.254\n",
      "Epoch 777 Batch   14/718   train_loss = 2.435\n",
      "Epoch 777 Batch  114/718   train_loss = 2.316\n",
      "Epoch 777 Batch  214/718   train_loss = 2.412\n",
      "Epoch 777 Batch  314/718   train_loss = 2.424\n",
      "Epoch 777 Batch  414/718   train_loss = 2.259\n",
      "Epoch 777 Batch  514/718   train_loss = 2.420\n",
      "Epoch 777 Batch  614/718   train_loss = 2.304\n",
      "Epoch 777 Batch  714/718   train_loss = 2.363\n",
      "Epoch 778 Batch   96/718   train_loss = 2.387\n",
      "Epoch 778 Batch  196/718   train_loss = 2.301\n",
      "Epoch 778 Batch  296/718   train_loss = 2.298\n",
      "Epoch 778 Batch  396/718   train_loss = 2.375\n",
      "Epoch 778 Batch  496/718   train_loss = 2.492\n",
      "Epoch 778 Batch  596/718   train_loss = 2.410\n",
      "Epoch 778 Batch  696/718   train_loss = 2.373\n",
      "Epoch 779 Batch   78/718   train_loss = 2.451\n",
      "Epoch 779 Batch  178/718   train_loss = 2.446\n",
      "Epoch 779 Batch  278/718   train_loss = 2.362\n",
      "Epoch 779 Batch  378/718   train_loss = 2.309\n",
      "Epoch 779 Batch  478/718   train_loss = 2.294\n",
      "Epoch 779 Batch  578/718   train_loss = 2.313\n",
      "Epoch 779 Batch  678/718   train_loss = 2.391\n",
      "Epoch 780 Batch   60/718   train_loss = 2.393\n",
      "Epoch 780 Batch  160/718   train_loss = 2.408\n",
      "Epoch 780 Batch  260/718   train_loss = 2.407\n",
      "Epoch 780 Batch  360/718   train_loss = 2.378\n",
      "Epoch 780 Batch  460/718   train_loss = 2.318\n",
      "Epoch 780 Batch  560/718   train_loss = 2.355\n",
      "Epoch 780 Batch  660/718   train_loss = 2.474\n",
      "Epoch 781 Batch   42/718   train_loss = 2.431\n",
      "Epoch 781 Batch  142/718   train_loss = 2.402\n",
      "Epoch 781 Batch  242/718   train_loss = 2.252\n",
      "Epoch 781 Batch  342/718   train_loss = 2.367\n",
      "Epoch 781 Batch  442/718   train_loss = 2.339\n",
      "Epoch 781 Batch  542/718   train_loss = 2.305\n",
      "Epoch 781 Batch  642/718   train_loss = 2.472\n",
      "Epoch 782 Batch   24/718   train_loss = 2.385\n",
      "Epoch 782 Batch  124/718   train_loss = 2.261\n",
      "Epoch 782 Batch  224/718   train_loss = 2.351\n",
      "Epoch 782 Batch  324/718   train_loss = 2.341\n",
      "Epoch 782 Batch  424/718   train_loss = 2.341\n",
      "Epoch 782 Batch  524/718   train_loss = 2.386\n",
      "Epoch 782 Batch  624/718   train_loss = 2.349\n",
      "Epoch 783 Batch    6/718   train_loss = 2.315\n",
      "Epoch 783 Batch  106/718   train_loss = 2.344\n",
      "Epoch 783 Batch  206/718   train_loss = 2.238\n",
      "Epoch 783 Batch  306/718   train_loss = 2.302\n",
      "Epoch 783 Batch  406/718   train_loss = 2.378\n",
      "Epoch 783 Batch  506/718   train_loss = 2.303\n",
      "Epoch 783 Batch  606/718   train_loss = 2.264\n",
      "Epoch 783 Batch  706/718   train_loss = 2.363\n",
      "Epoch 784 Batch   88/718   train_loss = 2.456\n",
      "Epoch 784 Batch  188/718   train_loss = 2.301\n",
      "Epoch 784 Batch  288/718   train_loss = 2.351\n",
      "Epoch 784 Batch  388/718   train_loss = 2.253\n",
      "Epoch 784 Batch  488/718   train_loss = 2.268\n",
      "Epoch 784 Batch  588/718   train_loss = 2.432\n",
      "Epoch 784 Batch  688/718   train_loss = 2.318\n",
      "Epoch 785 Batch   70/718   train_loss = 2.367\n",
      "Epoch 785 Batch  170/718   train_loss = 2.375\n",
      "Epoch 785 Batch  270/718   train_loss = 2.443\n",
      "Epoch 785 Batch  370/718   train_loss = 2.351\n",
      "Epoch 785 Batch  470/718   train_loss = 2.519\n",
      "Epoch 785 Batch  570/718   train_loss = 2.382\n",
      "Epoch 785 Batch  670/718   train_loss = 2.332\n",
      "Epoch 786 Batch   52/718   train_loss = 2.362\n",
      "Epoch 786 Batch  152/718   train_loss = 2.328\n",
      "Epoch 786 Batch  252/718   train_loss = 2.425\n",
      "Epoch 786 Batch  352/718   train_loss = 2.342\n",
      "Epoch 786 Batch  452/718   train_loss = 2.396\n",
      "Epoch 786 Batch  552/718   train_loss = 2.472\n",
      "Epoch 786 Batch  652/718   train_loss = 2.272\n",
      "Epoch 787 Batch   34/718   train_loss = 2.329\n",
      "Epoch 787 Batch  134/718   train_loss = 2.281\n",
      "Epoch 787 Batch  234/718   train_loss = 2.460\n",
      "Epoch 787 Batch  334/718   train_loss = 2.440\n",
      "Epoch 787 Batch  434/718   train_loss = 2.399\n",
      "Epoch 787 Batch  534/718   train_loss = 2.353\n",
      "Epoch 787 Batch  634/718   train_loss = 2.469\n",
      "Epoch 788 Batch   16/718   train_loss = 2.346\n",
      "Epoch 788 Batch  116/718   train_loss = 2.262\n",
      "Epoch 788 Batch  216/718   train_loss = 2.323\n",
      "Epoch 788 Batch  316/718   train_loss = 2.287\n",
      "Epoch 788 Batch  416/718   train_loss = 2.356\n",
      "Epoch 788 Batch  516/718   train_loss = 2.327\n",
      "Epoch 788 Batch  616/718   train_loss = 2.422\n",
      "Epoch 788 Batch  716/718   train_loss = 2.451\n",
      "Epoch 789 Batch   98/718   train_loss = 2.307\n",
      "Epoch 789 Batch  198/718   train_loss = 2.356\n",
      "Epoch 789 Batch  298/718   train_loss = 2.306\n",
      "Epoch 789 Batch  398/718   train_loss = 2.338\n",
      "Epoch 789 Batch  498/718   train_loss = 2.367\n",
      "Epoch 789 Batch  598/718   train_loss = 2.430\n",
      "Epoch 789 Batch  698/718   train_loss = 2.411\n",
      "Epoch 790 Batch   80/718   train_loss = 2.350\n",
      "Epoch 790 Batch  180/718   train_loss = 2.324\n",
      "Epoch 790 Batch  280/718   train_loss = 2.453\n",
      "Epoch 790 Batch  380/718   train_loss = 2.364\n",
      "Epoch 790 Batch  480/718   train_loss = 2.427\n",
      "Epoch 790 Batch  580/718   train_loss = 2.293\n",
      "Epoch 790 Batch  680/718   train_loss = 2.427\n",
      "Epoch 791 Batch   62/718   train_loss = 2.342\n",
      "Epoch 791 Batch  162/718   train_loss = 2.367\n",
      "Epoch 791 Batch  262/718   train_loss = 2.428\n",
      "Epoch 791 Batch  362/718   train_loss = 2.432\n",
      "Epoch 791 Batch  462/718   train_loss = 2.376\n",
      "Epoch 791 Batch  562/718   train_loss = 2.336\n",
      "Epoch 791 Batch  662/718   train_loss = 2.343\n",
      "Epoch 792 Batch   44/718   train_loss = 2.386\n",
      "Epoch 792 Batch  144/718   train_loss = 2.499\n",
      "Epoch 792 Batch  244/718   train_loss = 2.461\n",
      "Epoch 792 Batch  344/718   train_loss = 2.380\n",
      "Epoch 792 Batch  444/718   train_loss = 2.306\n",
      "Epoch 792 Batch  544/718   train_loss = 2.342\n",
      "Epoch 792 Batch  644/718   train_loss = 2.505\n",
      "Epoch 793 Batch   26/718   train_loss = 2.472\n",
      "Epoch 793 Batch  126/718   train_loss = 2.353\n",
      "Epoch 793 Batch  226/718   train_loss = 2.330\n",
      "Epoch 793 Batch  326/718   train_loss = 2.308\n",
      "Epoch 793 Batch  426/718   train_loss = 2.356\n",
      "Epoch 793 Batch  526/718   train_loss = 2.375\n",
      "Epoch 793 Batch  626/718   train_loss = 2.428\n",
      "Epoch 794 Batch    8/718   train_loss = 2.259\n",
      "Epoch 794 Batch  108/718   train_loss = 2.303\n",
      "Epoch 794 Batch  208/718   train_loss = 2.419\n",
      "Epoch 794 Batch  308/718   train_loss = 2.420\n",
      "Epoch 794 Batch  408/718   train_loss = 2.426\n",
      "Epoch 794 Batch  508/718   train_loss = 2.474\n",
      "Epoch 794 Batch  608/718   train_loss = 2.528\n",
      "Epoch 794 Batch  708/718   train_loss = 2.346\n",
      "Epoch 795 Batch   90/718   train_loss = 2.286\n",
      "Epoch 795 Batch  190/718   train_loss = 2.431\n",
      "Epoch 795 Batch  290/718   train_loss = 2.282\n",
      "Epoch 795 Batch  390/718   train_loss = 2.325\n",
      "Epoch 795 Batch  490/718   train_loss = 2.372\n",
      "Epoch 795 Batch  590/718   train_loss = 2.378\n",
      "Epoch 795 Batch  690/718   train_loss = 2.451\n",
      "Epoch 796 Batch   72/718   train_loss = 2.394\n",
      "Epoch 796 Batch  172/718   train_loss = 2.445\n",
      "Epoch 796 Batch  272/718   train_loss = 2.374\n",
      "Epoch 796 Batch  372/718   train_loss = 2.407\n",
      "Epoch 796 Batch  472/718   train_loss = 2.439\n",
      "Epoch 796 Batch  572/718   train_loss = 2.439\n",
      "Epoch 796 Batch  672/718   train_loss = 2.270\n",
      "Epoch 797 Batch   54/718   train_loss = 2.245\n",
      "Epoch 797 Batch  154/718   train_loss = 2.431\n",
      "Epoch 797 Batch  254/718   train_loss = 2.323\n",
      "Epoch 797 Batch  354/718   train_loss = 2.421\n",
      "Epoch 797 Batch  454/718   train_loss = 2.319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 797 Batch  554/718   train_loss = 2.487\n",
      "Epoch 797 Batch  654/718   train_loss = 2.450\n",
      "Epoch 798 Batch   36/718   train_loss = 2.394\n",
      "Epoch 798 Batch  136/718   train_loss = 2.343\n",
      "Epoch 798 Batch  236/718   train_loss = 2.282\n",
      "Epoch 798 Batch  336/718   train_loss = 2.414\n",
      "Epoch 798 Batch  436/718   train_loss = 2.369\n",
      "Epoch 798 Batch  536/718   train_loss = 2.469\n",
      "Epoch 798 Batch  636/718   train_loss = 2.421\n",
      "Epoch 799 Batch   18/718   train_loss = 2.394\n",
      "Epoch 799 Batch  118/718   train_loss = 2.322\n",
      "Epoch 799 Batch  218/718   train_loss = 2.419\n",
      "Epoch 799 Batch  318/718   train_loss = 2.374\n",
      "Epoch 799 Batch  418/718   train_loss = 2.470\n",
      "Epoch 799 Batch  518/718   train_loss = 2.415\n",
      "Epoch 799 Batch  618/718   train_loss = 2.385\n",
      "Epoch 800 Batch    0/718   train_loss = 2.434\n",
      "Epoch 800 Batch  100/718   train_loss = 2.356\n",
      "Epoch 800 Batch  200/718   train_loss = 2.297\n",
      "Epoch 800 Batch  300/718   train_loss = 2.361\n",
      "Epoch 800 Batch  400/718   train_loss = 2.438\n",
      "Epoch 800 Batch  500/718   train_loss = 2.511\n",
      "Epoch 800 Batch  600/718   train_loss = 2.474\n",
      "Epoch 800 Batch  700/718   train_loss = 2.329\n",
      "Epoch 801 Batch   82/718   train_loss = 2.350\n",
      "Epoch 801 Batch  182/718   train_loss = 2.445\n",
      "Epoch 801 Batch  282/718   train_loss = 2.461\n",
      "Epoch 801 Batch  382/718   train_loss = 2.323\n",
      "Epoch 801 Batch  482/718   train_loss = 2.482\n",
      "Epoch 801 Batch  582/718   train_loss = 2.300\n",
      "Epoch 801 Batch  682/718   train_loss = 2.373\n",
      "Epoch 802 Batch   64/718   train_loss = 2.384\n",
      "Epoch 802 Batch  164/718   train_loss = 2.243\n",
      "Epoch 802 Batch  264/718   train_loss = 2.408\n",
      "Epoch 802 Batch  364/718   train_loss = 2.501\n",
      "Epoch 802 Batch  464/718   train_loss = 2.324\n",
      "Epoch 802 Batch  564/718   train_loss = 2.406\n",
      "Epoch 802 Batch  664/718   train_loss = 2.482\n",
      "Epoch 803 Batch   46/718   train_loss = 2.299\n",
      "Epoch 803 Batch  146/718   train_loss = 2.250\n",
      "Epoch 803 Batch  246/718   train_loss = 2.486\n",
      "Epoch 803 Batch  346/718   train_loss = 2.302\n",
      "Epoch 803 Batch  446/718   train_loss = 2.341\n",
      "Epoch 803 Batch  546/718   train_loss = 2.280\n",
      "Epoch 803 Batch  646/718   train_loss = 2.395\n",
      "Epoch 804 Batch   28/718   train_loss = 2.315\n",
      "Epoch 804 Batch  128/718   train_loss = 2.339\n",
      "Epoch 804 Batch  228/718   train_loss = 2.439\n",
      "Epoch 804 Batch  328/718   train_loss = 2.277\n",
      "Epoch 804 Batch  428/718   train_loss = 2.284\n",
      "Epoch 804 Batch  528/718   train_loss = 2.404\n",
      "Epoch 804 Batch  628/718   train_loss = 2.469\n",
      "Epoch 805 Batch   10/718   train_loss = 2.318\n",
      "Epoch 805 Batch  110/718   train_loss = 2.355\n",
      "Epoch 805 Batch  210/718   train_loss = 2.313\n",
      "Epoch 805 Batch  310/718   train_loss = 2.444\n",
      "Epoch 805 Batch  410/718   train_loss = 2.286\n",
      "Epoch 805 Batch  510/718   train_loss = 2.426\n",
      "Epoch 805 Batch  610/718   train_loss = 2.418\n",
      "Epoch 805 Batch  710/718   train_loss = 2.310\n",
      "Epoch 806 Batch   92/718   train_loss = 2.280\n",
      "Epoch 806 Batch  192/718   train_loss = 2.403\n",
      "Epoch 806 Batch  292/718   train_loss = 2.228\n",
      "Epoch 806 Batch  392/718   train_loss = 2.338\n",
      "Epoch 806 Batch  492/718   train_loss = 2.439\n",
      "Epoch 806 Batch  592/718   train_loss = 2.409\n",
      "Epoch 806 Batch  692/718   train_loss = 2.389\n",
      "Epoch 807 Batch   74/718   train_loss = 2.471\n",
      "Epoch 807 Batch  174/718   train_loss = 2.451\n",
      "Epoch 807 Batch  274/718   train_loss = 2.387\n",
      "Epoch 807 Batch  374/718   train_loss = 2.286\n",
      "Epoch 807 Batch  474/718   train_loss = 2.426\n",
      "Epoch 807 Batch  574/718   train_loss = 2.233\n",
      "Epoch 807 Batch  674/718   train_loss = 2.302\n",
      "Epoch 808 Batch   56/718   train_loss = 2.455\n",
      "Epoch 808 Batch  156/718   train_loss = 2.341\n",
      "Epoch 808 Batch  256/718   train_loss = 2.280\n",
      "Epoch 808 Batch  356/718   train_loss = 2.326\n",
      "Epoch 808 Batch  456/718   train_loss = 2.318\n",
      "Epoch 808 Batch  556/718   train_loss = 2.320\n",
      "Epoch 808 Batch  656/718   train_loss = 2.305\n",
      "Epoch 809 Batch   38/718   train_loss = 2.383\n",
      "Epoch 809 Batch  138/718   train_loss = 2.401\n",
      "Epoch 809 Batch  238/718   train_loss = 2.412\n",
      "Epoch 809 Batch  338/718   train_loss = 2.430\n",
      "Epoch 809 Batch  438/718   train_loss = 2.267\n",
      "Epoch 809 Batch  538/718   train_loss = 2.351\n",
      "Epoch 809 Batch  638/718   train_loss = 2.335\n",
      "Epoch 810 Batch   20/718   train_loss = 2.367\n",
      "Epoch 810 Batch  120/718   train_loss = 2.334\n",
      "Epoch 810 Batch  220/718   train_loss = 2.333\n",
      "Epoch 810 Batch  320/718   train_loss = 2.389\n",
      "Epoch 810 Batch  420/718   train_loss = 2.494\n",
      "Epoch 810 Batch  520/718   train_loss = 2.425\n",
      "Epoch 810 Batch  620/718   train_loss = 2.518\n",
      "Epoch 811 Batch    2/718   train_loss = 2.411\n",
      "Epoch 811 Batch  102/718   train_loss = 2.288\n",
      "Epoch 811 Batch  202/718   train_loss = 2.378\n",
      "Epoch 811 Batch  302/718   train_loss = 2.449\n",
      "Epoch 811 Batch  402/718   train_loss = 2.396\n",
      "Epoch 811 Batch  502/718   train_loss = 2.361\n",
      "Epoch 811 Batch  602/718   train_loss = 2.329\n",
      "Epoch 811 Batch  702/718   train_loss = 2.288\n",
      "Epoch 812 Batch   84/718   train_loss = 2.298\n",
      "Epoch 812 Batch  184/718   train_loss = 2.358\n",
      "Epoch 812 Batch  284/718   train_loss = 2.345\n",
      "Epoch 812 Batch  384/718   train_loss = 2.348\n",
      "Epoch 812 Batch  484/718   train_loss = 2.355\n",
      "Epoch 812 Batch  584/718   train_loss = 2.318\n",
      "Epoch 812 Batch  684/718   train_loss = 2.357\n",
      "Epoch 813 Batch   66/718   train_loss = 2.270\n",
      "Epoch 813 Batch  166/718   train_loss = 2.324\n",
      "Epoch 813 Batch  266/718   train_loss = 2.442\n",
      "Epoch 813 Batch  366/718   train_loss = 2.470\n",
      "Epoch 813 Batch  466/718   train_loss = 2.378\n",
      "Epoch 813 Batch  566/718   train_loss = 2.340\n",
      "Epoch 813 Batch  666/718   train_loss = 2.328\n",
      "Epoch 814 Batch   48/718   train_loss = 2.235\n",
      "Epoch 814 Batch  148/718   train_loss = 2.364\n",
      "Epoch 814 Batch  248/718   train_loss = 2.398\n",
      "Epoch 814 Batch  348/718   train_loss = 2.318\n",
      "Epoch 814 Batch  448/718   train_loss = 2.351\n",
      "Epoch 814 Batch  548/718   train_loss = 2.425\n",
      "Epoch 814 Batch  648/718   train_loss = 2.374\n",
      "Epoch 815 Batch   30/718   train_loss = 2.334\n",
      "Epoch 815 Batch  130/718   train_loss = 2.225\n",
      "Epoch 815 Batch  230/718   train_loss = 2.431\n",
      "Epoch 815 Batch  330/718   train_loss = 2.432\n",
      "Epoch 815 Batch  430/718   train_loss = 2.469\n",
      "Epoch 815 Batch  530/718   train_loss = 2.351\n",
      "Epoch 815 Batch  630/718   train_loss = 2.312\n",
      "Epoch 816 Batch   12/718   train_loss = 2.254\n",
      "Epoch 816 Batch  112/718   train_loss = 2.402\n",
      "Epoch 816 Batch  212/718   train_loss = 2.445\n",
      "Epoch 816 Batch  312/718   train_loss = 2.380\n",
      "Epoch 816 Batch  412/718   train_loss = 2.320\n",
      "Epoch 816 Batch  512/718   train_loss = 2.368\n",
      "Epoch 816 Batch  612/718   train_loss = 2.461\n",
      "Epoch 816 Batch  712/718   train_loss = 2.428\n",
      "Epoch 817 Batch   94/718   train_loss = 2.212\n",
      "Epoch 817 Batch  194/718   train_loss = 2.287\n",
      "Epoch 817 Batch  294/718   train_loss = 2.426\n",
      "Epoch 817 Batch  394/718   train_loss = 2.352\n",
      "Epoch 817 Batch  494/718   train_loss = 2.483\n",
      "Epoch 817 Batch  594/718   train_loss = 2.433\n",
      "Epoch 817 Batch  694/718   train_loss = 2.486\n",
      "Epoch 818 Batch   76/718   train_loss = 2.298\n",
      "Epoch 818 Batch  176/718   train_loss = 2.404\n",
      "Epoch 818 Batch  276/718   train_loss = 2.394\n",
      "Epoch 818 Batch  376/718   train_loss = 2.357\n",
      "Epoch 818 Batch  476/718   train_loss = 2.390\n",
      "Epoch 818 Batch  576/718   train_loss = 2.411\n",
      "Epoch 818 Batch  676/718   train_loss = 2.380\n",
      "Epoch 819 Batch   58/718   train_loss = 2.381\n",
      "Epoch 819 Batch  158/718   train_loss = 2.396\n",
      "Epoch 819 Batch  258/718   train_loss = 2.350\n",
      "Epoch 819 Batch  358/718   train_loss = 2.267\n",
      "Epoch 819 Batch  458/718   train_loss = 2.463\n",
      "Epoch 819 Batch  558/718   train_loss = 2.460\n",
      "Epoch 819 Batch  658/718   train_loss = 2.293\n",
      "Epoch 820 Batch   40/718   train_loss = 2.418\n",
      "Epoch 820 Batch  140/718   train_loss = 2.342\n",
      "Epoch 820 Batch  240/718   train_loss = 2.281\n",
      "Epoch 820 Batch  340/718   train_loss = 2.428\n",
      "Epoch 820 Batch  440/718   train_loss = 2.346\n",
      "Epoch 820 Batch  540/718   train_loss = 2.429\n",
      "Epoch 820 Batch  640/718   train_loss = 2.307\n",
      "Epoch 821 Batch   22/718   train_loss = 2.209\n",
      "Epoch 821 Batch  122/718   train_loss = 2.349\n",
      "Epoch 821 Batch  222/718   train_loss = 2.355\n",
      "Epoch 821 Batch  322/718   train_loss = 2.375\n",
      "Epoch 821 Batch  422/718   train_loss = 2.424\n",
      "Epoch 821 Batch  522/718   train_loss = 2.368\n",
      "Epoch 821 Batch  622/718   train_loss = 2.389\n",
      "Epoch 822 Batch    4/718   train_loss = 2.266\n",
      "Epoch 822 Batch  104/718   train_loss = 2.355\n",
      "Epoch 822 Batch  204/718   train_loss = 2.243\n",
      "Epoch 822 Batch  304/718   train_loss = 2.328\n",
      "Epoch 822 Batch  404/718   train_loss = 2.321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 822 Batch  504/718   train_loss = 2.453\n",
      "Epoch 822 Batch  604/718   train_loss = 2.276\n",
      "Epoch 822 Batch  704/718   train_loss = 2.368\n",
      "Epoch 823 Batch   86/718   train_loss = 2.390\n",
      "Epoch 823 Batch  186/718   train_loss = 2.346\n",
      "Epoch 823 Batch  286/718   train_loss = 2.455\n",
      "Epoch 823 Batch  386/718   train_loss = 2.350\n",
      "Epoch 823 Batch  486/718   train_loss = 2.398\n",
      "Epoch 823 Batch  586/718   train_loss = 2.351\n",
      "Epoch 823 Batch  686/718   train_loss = 2.349\n",
      "Epoch 824 Batch   68/718   train_loss = 2.397\n",
      "Epoch 824 Batch  168/718   train_loss = 2.473\n",
      "Epoch 824 Batch  268/718   train_loss = 2.348\n",
      "Epoch 824 Batch  368/718   train_loss = 2.306\n",
      "Epoch 824 Batch  468/718   train_loss = 2.580\n",
      "Epoch 824 Batch  568/718   train_loss = 2.488\n",
      "Epoch 824 Batch  668/718   train_loss = 2.334\n",
      "Epoch 825 Batch   50/718   train_loss = 2.452\n",
      "Epoch 825 Batch  150/718   train_loss = 2.323\n",
      "Epoch 825 Batch  250/718   train_loss = 2.325\n",
      "Epoch 825 Batch  350/718   train_loss = 2.325\n",
      "Epoch 825 Batch  450/718   train_loss = 2.392\n",
      "Epoch 825 Batch  550/718   train_loss = 2.279\n",
      "Epoch 825 Batch  650/718   train_loss = 2.321\n",
      "Epoch 826 Batch   32/718   train_loss = 2.317\n",
      "Epoch 826 Batch  132/718   train_loss = 2.372\n",
      "Epoch 826 Batch  232/718   train_loss = 2.372\n",
      "Epoch 826 Batch  332/718   train_loss = 2.281\n",
      "Epoch 826 Batch  432/718   train_loss = 2.481\n",
      "Epoch 826 Batch  532/718   train_loss = 2.494\n",
      "Epoch 826 Batch  632/718   train_loss = 2.281\n",
      "Epoch 827 Batch   14/718   train_loss = 2.397\n",
      "Epoch 827 Batch  114/718   train_loss = 2.301\n",
      "Epoch 827 Batch  214/718   train_loss = 2.395\n",
      "Epoch 827 Batch  314/718   train_loss = 2.413\n",
      "Epoch 827 Batch  414/718   train_loss = 2.238\n",
      "Epoch 827 Batch  514/718   train_loss = 2.489\n",
      "Epoch 827 Batch  614/718   train_loss = 2.344\n",
      "Epoch 827 Batch  714/718   train_loss = 2.367\n",
      "Epoch 828 Batch   96/718   train_loss = 2.344\n",
      "Epoch 828 Batch  196/718   train_loss = 2.296\n",
      "Epoch 828 Batch  296/718   train_loss = 2.293\n",
      "Epoch 828 Batch  396/718   train_loss = 2.400\n",
      "Epoch 828 Batch  496/718   train_loss = 2.452\n",
      "Epoch 828 Batch  596/718   train_loss = 2.380\n",
      "Epoch 828 Batch  696/718   train_loss = 2.379\n",
      "Epoch 829 Batch   78/718   train_loss = 2.409\n",
      "Epoch 829 Batch  178/718   train_loss = 2.364\n",
      "Epoch 829 Batch  278/718   train_loss = 2.281\n",
      "Epoch 829 Batch  378/718   train_loss = 2.229\n",
      "Epoch 829 Batch  478/718   train_loss = 2.279\n",
      "Epoch 829 Batch  578/718   train_loss = 2.288\n",
      "Epoch 829 Batch  678/718   train_loss = 2.379\n",
      "Epoch 830 Batch   60/718   train_loss = 2.402\n",
      "Epoch 830 Batch  160/718   train_loss = 2.385\n",
      "Epoch 830 Batch  260/718   train_loss = 2.359\n",
      "Epoch 830 Batch  360/718   train_loss = 2.383\n",
      "Epoch 830 Batch  460/718   train_loss = 2.319\n",
      "Epoch 830 Batch  560/718   train_loss = 2.339\n",
      "Epoch 830 Batch  660/718   train_loss = 2.500\n",
      "Epoch 831 Batch   42/718   train_loss = 2.409\n",
      "Epoch 831 Batch  142/718   train_loss = 2.384\n",
      "Epoch 831 Batch  242/718   train_loss = 2.256\n",
      "Epoch 831 Batch  342/718   train_loss = 2.326\n",
      "Epoch 831 Batch  442/718   train_loss = 2.363\n",
      "Epoch 831 Batch  542/718   train_loss = 2.293\n",
      "Epoch 831 Batch  642/718   train_loss = 2.450\n",
      "Epoch 832 Batch   24/718   train_loss = 2.371\n",
      "Epoch 832 Batch  124/718   train_loss = 2.239\n",
      "Epoch 832 Batch  224/718   train_loss = 2.355\n",
      "Epoch 832 Batch  324/718   train_loss = 2.313\n",
      "Epoch 832 Batch  424/718   train_loss = 2.345\n",
      "Epoch 832 Batch  524/718   train_loss = 2.377\n",
      "Epoch 832 Batch  624/718   train_loss = 2.390\n",
      "Epoch 833 Batch    6/718   train_loss = 2.306\n",
      "Epoch 833 Batch  106/718   train_loss = 2.363\n",
      "Epoch 833 Batch  206/718   train_loss = 2.265\n",
      "Epoch 833 Batch  306/718   train_loss = 2.292\n",
      "Epoch 833 Batch  406/718   train_loss = 2.333\n",
      "Epoch 833 Batch  506/718   train_loss = 2.314\n",
      "Epoch 833 Batch  606/718   train_loss = 2.268\n",
      "Epoch 833 Batch  706/718   train_loss = 2.318\n",
      "Epoch 834 Batch   88/718   train_loss = 2.425\n",
      "Epoch 834 Batch  188/718   train_loss = 2.313\n",
      "Epoch 834 Batch  288/718   train_loss = 2.341\n",
      "Epoch 834 Batch  388/718   train_loss = 2.266\n",
      "Epoch 834 Batch  488/718   train_loss = 2.269\n",
      "Epoch 834 Batch  588/718   train_loss = 2.464\n",
      "Epoch 834 Batch  688/718   train_loss = 2.337\n",
      "Epoch 835 Batch   70/718   train_loss = 2.353\n",
      "Epoch 835 Batch  170/718   train_loss = 2.341\n",
      "Epoch 835 Batch  270/718   train_loss = 2.432\n",
      "Epoch 835 Batch  370/718   train_loss = 2.359\n",
      "Epoch 835 Batch  470/718   train_loss = 2.526\n",
      "Epoch 835 Batch  570/718   train_loss = 2.412\n",
      "Epoch 835 Batch  670/718   train_loss = 2.301\n",
      "Epoch 836 Batch   52/718   train_loss = 2.329\n",
      "Epoch 836 Batch  152/718   train_loss = 2.284\n",
      "Epoch 836 Batch  252/718   train_loss = 2.424\n",
      "Epoch 836 Batch  352/718   train_loss = 2.304\n",
      "Epoch 836 Batch  452/718   train_loss = 2.340\n",
      "Epoch 836 Batch  552/718   train_loss = 2.430\n",
      "Epoch 836 Batch  652/718   train_loss = 2.294\n",
      "Epoch 837 Batch   34/718   train_loss = 2.335\n",
      "Epoch 837 Batch  134/718   train_loss = 2.277\n",
      "Epoch 837 Batch  234/718   train_loss = 2.424\n",
      "Epoch 837 Batch  334/718   train_loss = 2.440\n",
      "Epoch 837 Batch  434/718   train_loss = 2.416\n",
      "Epoch 837 Batch  534/718   train_loss = 2.310\n",
      "Epoch 837 Batch  634/718   train_loss = 2.464\n",
      "Epoch 838 Batch   16/718   train_loss = 2.340\n",
      "Epoch 838 Batch  116/718   train_loss = 2.270\n",
      "Epoch 838 Batch  216/718   train_loss = 2.300\n",
      "Epoch 838 Batch  316/718   train_loss = 2.243\n",
      "Epoch 838 Batch  416/718   train_loss = 2.359\n",
      "Epoch 838 Batch  516/718   train_loss = 2.362\n",
      "Epoch 838 Batch  616/718   train_loss = 2.454\n",
      "Epoch 838 Batch  716/718   train_loss = 2.433\n",
      "Epoch 839 Batch   98/718   train_loss = 2.314\n",
      "Epoch 839 Batch  198/718   train_loss = 2.342\n",
      "Epoch 839 Batch  298/718   train_loss = 2.298\n",
      "Epoch 839 Batch  398/718   train_loss = 2.353\n",
      "Epoch 839 Batch  498/718   train_loss = 2.352\n",
      "Epoch 839 Batch  598/718   train_loss = 2.422\n",
      "Epoch 839 Batch  698/718   train_loss = 2.440\n",
      "Epoch 840 Batch   80/718   train_loss = 2.338\n",
      "Epoch 840 Batch  180/718   train_loss = 2.324\n",
      "Epoch 840 Batch  280/718   train_loss = 2.445\n",
      "Epoch 840 Batch  380/718   train_loss = 2.337\n",
      "Epoch 840 Batch  480/718   train_loss = 2.425\n",
      "Epoch 840 Batch  580/718   train_loss = 2.295\n",
      "Epoch 840 Batch  680/718   train_loss = 2.420\n",
      "Epoch 841 Batch   62/718   train_loss = 2.323\n",
      "Epoch 841 Batch  162/718   train_loss = 2.347\n",
      "Epoch 841 Batch  262/718   train_loss = 2.450\n",
      "Epoch 841 Batch  362/718   train_loss = 2.452\n",
      "Epoch 841 Batch  462/718   train_loss = 2.379\n",
      "Epoch 841 Batch  562/718   train_loss = 2.315\n",
      "Epoch 841 Batch  662/718   train_loss = 2.371\n",
      "Epoch 842 Batch   44/718   train_loss = 2.349\n",
      "Epoch 842 Batch  144/718   train_loss = 2.426\n",
      "Epoch 842 Batch  244/718   train_loss = 2.471\n",
      "Epoch 842 Batch  344/718   train_loss = 2.384\n",
      "Epoch 842 Batch  444/718   train_loss = 2.295\n",
      "Epoch 842 Batch  544/718   train_loss = 2.354\n",
      "Epoch 842 Batch  644/718   train_loss = 2.479\n",
      "Epoch 843 Batch   26/718   train_loss = 2.493\n",
      "Epoch 843 Batch  126/718   train_loss = 2.296\n",
      "Epoch 843 Batch  226/718   train_loss = 2.295\n",
      "Epoch 843 Batch  326/718   train_loss = 2.312\n",
      "Epoch 843 Batch  426/718   train_loss = 2.331\n",
      "Epoch 843 Batch  526/718   train_loss = 2.320\n",
      "Epoch 843 Batch  626/718   train_loss = 2.510\n",
      "Epoch 844 Batch    8/718   train_loss = 2.235\n",
      "Epoch 844 Batch  108/718   train_loss = 2.267\n",
      "Epoch 844 Batch  208/718   train_loss = 2.336\n",
      "Epoch 844 Batch  308/718   train_loss = 2.396\n",
      "Epoch 844 Batch  408/718   train_loss = 2.439\n",
      "Epoch 844 Batch  508/718   train_loss = 2.419\n",
      "Epoch 844 Batch  608/718   train_loss = 2.501\n",
      "Epoch 844 Batch  708/718   train_loss = 2.323\n",
      "Epoch 845 Batch   90/718   train_loss = 2.269\n",
      "Epoch 845 Batch  190/718   train_loss = 2.441\n",
      "Epoch 845 Batch  290/718   train_loss = 2.270\n",
      "Epoch 845 Batch  390/718   train_loss = 2.286\n",
      "Epoch 845 Batch  490/718   train_loss = 2.381\n",
      "Epoch 845 Batch  590/718   train_loss = 2.390\n",
      "Epoch 845 Batch  690/718   train_loss = 2.452\n",
      "Epoch 846 Batch   72/718   train_loss = 2.409\n",
      "Epoch 846 Batch  172/718   train_loss = 2.396\n",
      "Epoch 846 Batch  272/718   train_loss = 2.432\n",
      "Epoch 846 Batch  372/718   train_loss = 2.383\n",
      "Epoch 846 Batch  472/718   train_loss = 2.416\n",
      "Epoch 846 Batch  572/718   train_loss = 2.445\n",
      "Epoch 846 Batch  672/718   train_loss = 2.309\n",
      "Epoch 847 Batch   54/718   train_loss = 2.238\n",
      "Epoch 847 Batch  154/718   train_loss = 2.473\n",
      "Epoch 847 Batch  254/718   train_loss = 2.335\n",
      "Epoch 847 Batch  354/718   train_loss = 2.355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 847 Batch  454/718   train_loss = 2.319\n",
      "Epoch 847 Batch  554/718   train_loss = 2.498\n",
      "Epoch 847 Batch  654/718   train_loss = 2.393\n",
      "Epoch 848 Batch   36/718   train_loss = 2.391\n",
      "Epoch 848 Batch  136/718   train_loss = 2.320\n",
      "Epoch 848 Batch  236/718   train_loss = 2.287\n",
      "Epoch 848 Batch  336/718   train_loss = 2.400\n",
      "Epoch 848 Batch  436/718   train_loss = 2.392\n",
      "Epoch 848 Batch  536/718   train_loss = 2.386\n",
      "Epoch 848 Batch  636/718   train_loss = 2.441\n",
      "Epoch 849 Batch   18/718   train_loss = 2.405\n",
      "Epoch 849 Batch  118/718   train_loss = 2.336\n",
      "Epoch 849 Batch  218/718   train_loss = 2.437\n",
      "Epoch 849 Batch  318/718   train_loss = 2.389\n",
      "Epoch 849 Batch  418/718   train_loss = 2.533\n",
      "Epoch 849 Batch  518/718   train_loss = 2.399\n",
      "Epoch 849 Batch  618/718   train_loss = 2.367\n",
      "Epoch 850 Batch    0/718   train_loss = 2.431\n",
      "Epoch 850 Batch  100/718   train_loss = 2.360\n",
      "Epoch 850 Batch  200/718   train_loss = 2.289\n",
      "Epoch 850 Batch  300/718   train_loss = 2.327\n",
      "Epoch 850 Batch  400/718   train_loss = 2.431\n",
      "Epoch 850 Batch  500/718   train_loss = 2.490\n",
      "Epoch 850 Batch  600/718   train_loss = 2.428\n",
      "Epoch 850 Batch  700/718   train_loss = 2.315\n",
      "Epoch 851 Batch   82/718   train_loss = 2.299\n",
      "Epoch 851 Batch  182/718   train_loss = 2.425\n",
      "Epoch 851 Batch  282/718   train_loss = 2.444\n",
      "Epoch 851 Batch  382/718   train_loss = 2.301\n",
      "Epoch 851 Batch  482/718   train_loss = 2.474\n",
      "Epoch 851 Batch  582/718   train_loss = 2.291\n",
      "Epoch 851 Batch  682/718   train_loss = 2.379\n",
      "Epoch 852 Batch   64/718   train_loss = 2.327\n",
      "Epoch 852 Batch  164/718   train_loss = 2.238\n",
      "Epoch 852 Batch  264/718   train_loss = 2.359\n",
      "Epoch 852 Batch  364/718   train_loss = 2.513\n",
      "Epoch 852 Batch  464/718   train_loss = 2.279\n",
      "Epoch 852 Batch  564/718   train_loss = 2.378\n",
      "Epoch 852 Batch  664/718   train_loss = 2.428\n",
      "Epoch 853 Batch   46/718   train_loss = 2.291\n",
      "Epoch 853 Batch  146/718   train_loss = 2.215\n",
      "Epoch 853 Batch  246/718   train_loss = 2.444\n",
      "Epoch 853 Batch  346/718   train_loss = 2.254\n",
      "Epoch 853 Batch  446/718   train_loss = 2.327\n",
      "Epoch 853 Batch  546/718   train_loss = 2.307\n",
      "Epoch 853 Batch  646/718   train_loss = 2.412\n",
      "Epoch 854 Batch   28/718   train_loss = 2.288\n",
      "Epoch 854 Batch  128/718   train_loss = 2.362\n",
      "Epoch 854 Batch  228/718   train_loss = 2.404\n",
      "Epoch 854 Batch  328/718   train_loss = 2.305\n",
      "Epoch 854 Batch  428/718   train_loss = 2.270\n",
      "Epoch 854 Batch  528/718   train_loss = 2.426\n",
      "Epoch 854 Batch  628/718   train_loss = 2.431\n",
      "Epoch 855 Batch   10/718   train_loss = 2.361\n",
      "Epoch 855 Batch  110/718   train_loss = 2.364\n",
      "Epoch 855 Batch  210/718   train_loss = 2.315\n",
      "Epoch 855 Batch  310/718   train_loss = 2.451\n",
      "Epoch 855 Batch  410/718   train_loss = 2.230\n",
      "Epoch 855 Batch  510/718   train_loss = 2.409\n",
      "Epoch 855 Batch  610/718   train_loss = 2.410\n",
      "Epoch 855 Batch  710/718   train_loss = 2.288\n",
      "Epoch 856 Batch   92/718   train_loss = 2.327\n",
      "Epoch 856 Batch  192/718   train_loss = 2.334\n",
      "Epoch 856 Batch  292/718   train_loss = 2.245\n",
      "Epoch 856 Batch  392/718   train_loss = 2.304\n",
      "Epoch 856 Batch  492/718   train_loss = 2.389\n",
      "Epoch 856 Batch  592/718   train_loss = 2.424\n",
      "Epoch 856 Batch  692/718   train_loss = 2.344\n",
      "Epoch 857 Batch   74/718   train_loss = 2.441\n",
      "Epoch 857 Batch  174/718   train_loss = 2.464\n",
      "Epoch 857 Batch  274/718   train_loss = 2.418\n",
      "Epoch 857 Batch  374/718   train_loss = 2.317\n",
      "Epoch 857 Batch  474/718   train_loss = 2.402\n",
      "Epoch 857 Batch  574/718   train_loss = 2.261\n",
      "Epoch 857 Batch  674/718   train_loss = 2.312\n",
      "Epoch 858 Batch   56/718   train_loss = 2.459\n",
      "Epoch 858 Batch  156/718   train_loss = 2.327\n",
      "Epoch 858 Batch  256/718   train_loss = 2.255\n",
      "Epoch 858 Batch  356/718   train_loss = 2.320\n",
      "Epoch 858 Batch  456/718   train_loss = 2.326\n",
      "Epoch 858 Batch  556/718   train_loss = 2.281\n",
      "Epoch 858 Batch  656/718   train_loss = 2.291\n",
      "Epoch 859 Batch   38/718   train_loss = 2.376\n",
      "Epoch 859 Batch  138/718   train_loss = 2.374\n",
      "Epoch 859 Batch  238/718   train_loss = 2.400\n",
      "Epoch 859 Batch  338/718   train_loss = 2.387\n",
      "Epoch 859 Batch  438/718   train_loss = 2.267\n",
      "Epoch 859 Batch  538/718   train_loss = 2.341\n",
      "Epoch 859 Batch  638/718   train_loss = 2.302\n",
      "Epoch 860 Batch   20/718   train_loss = 2.349\n",
      "Epoch 860 Batch  120/718   train_loss = 2.361\n",
      "Epoch 860 Batch  220/718   train_loss = 2.325\n",
      "Epoch 860 Batch  320/718   train_loss = 2.377\n",
      "Epoch 860 Batch  420/718   train_loss = 2.494\n",
      "Epoch 860 Batch  520/718   train_loss = 2.405\n",
      "Epoch 860 Batch  620/718   train_loss = 2.500\n",
      "Epoch 861 Batch    2/718   train_loss = 2.412\n",
      "Epoch 861 Batch  102/718   train_loss = 2.264\n",
      "Epoch 861 Batch  202/718   train_loss = 2.346\n",
      "Epoch 861 Batch  302/718   train_loss = 2.437\n",
      "Epoch 861 Batch  402/718   train_loss = 2.410\n",
      "Epoch 861 Batch  502/718   train_loss = 2.351\n",
      "Epoch 861 Batch  602/718   train_loss = 2.313\n",
      "Epoch 861 Batch  702/718   train_loss = 2.305\n",
      "Epoch 862 Batch   84/718   train_loss = 2.263\n",
      "Epoch 862 Batch  184/718   train_loss = 2.351\n",
      "Epoch 862 Batch  284/718   train_loss = 2.334\n",
      "Epoch 862 Batch  384/718   train_loss = 2.344\n",
      "Epoch 862 Batch  484/718   train_loss = 2.358\n",
      "Epoch 862 Batch  584/718   train_loss = 2.280\n",
      "Epoch 862 Batch  684/718   train_loss = 2.349\n",
      "Epoch 863 Batch   66/718   train_loss = 2.279\n",
      "Epoch 863 Batch  166/718   train_loss = 2.379\n",
      "Epoch 863 Batch  266/718   train_loss = 2.433\n",
      "Epoch 863 Batch  366/718   train_loss = 2.470\n",
      "Epoch 863 Batch  466/718   train_loss = 2.327\n",
      "Epoch 863 Batch  566/718   train_loss = 2.335\n",
      "Epoch 863 Batch  666/718   train_loss = 2.272\n",
      "Epoch 864 Batch   48/718   train_loss = 2.245\n",
      "Epoch 864 Batch  148/718   train_loss = 2.343\n",
      "Epoch 864 Batch  248/718   train_loss = 2.399\n",
      "Epoch 864 Batch  348/718   train_loss = 2.288\n",
      "Epoch 864 Batch  448/718   train_loss = 2.367\n",
      "Epoch 864 Batch  548/718   train_loss = 2.419\n",
      "Epoch 864 Batch  648/718   train_loss = 2.330\n",
      "Epoch 865 Batch   30/718   train_loss = 2.363\n",
      "Epoch 865 Batch  130/718   train_loss = 2.266\n",
      "Epoch 865 Batch  230/718   train_loss = 2.426\n",
      "Epoch 865 Batch  330/718   train_loss = 2.430\n",
      "Epoch 865 Batch  430/718   train_loss = 2.476\n",
      "Epoch 865 Batch  530/718   train_loss = 2.321\n",
      "Epoch 865 Batch  630/718   train_loss = 2.316\n",
      "Epoch 866 Batch   12/718   train_loss = 2.276\n",
      "Epoch 866 Batch  112/718   train_loss = 2.361\n",
      "Epoch 866 Batch  212/718   train_loss = 2.421\n",
      "Epoch 866 Batch  312/718   train_loss = 2.359\n",
      "Epoch 866 Batch  412/718   train_loss = 2.290\n",
      "Epoch 866 Batch  512/718   train_loss = 2.346\n",
      "Epoch 866 Batch  612/718   train_loss = 2.455\n",
      "Epoch 866 Batch  712/718   train_loss = 2.453\n",
      "Epoch 867 Batch   94/718   train_loss = 2.219\n",
      "Epoch 867 Batch  194/718   train_loss = 2.290\n",
      "Epoch 867 Batch  294/718   train_loss = 2.427\n",
      "Epoch 867 Batch  394/718   train_loss = 2.331\n",
      "Epoch 867 Batch  494/718   train_loss = 2.469\n",
      "Epoch 867 Batch  594/718   train_loss = 2.438\n",
      "Epoch 867 Batch  694/718   train_loss = 2.444\n",
      "Epoch 868 Batch   76/718   train_loss = 2.267\n",
      "Epoch 868 Batch  176/718   train_loss = 2.381\n",
      "Epoch 868 Batch  276/718   train_loss = 2.352\n",
      "Epoch 868 Batch  376/718   train_loss = 2.380\n",
      "Epoch 868 Batch  476/718   train_loss = 2.383\n",
      "Epoch 868 Batch  576/718   train_loss = 2.387\n",
      "Epoch 868 Batch  676/718   train_loss = 2.335\n",
      "Epoch 869 Batch   58/718   train_loss = 2.383\n",
      "Epoch 869 Batch  158/718   train_loss = 2.373\n",
      "Epoch 869 Batch  258/718   train_loss = 2.327\n",
      "Epoch 869 Batch  358/718   train_loss = 2.314\n",
      "Epoch 869 Batch  458/718   train_loss = 2.428\n",
      "Epoch 869 Batch  558/718   train_loss = 2.482\n",
      "Epoch 869 Batch  658/718   train_loss = 2.226\n",
      "Epoch 870 Batch   40/718   train_loss = 2.410\n",
      "Epoch 870 Batch  140/718   train_loss = 2.330\n",
      "Epoch 870 Batch  240/718   train_loss = 2.240\n",
      "Epoch 870 Batch  340/718   train_loss = 2.345\n",
      "Epoch 870 Batch  440/718   train_loss = 2.339\n",
      "Epoch 870 Batch  540/718   train_loss = 2.433\n",
      "Epoch 870 Batch  640/718   train_loss = 2.343\n",
      "Epoch 871 Batch   22/718   train_loss = 2.187\n",
      "Epoch 871 Batch  122/718   train_loss = 2.345\n",
      "Epoch 871 Batch  222/718   train_loss = 2.310\n",
      "Epoch 871 Batch  322/718   train_loss = 2.371\n",
      "Epoch 871 Batch  422/718   train_loss = 2.362\n",
      "Epoch 871 Batch  522/718   train_loss = 2.369\n",
      "Epoch 871 Batch  622/718   train_loss = 2.395\n",
      "Epoch 872 Batch    4/718   train_loss = 2.284\n",
      "Epoch 872 Batch  104/718   train_loss = 2.367\n",
      "Epoch 872 Batch  204/718   train_loss = 2.270\n",
      "Epoch 872 Batch  304/718   train_loss = 2.337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 872 Batch  404/718   train_loss = 2.286\n",
      "Epoch 872 Batch  504/718   train_loss = 2.518\n",
      "Epoch 872 Batch  604/718   train_loss = 2.228\n",
      "Epoch 872 Batch  704/718   train_loss = 2.353\n",
      "Epoch 873 Batch   86/718   train_loss = 2.382\n",
      "Epoch 873 Batch  186/718   train_loss = 2.339\n",
      "Epoch 873 Batch  286/718   train_loss = 2.445\n",
      "Epoch 873 Batch  386/718   train_loss = 2.343\n",
      "Epoch 873 Batch  486/718   train_loss = 2.358\n",
      "Epoch 873 Batch  586/718   train_loss = 2.329\n",
      "Epoch 873 Batch  686/718   train_loss = 2.310\n",
      "Epoch 874 Batch   68/718   train_loss = 2.386\n",
      "Epoch 874 Batch  168/718   train_loss = 2.456\n",
      "Epoch 874 Batch  268/718   train_loss = 2.323\n",
      "Epoch 874 Batch  368/718   train_loss = 2.310\n",
      "Epoch 874 Batch  468/718   train_loss = 2.580\n",
      "Epoch 874 Batch  568/718   train_loss = 2.462\n",
      "Epoch 874 Batch  668/718   train_loss = 2.341\n",
      "Epoch 875 Batch   50/718   train_loss = 2.432\n",
      "Epoch 875 Batch  150/718   train_loss = 2.302\n",
      "Epoch 875 Batch  250/718   train_loss = 2.317\n",
      "Epoch 875 Batch  350/718   train_loss = 2.345\n",
      "Epoch 875 Batch  450/718   train_loss = 2.329\n",
      "Epoch 875 Batch  550/718   train_loss = 2.256\n",
      "Epoch 875 Batch  650/718   train_loss = 2.266\n",
      "Epoch 876 Batch   32/718   train_loss = 2.316\n",
      "Epoch 876 Batch  132/718   train_loss = 2.380\n",
      "Epoch 876 Batch  232/718   train_loss = 2.301\n",
      "Epoch 876 Batch  332/718   train_loss = 2.246\n",
      "Epoch 876 Batch  432/718   train_loss = 2.438\n",
      "Epoch 876 Batch  532/718   train_loss = 2.432\n",
      "Epoch 876 Batch  632/718   train_loss = 2.239\n",
      "Epoch 877 Batch   14/718   train_loss = 2.367\n",
      "Epoch 877 Batch  114/718   train_loss = 2.313\n",
      "Epoch 877 Batch  214/718   train_loss = 2.386\n",
      "Epoch 877 Batch  314/718   train_loss = 2.377\n",
      "Epoch 877 Batch  414/718   train_loss = 2.258\n",
      "Epoch 877 Batch  514/718   train_loss = 2.485\n",
      "Epoch 877 Batch  614/718   train_loss = 2.321\n",
      "Epoch 877 Batch  714/718   train_loss = 2.331\n",
      "Epoch 878 Batch   96/718   train_loss = 2.352\n",
      "Epoch 878 Batch  196/718   train_loss = 2.303\n",
      "Epoch 878 Batch  296/718   train_loss = 2.283\n",
      "Epoch 878 Batch  396/718   train_loss = 2.391\n",
      "Epoch 878 Batch  496/718   train_loss = 2.485\n",
      "Epoch 878 Batch  596/718   train_loss = 2.354\n",
      "Epoch 878 Batch  696/718   train_loss = 2.362\n",
      "Epoch 879 Batch   78/718   train_loss = 2.410\n",
      "Epoch 879 Batch  178/718   train_loss = 2.370\n",
      "Epoch 879 Batch  278/718   train_loss = 2.336\n",
      "Epoch 879 Batch  378/718   train_loss = 2.257\n",
      "Epoch 879 Batch  478/718   train_loss = 2.280\n",
      "Epoch 879 Batch  578/718   train_loss = 2.265\n",
      "Epoch 879 Batch  678/718   train_loss = 2.390\n",
      "Epoch 880 Batch   60/718   train_loss = 2.416\n",
      "Epoch 880 Batch  160/718   train_loss = 2.408\n",
      "Epoch 880 Batch  260/718   train_loss = 2.352\n",
      "Epoch 880 Batch  360/718   train_loss = 2.365\n",
      "Epoch 880 Batch  460/718   train_loss = 2.285\n",
      "Epoch 880 Batch  560/718   train_loss = 2.364\n",
      "Epoch 880 Batch  660/718   train_loss = 2.460\n",
      "Epoch 881 Batch   42/718   train_loss = 2.397\n",
      "Epoch 881 Batch  142/718   train_loss = 2.395\n",
      "Epoch 881 Batch  242/718   train_loss = 2.241\n",
      "Epoch 881 Batch  342/718   train_loss = 2.348\n",
      "Epoch 881 Batch  442/718   train_loss = 2.310\n",
      "Epoch 881 Batch  542/718   train_loss = 2.254\n",
      "Epoch 881 Batch  642/718   train_loss = 2.434\n",
      "Epoch 882 Batch   24/718   train_loss = 2.365\n",
      "Epoch 882 Batch  124/718   train_loss = 2.246\n",
      "Epoch 882 Batch  224/718   train_loss = 2.341\n",
      "Epoch 882 Batch  324/718   train_loss = 2.311\n",
      "Epoch 882 Batch  424/718   train_loss = 2.333\n",
      "Epoch 882 Batch  524/718   train_loss = 2.345\n",
      "Epoch 882 Batch  624/718   train_loss = 2.390\n",
      "Epoch 883 Batch    6/718   train_loss = 2.318\n",
      "Epoch 883 Batch  106/718   train_loss = 2.341\n",
      "Epoch 883 Batch  206/718   train_loss = 2.205\n",
      "Epoch 883 Batch  306/718   train_loss = 2.333\n",
      "Epoch 883 Batch  406/718   train_loss = 2.341\n",
      "Epoch 883 Batch  506/718   train_loss = 2.286\n",
      "Epoch 883 Batch  606/718   train_loss = 2.290\n",
      "Epoch 883 Batch  706/718   train_loss = 2.364\n",
      "Epoch 884 Batch   88/718   train_loss = 2.433\n",
      "Epoch 884 Batch  188/718   train_loss = 2.262\n",
      "Epoch 884 Batch  288/718   train_loss = 2.308\n",
      "Epoch 884 Batch  388/718   train_loss = 2.233\n",
      "Epoch 884 Batch  488/718   train_loss = 2.256\n",
      "Epoch 884 Batch  588/718   train_loss = 2.412\n",
      "Epoch 884 Batch  688/718   train_loss = 2.325\n",
      "Epoch 885 Batch   70/718   train_loss = 2.368\n",
      "Epoch 885 Batch  170/718   train_loss = 2.333\n",
      "Epoch 885 Batch  270/718   train_loss = 2.410\n",
      "Epoch 885 Batch  370/718   train_loss = 2.380\n",
      "Epoch 885 Batch  470/718   train_loss = 2.483\n",
      "Epoch 885 Batch  570/718   train_loss = 2.411\n",
      "Epoch 885 Batch  670/718   train_loss = 2.334\n",
      "Epoch 886 Batch   52/718   train_loss = 2.260\n",
      "Epoch 886 Batch  152/718   train_loss = 2.301\n",
      "Epoch 886 Batch  252/718   train_loss = 2.425\n",
      "Epoch 886 Batch  352/718   train_loss = 2.299\n",
      "Epoch 886 Batch  452/718   train_loss = 2.347\n",
      "Epoch 886 Batch  552/718   train_loss = 2.396\n",
      "Epoch 886 Batch  652/718   train_loss = 2.235\n",
      "Epoch 887 Batch   34/718   train_loss = 2.311\n",
      "Epoch 887 Batch  134/718   train_loss = 2.285\n",
      "Epoch 887 Batch  234/718   train_loss = 2.427\n",
      "Epoch 887 Batch  334/718   train_loss = 2.432\n",
      "Epoch 887 Batch  434/718   train_loss = 2.413\n",
      "Epoch 887 Batch  534/718   train_loss = 2.330\n",
      "Epoch 887 Batch  634/718   train_loss = 2.465\n",
      "Epoch 888 Batch   16/718   train_loss = 2.311\n",
      "Epoch 888 Batch  116/718   train_loss = 2.293\n",
      "Epoch 888 Batch  216/718   train_loss = 2.310\n",
      "Epoch 888 Batch  316/718   train_loss = 2.242\n",
      "Epoch 888 Batch  416/718   train_loss = 2.336\n",
      "Epoch 888 Batch  516/718   train_loss = 2.323\n",
      "Epoch 888 Batch  616/718   train_loss = 2.443\n",
      "Epoch 888 Batch  716/718   train_loss = 2.401\n",
      "Epoch 889 Batch   98/718   train_loss = 2.253\n",
      "Epoch 889 Batch  198/718   train_loss = 2.327\n",
      "Epoch 889 Batch  298/718   train_loss = 2.282\n",
      "Epoch 889 Batch  398/718   train_loss = 2.361\n",
      "Epoch 889 Batch  498/718   train_loss = 2.387\n",
      "Epoch 889 Batch  598/718   train_loss = 2.399\n",
      "Epoch 889 Batch  698/718   train_loss = 2.410\n",
      "Epoch 890 Batch   80/718   train_loss = 2.350\n",
      "Epoch 890 Batch  180/718   train_loss = 2.278\n",
      "Epoch 890 Batch  280/718   train_loss = 2.443\n",
      "Epoch 890 Batch  380/718   train_loss = 2.348\n",
      "Epoch 890 Batch  480/718   train_loss = 2.436\n",
      "Epoch 890 Batch  580/718   train_loss = 2.261\n",
      "Epoch 890 Batch  680/718   train_loss = 2.398\n",
      "Epoch 891 Batch   62/718   train_loss = 2.305\n",
      "Epoch 891 Batch  162/718   train_loss = 2.322\n",
      "Epoch 891 Batch  262/718   train_loss = 2.455\n",
      "Epoch 891 Batch  362/718   train_loss = 2.389\n",
      "Epoch 891 Batch  462/718   train_loss = 2.344\n",
      "Epoch 891 Batch  562/718   train_loss = 2.305\n",
      "Epoch 891 Batch  662/718   train_loss = 2.354\n",
      "Epoch 892 Batch   44/718   train_loss = 2.345\n",
      "Epoch 892 Batch  144/718   train_loss = 2.466\n",
      "Epoch 892 Batch  244/718   train_loss = 2.466\n",
      "Epoch 892 Batch  344/718   train_loss = 2.365\n",
      "Epoch 892 Batch  444/718   train_loss = 2.301\n",
      "Epoch 892 Batch  544/718   train_loss = 2.313\n",
      "Epoch 892 Batch  644/718   train_loss = 2.471\n",
      "Epoch 893 Batch   26/718   train_loss = 2.455\n",
      "Epoch 893 Batch  126/718   train_loss = 2.319\n",
      "Epoch 893 Batch  226/718   train_loss = 2.293\n",
      "Epoch 893 Batch  326/718   train_loss = 2.301\n",
      "Epoch 893 Batch  426/718   train_loss = 2.316\n",
      "Epoch 893 Batch  526/718   train_loss = 2.319\n",
      "Epoch 893 Batch  626/718   train_loss = 2.438\n",
      "Epoch 894 Batch    8/718   train_loss = 2.249\n",
      "Epoch 894 Batch  108/718   train_loss = 2.253\n",
      "Epoch 894 Batch  208/718   train_loss = 2.388\n",
      "Epoch 894 Batch  308/718   train_loss = 2.410\n",
      "Epoch 894 Batch  408/718   train_loss = 2.452\n",
      "Epoch 894 Batch  508/718   train_loss = 2.445\n",
      "Epoch 894 Batch  608/718   train_loss = 2.508\n",
      "Epoch 894 Batch  708/718   train_loss = 2.311\n",
      "Epoch 895 Batch   90/718   train_loss = 2.260\n",
      "Epoch 895 Batch  190/718   train_loss = 2.416\n",
      "Epoch 895 Batch  290/718   train_loss = 2.279\n",
      "Epoch 895 Batch  390/718   train_loss = 2.357\n",
      "Epoch 895 Batch  490/718   train_loss = 2.311\n",
      "Epoch 895 Batch  590/718   train_loss = 2.346\n",
      "Epoch 895 Batch  690/718   train_loss = 2.441\n",
      "Epoch 896 Batch   72/718   train_loss = 2.393\n",
      "Epoch 896 Batch  172/718   train_loss = 2.411\n",
      "Epoch 896 Batch  272/718   train_loss = 2.414\n",
      "Epoch 896 Batch  372/718   train_loss = 2.394\n",
      "Epoch 896 Batch  472/718   train_loss = 2.433\n",
      "Epoch 896 Batch  572/718   train_loss = 2.376\n",
      "Epoch 896 Batch  672/718   train_loss = 2.265\n",
      "Epoch 897 Batch   54/718   train_loss = 2.190\n",
      "Epoch 897 Batch  154/718   train_loss = 2.489\n",
      "Epoch 897 Batch  254/718   train_loss = 2.315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 897 Batch  354/718   train_loss = 2.411\n",
      "Epoch 897 Batch  454/718   train_loss = 2.255\n",
      "Epoch 897 Batch  554/718   train_loss = 2.498\n",
      "Epoch 897 Batch  654/718   train_loss = 2.442\n",
      "Epoch 898 Batch   36/718   train_loss = 2.380\n",
      "Epoch 898 Batch  136/718   train_loss = 2.301\n",
      "Epoch 898 Batch  236/718   train_loss = 2.326\n",
      "Epoch 898 Batch  336/718   train_loss = 2.362\n",
      "Epoch 898 Batch  436/718   train_loss = 2.363\n",
      "Epoch 898 Batch  536/718   train_loss = 2.410\n",
      "Epoch 898 Batch  636/718   train_loss = 2.448\n",
      "Epoch 899 Batch   18/718   train_loss = 2.340\n",
      "Epoch 899 Batch  118/718   train_loss = 2.367\n",
      "Epoch 899 Batch  218/718   train_loss = 2.418\n",
      "Epoch 899 Batch  318/718   train_loss = 2.351\n",
      "Epoch 899 Batch  418/718   train_loss = 2.485\n",
      "Epoch 899 Batch  518/718   train_loss = 2.418\n",
      "Epoch 899 Batch  618/718   train_loss = 2.346\n",
      "Epoch 900 Batch    0/718   train_loss = 2.412\n",
      "Epoch 900 Batch  100/718   train_loss = 2.390\n",
      "Epoch 900 Batch  200/718   train_loss = 2.286\n",
      "Epoch 900 Batch  300/718   train_loss = 2.318\n",
      "Epoch 900 Batch  400/718   train_loss = 2.425\n",
      "Epoch 900 Batch  500/718   train_loss = 2.492\n",
      "Epoch 900 Batch  600/718   train_loss = 2.452\n",
      "Epoch 900 Batch  700/718   train_loss = 2.323\n",
      "Epoch 901 Batch   82/718   train_loss = 2.280\n",
      "Epoch 901 Batch  182/718   train_loss = 2.416\n",
      "Epoch 901 Batch  282/718   train_loss = 2.449\n",
      "Epoch 901 Batch  382/718   train_loss = 2.295\n",
      "Epoch 901 Batch  482/718   train_loss = 2.457\n",
      "Epoch 901 Batch  582/718   train_loss = 2.316\n",
      "Epoch 901 Batch  682/718   train_loss = 2.335\n",
      "Epoch 902 Batch   64/718   train_loss = 2.303\n",
      "Epoch 902 Batch  164/718   train_loss = 2.216\n",
      "Epoch 902 Batch  264/718   train_loss = 2.369\n",
      "Epoch 902 Batch  364/718   train_loss = 2.460\n",
      "Epoch 902 Batch  464/718   train_loss = 2.276\n",
      "Epoch 902 Batch  564/718   train_loss = 2.429\n",
      "Epoch 902 Batch  664/718   train_loss = 2.459\n",
      "Epoch 903 Batch   46/718   train_loss = 2.294\n",
      "Epoch 903 Batch  146/718   train_loss = 2.253\n",
      "Epoch 903 Batch  246/718   train_loss = 2.435\n",
      "Epoch 903 Batch  346/718   train_loss = 2.275\n",
      "Epoch 903 Batch  446/718   train_loss = 2.304\n",
      "Epoch 903 Batch  546/718   train_loss = 2.282\n",
      "Epoch 903 Batch  646/718   train_loss = 2.382\n",
      "Epoch 904 Batch   28/718   train_loss = 2.325\n",
      "Epoch 904 Batch  128/718   train_loss = 2.333\n",
      "Epoch 904 Batch  228/718   train_loss = 2.414\n",
      "Epoch 904 Batch  328/718   train_loss = 2.311\n",
      "Epoch 904 Batch  428/718   train_loss = 2.233\n",
      "Epoch 904 Batch  528/718   train_loss = 2.418\n",
      "Epoch 904 Batch  628/718   train_loss = 2.439\n",
      "Epoch 905 Batch   10/718   train_loss = 2.317\n",
      "Epoch 905 Batch  110/718   train_loss = 2.342\n",
      "Epoch 905 Batch  210/718   train_loss = 2.287\n",
      "Epoch 905 Batch  310/718   train_loss = 2.433\n",
      "Epoch 905 Batch  410/718   train_loss = 2.225\n",
      "Epoch 905 Batch  510/718   train_loss = 2.384\n",
      "Epoch 905 Batch  610/718   train_loss = 2.385\n",
      "Epoch 905 Batch  710/718   train_loss = 2.271\n",
      "Epoch 906 Batch   92/718   train_loss = 2.256\n",
      "Epoch 906 Batch  192/718   train_loss = 2.317\n",
      "Epoch 906 Batch  292/718   train_loss = 2.226\n",
      "Epoch 906 Batch  392/718   train_loss = 2.295\n",
      "Epoch 906 Batch  492/718   train_loss = 2.387\n",
      "Epoch 906 Batch  592/718   train_loss = 2.402\n",
      "Epoch 906 Batch  692/718   train_loss = 2.354\n",
      "Epoch 907 Batch   74/718   train_loss = 2.417\n",
      "Epoch 907 Batch  174/718   train_loss = 2.452\n",
      "Epoch 907 Batch  274/718   train_loss = 2.349\n",
      "Epoch 907 Batch  374/718   train_loss = 2.295\n",
      "Epoch 907 Batch  474/718   train_loss = 2.403\n",
      "Epoch 907 Batch  574/718   train_loss = 2.226\n",
      "Epoch 907 Batch  674/718   train_loss = 2.286\n",
      "Epoch 908 Batch   56/718   train_loss = 2.417\n",
      "Epoch 908 Batch  156/718   train_loss = 2.282\n",
      "Epoch 908 Batch  256/718   train_loss = 2.265\n",
      "Epoch 908 Batch  356/718   train_loss = 2.256\n",
      "Epoch 908 Batch  456/718   train_loss = 2.319\n",
      "Epoch 908 Batch  556/718   train_loss = 2.261\n",
      "Epoch 908 Batch  656/718   train_loss = 2.314\n",
      "Epoch 909 Batch   38/718   train_loss = 2.399\n",
      "Epoch 909 Batch  138/718   train_loss = 2.396\n",
      "Epoch 909 Batch  238/718   train_loss = 2.382\n",
      "Epoch 909 Batch  338/718   train_loss = 2.382\n",
      "Epoch 909 Batch  438/718   train_loss = 2.248\n",
      "Epoch 909 Batch  538/718   train_loss = 2.304\n",
      "Epoch 909 Batch  638/718   train_loss = 2.328\n",
      "Epoch 910 Batch   20/718   train_loss = 2.337\n",
      "Epoch 910 Batch  120/718   train_loss = 2.317\n",
      "Epoch 910 Batch  220/718   train_loss = 2.329\n",
      "Epoch 910 Batch  320/718   train_loss = 2.397\n",
      "Epoch 910 Batch  420/718   train_loss = 2.473\n",
      "Epoch 910 Batch  520/718   train_loss = 2.360\n",
      "Epoch 910 Batch  620/718   train_loss = 2.466\n",
      "Epoch 911 Batch    2/718   train_loss = 2.402\n",
      "Epoch 911 Batch  102/718   train_loss = 2.261\n",
      "Epoch 911 Batch  202/718   train_loss = 2.343\n",
      "Epoch 911 Batch  302/718   train_loss = 2.455\n",
      "Epoch 911 Batch  402/718   train_loss = 2.418\n",
      "Epoch 911 Batch  502/718   train_loss = 2.334\n",
      "Epoch 911 Batch  602/718   train_loss = 2.294\n",
      "Epoch 911 Batch  702/718   train_loss = 2.265\n",
      "Epoch 912 Batch   84/718   train_loss = 2.226\n",
      "Epoch 912 Batch  184/718   train_loss = 2.331\n",
      "Epoch 912 Batch  284/718   train_loss = 2.311\n",
      "Epoch 912 Batch  384/718   train_loss = 2.351\n",
      "Epoch 912 Batch  484/718   train_loss = 2.336\n",
      "Epoch 912 Batch  584/718   train_loss = 2.265\n",
      "Epoch 912 Batch  684/718   train_loss = 2.353\n",
      "Epoch 913 Batch   66/718   train_loss = 2.255\n",
      "Epoch 913 Batch  166/718   train_loss = 2.314\n",
      "Epoch 913 Batch  266/718   train_loss = 2.412\n",
      "Epoch 913 Batch  366/718   train_loss = 2.484\n",
      "Epoch 913 Batch  466/718   train_loss = 2.371\n",
      "Epoch 913 Batch  566/718   train_loss = 2.284\n",
      "Epoch 913 Batch  666/718   train_loss = 2.285\n",
      "Epoch 914 Batch   48/718   train_loss = 2.223\n",
      "Epoch 914 Batch  148/718   train_loss = 2.368\n",
      "Epoch 914 Batch  248/718   train_loss = 2.326\n",
      "Epoch 914 Batch  348/718   train_loss = 2.269\n",
      "Epoch 914 Batch  448/718   train_loss = 2.346\n",
      "Epoch 914 Batch  548/718   train_loss = 2.388\n",
      "Epoch 914 Batch  648/718   train_loss = 2.358\n",
      "Epoch 915 Batch   30/718   train_loss = 2.366\n",
      "Epoch 915 Batch  130/718   train_loss = 2.273\n",
      "Epoch 915 Batch  230/718   train_loss = 2.404\n",
      "Epoch 915 Batch  330/718   train_loss = 2.407\n",
      "Epoch 915 Batch  430/718   train_loss = 2.447\n",
      "Epoch 915 Batch  530/718   train_loss = 2.329\n",
      "Epoch 915 Batch  630/718   train_loss = 2.318\n",
      "Epoch 916 Batch   12/718   train_loss = 2.214\n",
      "Epoch 916 Batch  112/718   train_loss = 2.336\n",
      "Epoch 916 Batch  212/718   train_loss = 2.451\n",
      "Epoch 916 Batch  312/718   train_loss = 2.379\n",
      "Epoch 916 Batch  412/718   train_loss = 2.276\n",
      "Epoch 916 Batch  512/718   train_loss = 2.345\n",
      "Epoch 916 Batch  612/718   train_loss = 2.389\n",
      "Epoch 916 Batch  712/718   train_loss = 2.400\n",
      "Epoch 917 Batch   94/718   train_loss = 2.207\n",
      "Epoch 917 Batch  194/718   train_loss = 2.267\n",
      "Epoch 917 Batch  294/718   train_loss = 2.427\n",
      "Epoch 917 Batch  394/718   train_loss = 2.327\n",
      "Epoch 917 Batch  494/718   train_loss = 2.491\n",
      "Epoch 917 Batch  594/718   train_loss = 2.468\n",
      "Epoch 917 Batch  694/718   train_loss = 2.457\n",
      "Epoch 918 Batch   76/718   train_loss = 2.256\n",
      "Epoch 918 Batch  176/718   train_loss = 2.407\n",
      "Epoch 918 Batch  276/718   train_loss = 2.360\n",
      "Epoch 918 Batch  376/718   train_loss = 2.333\n",
      "Epoch 918 Batch  476/718   train_loss = 2.400\n",
      "Epoch 918 Batch  576/718   train_loss = 2.420\n",
      "Epoch 918 Batch  676/718   train_loss = 2.330\n",
      "Epoch 919 Batch   58/718   train_loss = 2.366\n",
      "Epoch 919 Batch  158/718   train_loss = 2.390\n",
      "Epoch 919 Batch  258/718   train_loss = 2.352\n",
      "Epoch 919 Batch  358/718   train_loss = 2.266\n",
      "Epoch 919 Batch  458/718   train_loss = 2.426\n",
      "Epoch 919 Batch  558/718   train_loss = 2.476\n",
      "Epoch 919 Batch  658/718   train_loss = 2.230\n",
      "Epoch 920 Batch   40/718   train_loss = 2.410\n",
      "Epoch 920 Batch  140/718   train_loss = 2.297\n",
      "Epoch 920 Batch  240/718   train_loss = 2.233\n",
      "Epoch 920 Batch  340/718   train_loss = 2.365\n",
      "Epoch 920 Batch  440/718   train_loss = 2.321\n",
      "Epoch 920 Batch  540/718   train_loss = 2.382\n",
      "Epoch 920 Batch  640/718   train_loss = 2.322\n",
      "Epoch 921 Batch   22/718   train_loss = 2.177\n",
      "Epoch 921 Batch  122/718   train_loss = 2.352\n",
      "Epoch 921 Batch  222/718   train_loss = 2.285\n",
      "Epoch 921 Batch  322/718   train_loss = 2.364\n",
      "Epoch 921 Batch  422/718   train_loss = 2.365\n",
      "Epoch 921 Batch  522/718   train_loss = 2.298\n",
      "Epoch 921 Batch  622/718   train_loss = 2.361\n",
      "Epoch 922 Batch    4/718   train_loss = 2.227\n",
      "Epoch 922 Batch  104/718   train_loss = 2.379\n",
      "Epoch 922 Batch  204/718   train_loss = 2.213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 922 Batch  304/718   train_loss = 2.325\n",
      "Epoch 922 Batch  404/718   train_loss = 2.295\n",
      "Epoch 922 Batch  504/718   train_loss = 2.505\n",
      "Epoch 922 Batch  604/718   train_loss = 2.226\n",
      "Epoch 922 Batch  704/718   train_loss = 2.369\n",
      "Epoch 923 Batch   86/718   train_loss = 2.337\n",
      "Epoch 923 Batch  186/718   train_loss = 2.310\n",
      "Epoch 923 Batch  286/718   train_loss = 2.440\n",
      "Epoch 923 Batch  386/718   train_loss = 2.354\n",
      "Epoch 923 Batch  486/718   train_loss = 2.328\n",
      "Epoch 923 Batch  586/718   train_loss = 2.331\n",
      "Epoch 923 Batch  686/718   train_loss = 2.293\n",
      "Epoch 924 Batch   68/718   train_loss = 2.379\n",
      "Epoch 924 Batch  168/718   train_loss = 2.383\n",
      "Epoch 924 Batch  268/718   train_loss = 2.342\n",
      "Epoch 924 Batch  368/718   train_loss = 2.330\n",
      "Epoch 924 Batch  468/718   train_loss = 2.540\n",
      "Epoch 924 Batch  568/718   train_loss = 2.477\n",
      "Epoch 924 Batch  668/718   train_loss = 2.348\n",
      "Epoch 925 Batch   50/718   train_loss = 2.414\n",
      "Epoch 925 Batch  150/718   train_loss = 2.283\n",
      "Epoch 925 Batch  250/718   train_loss = 2.262\n",
      "Epoch 925 Batch  350/718   train_loss = 2.283\n",
      "Epoch 925 Batch  450/718   train_loss = 2.344\n",
      "Epoch 925 Batch  550/718   train_loss = 2.292\n",
      "Epoch 925 Batch  650/718   train_loss = 2.274\n",
      "Epoch 926 Batch   32/718   train_loss = 2.282\n",
      "Epoch 926 Batch  132/718   train_loss = 2.373\n",
      "Epoch 926 Batch  232/718   train_loss = 2.317\n",
      "Epoch 926 Batch  332/718   train_loss = 2.252\n",
      "Epoch 926 Batch  432/718   train_loss = 2.440\n",
      "Epoch 926 Batch  532/718   train_loss = 2.410\n",
      "Epoch 926 Batch  632/718   train_loss = 2.228\n",
      "Epoch 927 Batch   14/718   train_loss = 2.363\n",
      "Epoch 927 Batch  114/718   train_loss = 2.314\n",
      "Epoch 927 Batch  214/718   train_loss = 2.366\n",
      "Epoch 927 Batch  314/718   train_loss = 2.363\n",
      "Epoch 927 Batch  414/718   train_loss = 2.200\n",
      "Epoch 927 Batch  514/718   train_loss = 2.461\n",
      "Epoch 927 Batch  614/718   train_loss = 2.324\n",
      "Epoch 927 Batch  714/718   train_loss = 2.312\n",
      "Epoch 928 Batch   96/718   train_loss = 2.331\n",
      "Epoch 928 Batch  196/718   train_loss = 2.313\n",
      "Epoch 928 Batch  296/718   train_loss = 2.287\n",
      "Epoch 928 Batch  396/718   train_loss = 2.399\n",
      "Epoch 928 Batch  496/718   train_loss = 2.440\n",
      "Epoch 928 Batch  596/718   train_loss = 2.326\n",
      "Epoch 928 Batch  696/718   train_loss = 2.361\n",
      "Epoch 929 Batch   78/718   train_loss = 2.392\n",
      "Epoch 929 Batch  178/718   train_loss = 2.349\n",
      "Epoch 929 Batch  278/718   train_loss = 2.330\n",
      "Epoch 929 Batch  378/718   train_loss = 2.282\n",
      "Epoch 929 Batch  478/718   train_loss = 2.247\n",
      "Epoch 929 Batch  578/718   train_loss = 2.277\n",
      "Epoch 929 Batch  678/718   train_loss = 2.345\n",
      "Epoch 930 Batch   60/718   train_loss = 2.359\n",
      "Epoch 930 Batch  160/718   train_loss = 2.379\n",
      "Epoch 930 Batch  260/718   train_loss = 2.370\n",
      "Epoch 930 Batch  360/718   train_loss = 2.330\n",
      "Epoch 930 Batch  460/718   train_loss = 2.250\n",
      "Epoch 930 Batch  560/718   train_loss = 2.340\n",
      "Epoch 930 Batch  660/718   train_loss = 2.473\n",
      "Epoch 931 Batch   42/718   train_loss = 2.394\n",
      "Epoch 931 Batch  142/718   train_loss = 2.361\n",
      "Epoch 931 Batch  242/718   train_loss = 2.205\n",
      "Epoch 931 Batch  342/718   train_loss = 2.351\n",
      "Epoch 931 Batch  442/718   train_loss = 2.313\n",
      "Epoch 931 Batch  542/718   train_loss = 2.241\n",
      "Epoch 931 Batch  642/718   train_loss = 2.423\n",
      "Epoch 932 Batch   24/718   train_loss = 2.330\n",
      "Epoch 932 Batch  124/718   train_loss = 2.210\n",
      "Epoch 932 Batch  224/718   train_loss = 2.361\n",
      "Epoch 932 Batch  324/718   train_loss = 2.326\n",
      "Epoch 932 Batch  424/718   train_loss = 2.321\n",
      "Epoch 932 Batch  524/718   train_loss = 2.424\n",
      "Epoch 932 Batch  624/718   train_loss = 2.356\n",
      "Epoch 933 Batch    6/718   train_loss = 2.274\n",
      "Epoch 933 Batch  106/718   train_loss = 2.318\n",
      "Epoch 933 Batch  206/718   train_loss = 2.229\n",
      "Epoch 933 Batch  306/718   train_loss = 2.298\n",
      "Epoch 933 Batch  406/718   train_loss = 2.358\n",
      "Epoch 933 Batch  506/718   train_loss = 2.236\n",
      "Epoch 933 Batch  606/718   train_loss = 2.279\n",
      "Epoch 933 Batch  706/718   train_loss = 2.355\n",
      "Epoch 934 Batch   88/718   train_loss = 2.429\n",
      "Epoch 934 Batch  188/718   train_loss = 2.253\n",
      "Epoch 934 Batch  288/718   train_loss = 2.333\n",
      "Epoch 934 Batch  388/718   train_loss = 2.237\n",
      "Epoch 934 Batch  488/718   train_loss = 2.252\n",
      "Epoch 934 Batch  588/718   train_loss = 2.393\n",
      "Epoch 934 Batch  688/718   train_loss = 2.285\n",
      "Epoch 935 Batch   70/718   train_loss = 2.378\n",
      "Epoch 935 Batch  170/718   train_loss = 2.350\n",
      "Epoch 935 Batch  270/718   train_loss = 2.400\n",
      "Epoch 935 Batch  370/718   train_loss = 2.327\n",
      "Epoch 935 Batch  470/718   train_loss = 2.508\n",
      "Epoch 935 Batch  570/718   train_loss = 2.394\n",
      "Epoch 935 Batch  670/718   train_loss = 2.310\n",
      "Epoch 936 Batch   52/718   train_loss = 2.267\n",
      "Epoch 936 Batch  152/718   train_loss = 2.297\n",
      "Epoch 936 Batch  252/718   train_loss = 2.351\n",
      "Epoch 936 Batch  352/718   train_loss = 2.298\n",
      "Epoch 936 Batch  452/718   train_loss = 2.368\n",
      "Epoch 936 Batch  552/718   train_loss = 2.418\n",
      "Epoch 936 Batch  652/718   train_loss = 2.278\n",
      "Epoch 937 Batch   34/718   train_loss = 2.290\n",
      "Epoch 937 Batch  134/718   train_loss = 2.276\n",
      "Epoch 937 Batch  234/718   train_loss = 2.416\n",
      "Epoch 937 Batch  334/718   train_loss = 2.390\n",
      "Epoch 937 Batch  434/718   train_loss = 2.406\n",
      "Epoch 937 Batch  534/718   train_loss = 2.284\n",
      "Epoch 937 Batch  634/718   train_loss = 2.502\n",
      "Epoch 938 Batch   16/718   train_loss = 2.299\n",
      "Epoch 938 Batch  116/718   train_loss = 2.256\n",
      "Epoch 938 Batch  216/718   train_loss = 2.317\n",
      "Epoch 938 Batch  316/718   train_loss = 2.225\n",
      "Epoch 938 Batch  416/718   train_loss = 2.330\n",
      "Epoch 938 Batch  516/718   train_loss = 2.299\n",
      "Epoch 938 Batch  616/718   train_loss = 2.403\n",
      "Epoch 938 Batch  716/718   train_loss = 2.375\n",
      "Epoch 939 Batch   98/718   train_loss = 2.305\n",
      "Epoch 939 Batch  198/718   train_loss = 2.347\n",
      "Epoch 939 Batch  298/718   train_loss = 2.280\n",
      "Epoch 939 Batch  398/718   train_loss = 2.310\n",
      "Epoch 939 Batch  498/718   train_loss = 2.369\n",
      "Epoch 939 Batch  598/718   train_loss = 2.393\n",
      "Epoch 939 Batch  698/718   train_loss = 2.375\n",
      "Epoch 940 Batch   80/718   train_loss = 2.327\n",
      "Epoch 940 Batch  180/718   train_loss = 2.297\n",
      "Epoch 940 Batch  280/718   train_loss = 2.412\n",
      "Epoch 940 Batch  380/718   train_loss = 2.371\n",
      "Epoch 940 Batch  480/718   train_loss = 2.439\n",
      "Epoch 940 Batch  580/718   train_loss = 2.292\n",
      "Epoch 940 Batch  680/718   train_loss = 2.350\n",
      "Epoch 941 Batch   62/718   train_loss = 2.310\n",
      "Epoch 941 Batch  162/718   train_loss = 2.350\n",
      "Epoch 941 Batch  262/718   train_loss = 2.455\n",
      "Epoch 941 Batch  362/718   train_loss = 2.406\n",
      "Epoch 941 Batch  462/718   train_loss = 2.358\n",
      "Epoch 941 Batch  562/718   train_loss = 2.310\n",
      "Epoch 941 Batch  662/718   train_loss = 2.320\n",
      "Epoch 942 Batch   44/718   train_loss = 2.334\n",
      "Epoch 942 Batch  144/718   train_loss = 2.440\n",
      "Epoch 942 Batch  244/718   train_loss = 2.446\n",
      "Epoch 942 Batch  344/718   train_loss = 2.336\n",
      "Epoch 942 Batch  444/718   train_loss = 2.269\n",
      "Epoch 942 Batch  544/718   train_loss = 2.326\n",
      "Epoch 942 Batch  644/718   train_loss = 2.471\n",
      "Epoch 943 Batch   26/718   train_loss = 2.418\n",
      "Epoch 943 Batch  126/718   train_loss = 2.311\n",
      "Epoch 943 Batch  226/718   train_loss = 2.319\n",
      "Epoch 943 Batch  326/718   train_loss = 2.253\n",
      "Epoch 943 Batch  426/718   train_loss = 2.275\n",
      "Epoch 943 Batch  526/718   train_loss = 2.323\n",
      "Epoch 943 Batch  626/718   train_loss = 2.450\n",
      "Epoch 944 Batch    8/718   train_loss = 2.189\n",
      "Epoch 944 Batch  108/718   train_loss = 2.251\n",
      "Epoch 944 Batch  208/718   train_loss = 2.375\n",
      "Epoch 944 Batch  308/718   train_loss = 2.405\n",
      "Epoch 944 Batch  408/718   train_loss = 2.426\n",
      "Epoch 944 Batch  508/718   train_loss = 2.440\n",
      "Epoch 944 Batch  608/718   train_loss = 2.501\n",
      "Epoch 944 Batch  708/718   train_loss = 2.281\n",
      "Epoch 945 Batch   90/718   train_loss = 2.273\n",
      "Epoch 945 Batch  190/718   train_loss = 2.435\n",
      "Epoch 945 Batch  290/718   train_loss = 2.246\n",
      "Epoch 945 Batch  390/718   train_loss = 2.303\n",
      "Epoch 945 Batch  490/718   train_loss = 2.344\n",
      "Epoch 945 Batch  590/718   train_loss = 2.357\n",
      "Epoch 945 Batch  690/718   train_loss = 2.427\n",
      "Epoch 946 Batch   72/718   train_loss = 2.393\n",
      "Epoch 946 Batch  172/718   train_loss = 2.373\n",
      "Epoch 946 Batch  272/718   train_loss = 2.361\n",
      "Epoch 946 Batch  372/718   train_loss = 2.344\n",
      "Epoch 946 Batch  472/718   train_loss = 2.393\n",
      "Epoch 946 Batch  572/718   train_loss = 2.349\n",
      "Epoch 946 Batch  672/718   train_loss = 2.262\n",
      "Epoch 947 Batch   54/718   train_loss = 2.228\n",
      "Epoch 947 Batch  154/718   train_loss = 2.407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 947 Batch  254/718   train_loss = 2.313\n",
      "Epoch 947 Batch  354/718   train_loss = 2.339\n",
      "Epoch 947 Batch  454/718   train_loss = 2.266\n",
      "Epoch 947 Batch  554/718   train_loss = 2.442\n",
      "Epoch 947 Batch  654/718   train_loss = 2.389\n",
      "Epoch 948 Batch   36/718   train_loss = 2.338\n",
      "Epoch 948 Batch  136/718   train_loss = 2.249\n",
      "Epoch 948 Batch  236/718   train_loss = 2.254\n",
      "Epoch 948 Batch  336/718   train_loss = 2.389\n",
      "Epoch 948 Batch  436/718   train_loss = 2.340\n",
      "Epoch 948 Batch  536/718   train_loss = 2.433\n",
      "Epoch 948 Batch  636/718   train_loss = 2.407\n",
      "Epoch 949 Batch   18/718   train_loss = 2.323\n",
      "Epoch 949 Batch  118/718   train_loss = 2.283\n",
      "Epoch 949 Batch  218/718   train_loss = 2.381\n",
      "Epoch 949 Batch  318/718   train_loss = 2.372\n",
      "Epoch 949 Batch  418/718   train_loss = 2.489\n",
      "Epoch 949 Batch  518/718   train_loss = 2.373\n",
      "Epoch 949 Batch  618/718   train_loss = 2.350\n",
      "Epoch 950 Batch    0/718   train_loss = 2.409\n",
      "Epoch 950 Batch  100/718   train_loss = 2.349\n",
      "Epoch 950 Batch  200/718   train_loss = 2.240\n",
      "Epoch 950 Batch  300/718   train_loss = 2.319\n",
      "Epoch 950 Batch  400/718   train_loss = 2.435\n",
      "Epoch 950 Batch  500/718   train_loss = 2.466\n",
      "Epoch 950 Batch  600/718   train_loss = 2.448\n",
      "Epoch 950 Batch  700/718   train_loss = 2.316\n",
      "Epoch 951 Batch   82/718   train_loss = 2.266\n",
      "Epoch 951 Batch  182/718   train_loss = 2.411\n",
      "Epoch 951 Batch  282/718   train_loss = 2.383\n",
      "Epoch 951 Batch  382/718   train_loss = 2.266\n",
      "Epoch 951 Batch  482/718   train_loss = 2.469\n",
      "Epoch 951 Batch  582/718   train_loss = 2.262\n",
      "Epoch 951 Batch  682/718   train_loss = 2.391\n",
      "Epoch 952 Batch   64/718   train_loss = 2.341\n",
      "Epoch 952 Batch  164/718   train_loss = 2.215\n",
      "Epoch 952 Batch  264/718   train_loss = 2.358\n",
      "Epoch 952 Batch  364/718   train_loss = 2.441\n",
      "Epoch 952 Batch  464/718   train_loss = 2.273\n",
      "Epoch 952 Batch  564/718   train_loss = 2.388\n",
      "Epoch 952 Batch  664/718   train_loss = 2.460\n",
      "Epoch 953 Batch   46/718   train_loss = 2.266\n",
      "Epoch 953 Batch  146/718   train_loss = 2.207\n",
      "Epoch 953 Batch  246/718   train_loss = 2.417\n",
      "Epoch 953 Batch  346/718   train_loss = 2.259\n",
      "Epoch 953 Batch  446/718   train_loss = 2.281\n",
      "Epoch 953 Batch  546/718   train_loss = 2.247\n",
      "Epoch 953 Batch  646/718   train_loss = 2.407\n",
      "Epoch 954 Batch   28/718   train_loss = 2.270\n",
      "Epoch 954 Batch  128/718   train_loss = 2.352\n",
      "Epoch 954 Batch  228/718   train_loss = 2.372\n",
      "Epoch 954 Batch  328/718   train_loss = 2.277\n",
      "Epoch 954 Batch  428/718   train_loss = 2.260\n",
      "Epoch 954 Batch  528/718   train_loss = 2.400\n",
      "Epoch 954 Batch  628/718   train_loss = 2.461\n",
      "Epoch 955 Batch   10/718   train_loss = 2.283\n",
      "Epoch 955 Batch  110/718   train_loss = 2.329\n",
      "Epoch 955 Batch  210/718   train_loss = 2.308\n",
      "Epoch 955 Batch  310/718   train_loss = 2.436\n",
      "Epoch 955 Batch  410/718   train_loss = 2.255\n",
      "Epoch 955 Batch  510/718   train_loss = 2.393\n",
      "Epoch 955 Batch  610/718   train_loss = 2.379\n",
      "Epoch 955 Batch  710/718   train_loss = 2.258\n",
      "Epoch 956 Batch   92/718   train_loss = 2.282\n",
      "Epoch 956 Batch  192/718   train_loss = 2.294\n",
      "Epoch 956 Batch  292/718   train_loss = 2.237\n",
      "Epoch 956 Batch  392/718   train_loss = 2.287\n",
      "Epoch 956 Batch  492/718   train_loss = 2.364\n",
      "Epoch 956 Batch  592/718   train_loss = 2.429\n",
      "Epoch 956 Batch  692/718   train_loss = 2.325\n",
      "Epoch 957 Batch   74/718   train_loss = 2.446\n",
      "Epoch 957 Batch  174/718   train_loss = 2.410\n",
      "Epoch 957 Batch  274/718   train_loss = 2.361\n",
      "Epoch 957 Batch  374/718   train_loss = 2.243\n",
      "Epoch 957 Batch  474/718   train_loss = 2.378\n",
      "Epoch 957 Batch  574/718   train_loss = 2.241\n",
      "Epoch 957 Batch  674/718   train_loss = 2.288\n",
      "Epoch 958 Batch   56/718   train_loss = 2.401\n",
      "Epoch 958 Batch  156/718   train_loss = 2.327\n",
      "Epoch 958 Batch  256/718   train_loss = 2.266\n",
      "Epoch 958 Batch  356/718   train_loss = 2.264\n",
      "Epoch 958 Batch  456/718   train_loss = 2.307\n",
      "Epoch 958 Batch  556/718   train_loss = 2.292\n",
      "Epoch 958 Batch  656/718   train_loss = 2.284\n",
      "Epoch 959 Batch   38/718   train_loss = 2.405\n",
      "Epoch 959 Batch  138/718   train_loss = 2.377\n",
      "Epoch 959 Batch  238/718   train_loss = 2.363\n",
      "Epoch 959 Batch  338/718   train_loss = 2.352\n",
      "Epoch 959 Batch  438/718   train_loss = 2.242\n",
      "Epoch 959 Batch  538/718   train_loss = 2.282\n",
      "Epoch 959 Batch  638/718   train_loss = 2.295\n",
      "Epoch 960 Batch   20/718   train_loss = 2.344\n",
      "Epoch 960 Batch  120/718   train_loss = 2.324\n",
      "Epoch 960 Batch  220/718   train_loss = 2.295\n",
      "Epoch 960 Batch  320/718   train_loss = 2.381\n",
      "Epoch 960 Batch  420/718   train_loss = 2.491\n",
      "Epoch 960 Batch  520/718   train_loss = 2.376\n",
      "Epoch 960 Batch  620/718   train_loss = 2.516\n",
      "Epoch 961 Batch    2/718   train_loss = 2.402\n",
      "Epoch 961 Batch  102/718   train_loss = 2.241\n",
      "Epoch 961 Batch  202/718   train_loss = 2.336\n",
      "Epoch 961 Batch  302/718   train_loss = 2.437\n",
      "Epoch 961 Batch  402/718   train_loss = 2.382\n",
      "Epoch 961 Batch  502/718   train_loss = 2.339\n",
      "Epoch 961 Batch  602/718   train_loss = 2.299\n",
      "Epoch 961 Batch  702/718   train_loss = 2.254\n",
      "Epoch 962 Batch   84/718   train_loss = 2.274\n",
      "Epoch 962 Batch  184/718   train_loss = 2.308\n",
      "Epoch 962 Batch  284/718   train_loss = 2.335\n",
      "Epoch 962 Batch  384/718   train_loss = 2.322\n",
      "Epoch 962 Batch  484/718   train_loss = 2.306\n",
      "Epoch 962 Batch  584/718   train_loss = 2.264\n",
      "Epoch 962 Batch  684/718   train_loss = 2.323\n",
      "Epoch 963 Batch   66/718   train_loss = 2.263\n",
      "Epoch 963 Batch  166/718   train_loss = 2.329\n",
      "Epoch 963 Batch  266/718   train_loss = 2.377\n",
      "Epoch 963 Batch  366/718   train_loss = 2.418\n",
      "Epoch 963 Batch  466/718   train_loss = 2.323\n",
      "Epoch 963 Batch  566/718   train_loss = 2.313\n",
      "Epoch 963 Batch  666/718   train_loss = 2.305\n",
      "Epoch 964 Batch   48/718   train_loss = 2.239\n",
      "Epoch 964 Batch  148/718   train_loss = 2.324\n",
      "Epoch 964 Batch  248/718   train_loss = 2.380\n",
      "Epoch 964 Batch  348/718   train_loss = 2.276\n",
      "Epoch 964 Batch  448/718   train_loss = 2.341\n",
      "Epoch 964 Batch  548/718   train_loss = 2.430\n",
      "Epoch 964 Batch  648/718   train_loss = 2.328\n",
      "Epoch 965 Batch   30/718   train_loss = 2.295\n",
      "Epoch 965 Batch  130/718   train_loss = 2.292\n",
      "Epoch 965 Batch  230/718   train_loss = 2.392\n",
      "Epoch 965 Batch  330/718   train_loss = 2.397\n",
      "Epoch 965 Batch  430/718   train_loss = 2.468\n",
      "Epoch 965 Batch  530/718   train_loss = 2.298\n",
      "Epoch 965 Batch  630/718   train_loss = 2.273\n",
      "Epoch 966 Batch   12/718   train_loss = 2.207\n",
      "Epoch 966 Batch  112/718   train_loss = 2.363\n",
      "Epoch 966 Batch  212/718   train_loss = 2.430\n",
      "Epoch 966 Batch  312/718   train_loss = 2.317\n",
      "Epoch 966 Batch  412/718   train_loss = 2.243\n",
      "Epoch 966 Batch  512/718   train_loss = 2.316\n",
      "Epoch 966 Batch  612/718   train_loss = 2.395\n",
      "Epoch 966 Batch  712/718   train_loss = 2.415\n",
      "Epoch 967 Batch   94/718   train_loss = 2.189\n",
      "Epoch 967 Batch  194/718   train_loss = 2.270\n",
      "Epoch 967 Batch  294/718   train_loss = 2.424\n",
      "Epoch 967 Batch  394/718   train_loss = 2.325\n",
      "Epoch 967 Batch  494/718   train_loss = 2.444\n",
      "Epoch 967 Batch  594/718   train_loss = 2.429\n",
      "Epoch 967 Batch  694/718   train_loss = 2.454\n",
      "Epoch 968 Batch   76/718   train_loss = 2.265\n",
      "Epoch 968 Batch  176/718   train_loss = 2.378\n",
      "Epoch 968 Batch  276/718   train_loss = 2.346\n",
      "Epoch 968 Batch  376/718   train_loss = 2.396\n",
      "Epoch 968 Batch  476/718   train_loss = 2.392\n",
      "Epoch 968 Batch  576/718   train_loss = 2.395\n",
      "Epoch 968 Batch  676/718   train_loss = 2.338\n",
      "Epoch 969 Batch   58/718   train_loss = 2.384\n",
      "Epoch 969 Batch  158/718   train_loss = 2.331\n",
      "Epoch 969 Batch  258/718   train_loss = 2.336\n",
      "Epoch 969 Batch  358/718   train_loss = 2.223\n",
      "Epoch 969 Batch  458/718   train_loss = 2.407\n",
      "Epoch 969 Batch  558/718   train_loss = 2.409\n",
      "Epoch 969 Batch  658/718   train_loss = 2.244\n",
      "Epoch 970 Batch   40/718   train_loss = 2.352\n",
      "Epoch 970 Batch  140/718   train_loss = 2.303\n",
      "Epoch 970 Batch  240/718   train_loss = 2.273\n",
      "Epoch 970 Batch  340/718   train_loss = 2.354\n",
      "Epoch 970 Batch  440/718   train_loss = 2.307\n",
      "Epoch 970 Batch  540/718   train_loss = 2.390\n",
      "Epoch 970 Batch  640/718   train_loss = 2.333\n",
      "Epoch 971 Batch   22/718   train_loss = 2.193\n",
      "Epoch 971 Batch  122/718   train_loss = 2.295\n",
      "Epoch 971 Batch  222/718   train_loss = 2.319\n",
      "Epoch 971 Batch  322/718   train_loss = 2.343\n",
      "Epoch 971 Batch  422/718   train_loss = 2.439\n",
      "Epoch 971 Batch  522/718   train_loss = 2.332\n",
      "Epoch 971 Batch  622/718   train_loss = 2.382\n",
      "Epoch 972 Batch    4/718   train_loss = 2.242\n",
      "Epoch 972 Batch  104/718   train_loss = 2.346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 972 Batch  204/718   train_loss = 2.191\n",
      "Epoch 972 Batch  304/718   train_loss = 2.285\n",
      "Epoch 972 Batch  404/718   train_loss = 2.311\n",
      "Epoch 972 Batch  504/718   train_loss = 2.452\n",
      "Epoch 972 Batch  604/718   train_loss = 2.256\n",
      "Epoch 972 Batch  704/718   train_loss = 2.320\n",
      "Epoch 973 Batch   86/718   train_loss = 2.339\n",
      "Epoch 973 Batch  186/718   train_loss = 2.330\n",
      "Epoch 973 Batch  286/718   train_loss = 2.416\n",
      "Epoch 973 Batch  386/718   train_loss = 2.360\n",
      "Epoch 973 Batch  486/718   train_loss = 2.326\n",
      "Epoch 973 Batch  586/718   train_loss = 2.303\n",
      "Epoch 973 Batch  686/718   train_loss = 2.336\n",
      "Epoch 974 Batch   68/718   train_loss = 2.363\n",
      "Epoch 974 Batch  168/718   train_loss = 2.433\n",
      "Epoch 974 Batch  268/718   train_loss = 2.275\n",
      "Epoch 974 Batch  368/718   train_loss = 2.293\n",
      "Epoch 974 Batch  468/718   train_loss = 2.550\n",
      "Epoch 974 Batch  568/718   train_loss = 2.458\n",
      "Epoch 974 Batch  668/718   train_loss = 2.333\n",
      "Epoch 975 Batch   50/718   train_loss = 2.421\n",
      "Epoch 975 Batch  150/718   train_loss = 2.331\n",
      "Epoch 975 Batch  250/718   train_loss = 2.296\n",
      "Epoch 975 Batch  350/718   train_loss = 2.343\n",
      "Epoch 975 Batch  450/718   train_loss = 2.329\n",
      "Epoch 975 Batch  550/718   train_loss = 2.282\n",
      "Epoch 975 Batch  650/718   train_loss = 2.279\n",
      "Epoch 976 Batch   32/718   train_loss = 2.272\n",
      "Epoch 976 Batch  132/718   train_loss = 2.360\n",
      "Epoch 976 Batch  232/718   train_loss = 2.269\n",
      "Epoch 976 Batch  332/718   train_loss = 2.230\n",
      "Epoch 976 Batch  432/718   train_loss = 2.434\n",
      "Epoch 976 Batch  532/718   train_loss = 2.405\n",
      "Epoch 976 Batch  632/718   train_loss = 2.232\n",
      "Epoch 977 Batch   14/718   train_loss = 2.355\n",
      "Epoch 977 Batch  114/718   train_loss = 2.292\n",
      "Epoch 977 Batch  214/718   train_loss = 2.388\n",
      "Epoch 977 Batch  314/718   train_loss = 2.368\n",
      "Epoch 977 Batch  414/718   train_loss = 2.218\n",
      "Epoch 977 Batch  514/718   train_loss = 2.471\n",
      "Epoch 977 Batch  614/718   train_loss = 2.301\n",
      "Epoch 977 Batch  714/718   train_loss = 2.320\n",
      "Epoch 978 Batch   96/718   train_loss = 2.358\n",
      "Epoch 978 Batch  196/718   train_loss = 2.304\n",
      "Epoch 978 Batch  296/718   train_loss = 2.306\n",
      "Epoch 978 Batch  396/718   train_loss = 2.376\n",
      "Epoch 978 Batch  496/718   train_loss = 2.463\n",
      "Epoch 978 Batch  596/718   train_loss = 2.298\n",
      "Epoch 978 Batch  696/718   train_loss = 2.353\n",
      "Epoch 979 Batch   78/718   train_loss = 2.359\n",
      "Epoch 979 Batch  178/718   train_loss = 2.384\n",
      "Epoch 979 Batch  278/718   train_loss = 2.292\n",
      "Epoch 979 Batch  378/718   train_loss = 2.290\n",
      "Epoch 979 Batch  478/718   train_loss = 2.273\n",
      "Epoch 979 Batch  578/718   train_loss = 2.263\n",
      "Epoch 979 Batch  678/718   train_loss = 2.349\n",
      "Epoch 980 Batch   60/718   train_loss = 2.356\n",
      "Epoch 980 Batch  160/718   train_loss = 2.377\n",
      "Epoch 980 Batch  260/718   train_loss = 2.339\n",
      "Epoch 980 Batch  360/718   train_loss = 2.326\n",
      "Epoch 980 Batch  460/718   train_loss = 2.285\n",
      "Epoch 980 Batch  560/718   train_loss = 2.339\n",
      "Epoch 980 Batch  660/718   train_loss = 2.430\n",
      "Epoch 981 Batch   42/718   train_loss = 2.393\n",
      "Epoch 981 Batch  142/718   train_loss = 2.375\n",
      "Epoch 981 Batch  242/718   train_loss = 2.186\n",
      "Epoch 981 Batch  342/718   train_loss = 2.374\n",
      "Epoch 981 Batch  442/718   train_loss = 2.297\n",
      "Epoch 981 Batch  542/718   train_loss = 2.247\n",
      "Epoch 981 Batch  642/718   train_loss = 2.420\n",
      "Epoch 982 Batch   24/718   train_loss = 2.327\n",
      "Epoch 982 Batch  124/718   train_loss = 2.189\n",
      "Epoch 982 Batch  224/718   train_loss = 2.313\n",
      "Epoch 982 Batch  324/718   train_loss = 2.321\n",
      "Epoch 982 Batch  424/718   train_loss = 2.277\n",
      "Epoch 982 Batch  524/718   train_loss = 2.333\n",
      "Epoch 982 Batch  624/718   train_loss = 2.341\n",
      "Epoch 983 Batch    6/718   train_loss = 2.238\n",
      "Epoch 983 Batch  106/718   train_loss = 2.330\n",
      "Epoch 983 Batch  206/718   train_loss = 2.232\n",
      "Epoch 983 Batch  306/718   train_loss = 2.295\n",
      "Epoch 983 Batch  406/718   train_loss = 2.333\n",
      "Epoch 983 Batch  506/718   train_loss = 2.236\n",
      "Epoch 983 Batch  606/718   train_loss = 2.269\n",
      "Epoch 983 Batch  706/718   train_loss = 2.326\n",
      "Epoch 984 Batch   88/718   train_loss = 2.415\n",
      "Epoch 984 Batch  188/718   train_loss = 2.282\n",
      "Epoch 984 Batch  288/718   train_loss = 2.333\n",
      "Epoch 984 Batch  388/718   train_loss = 2.209\n",
      "Epoch 984 Batch  488/718   train_loss = 2.247\n",
      "Epoch 984 Batch  588/718   train_loss = 2.425\n",
      "Epoch 984 Batch  688/718   train_loss = 2.313\n",
      "Epoch 985 Batch   70/718   train_loss = 2.365\n",
      "Epoch 985 Batch  170/718   train_loss = 2.352\n",
      "Epoch 985 Batch  270/718   train_loss = 2.406\n",
      "Epoch 985 Batch  370/718   train_loss = 2.335\n",
      "Epoch 985 Batch  470/718   train_loss = 2.472\n",
      "Epoch 985 Batch  570/718   train_loss = 2.358\n",
      "Epoch 985 Batch  670/718   train_loss = 2.280\n",
      "Epoch 986 Batch   52/718   train_loss = 2.279\n",
      "Epoch 986 Batch  152/718   train_loss = 2.294\n",
      "Epoch 986 Batch  252/718   train_loss = 2.374\n",
      "Epoch 986 Batch  352/718   train_loss = 2.278\n",
      "Epoch 986 Batch  452/718   train_loss = 2.332\n",
      "Epoch 986 Batch  552/718   train_loss = 2.422\n",
      "Epoch 986 Batch  652/718   train_loss = 2.257\n",
      "Epoch 987 Batch   34/718   train_loss = 2.299\n",
      "Epoch 987 Batch  134/718   train_loss = 2.282\n",
      "Epoch 987 Batch  234/718   train_loss = 2.425\n",
      "Epoch 987 Batch  334/718   train_loss = 2.364\n",
      "Epoch 987 Batch  434/718   train_loss = 2.394\n",
      "Epoch 987 Batch  534/718   train_loss = 2.342\n",
      "Epoch 987 Batch  634/718   train_loss = 2.441\n",
      "Epoch 988 Batch   16/718   train_loss = 2.303\n",
      "Epoch 988 Batch  116/718   train_loss = 2.258\n",
      "Epoch 988 Batch  216/718   train_loss = 2.283\n",
      "Epoch 988 Batch  316/718   train_loss = 2.234\n",
      "Epoch 988 Batch  416/718   train_loss = 2.317\n",
      "Epoch 988 Batch  516/718   train_loss = 2.323\n",
      "Epoch 988 Batch  616/718   train_loss = 2.432\n",
      "Epoch 988 Batch  716/718   train_loss = 2.389\n",
      "Epoch 989 Batch   98/718   train_loss = 2.277\n",
      "Epoch 989 Batch  198/718   train_loss = 2.343\n",
      "Epoch 989 Batch  298/718   train_loss = 2.298\n",
      "Epoch 989 Batch  398/718   train_loss = 2.294\n",
      "Epoch 989 Batch  498/718   train_loss = 2.353\n",
      "Epoch 989 Batch  598/718   train_loss = 2.370\n",
      "Epoch 989 Batch  698/718   train_loss = 2.399\n",
      "Epoch 990 Batch   80/718   train_loss = 2.295\n",
      "Epoch 990 Batch  180/718   train_loss = 2.248\n",
      "Epoch 990 Batch  280/718   train_loss = 2.439\n",
      "Epoch 990 Batch  380/718   train_loss = 2.354\n",
      "Epoch 990 Batch  480/718   train_loss = 2.427\n",
      "Epoch 990 Batch  580/718   train_loss = 2.314\n",
      "Epoch 990 Batch  680/718   train_loss = 2.339\n",
      "Epoch 991 Batch   62/718   train_loss = 2.309\n",
      "Epoch 991 Batch  162/718   train_loss = 2.304\n",
      "Epoch 991 Batch  262/718   train_loss = 2.422\n",
      "Epoch 991 Batch  362/718   train_loss = 2.371\n",
      "Epoch 991 Batch  462/718   train_loss = 2.362\n",
      "Epoch 991 Batch  562/718   train_loss = 2.278\n",
      "Epoch 991 Batch  662/718   train_loss = 2.301\n",
      "Epoch 992 Batch   44/718   train_loss = 2.316\n",
      "Epoch 992 Batch  144/718   train_loss = 2.408\n",
      "Epoch 992 Batch  244/718   train_loss = 2.417\n",
      "Epoch 992 Batch  344/718   train_loss = 2.311\n",
      "Epoch 992 Batch  444/718   train_loss = 2.284\n",
      "Epoch 992 Batch  544/718   train_loss = 2.290\n",
      "Epoch 992 Batch  644/718   train_loss = 2.434\n",
      "Epoch 993 Batch   26/718   train_loss = 2.417\n",
      "Epoch 993 Batch  126/718   train_loss = 2.293\n",
      "Epoch 993 Batch  226/718   train_loss = 2.275\n",
      "Epoch 993 Batch  326/718   train_loss = 2.259\n",
      "Epoch 993 Batch  426/718   train_loss = 2.296\n",
      "Epoch 993 Batch  526/718   train_loss = 2.283\n",
      "Epoch 993 Batch  626/718   train_loss = 2.440\n",
      "Epoch 994 Batch    8/718   train_loss = 2.248\n",
      "Epoch 994 Batch  108/718   train_loss = 2.269\n",
      "Epoch 994 Batch  208/718   train_loss = 2.401\n",
      "Epoch 994 Batch  308/718   train_loss = 2.396\n",
      "Epoch 994 Batch  408/718   train_loss = 2.406\n",
      "Epoch 994 Batch  508/718   train_loss = 2.397\n",
      "Epoch 994 Batch  608/718   train_loss = 2.484\n",
      "Epoch 994 Batch  708/718   train_loss = 2.315\n",
      "Epoch 995 Batch   90/718   train_loss = 2.257\n",
      "Epoch 995 Batch  190/718   train_loss = 2.421\n",
      "Epoch 995 Batch  290/718   train_loss = 2.230\n",
      "Epoch 995 Batch  390/718   train_loss = 2.294\n",
      "Epoch 995 Batch  490/718   train_loss = 2.324\n",
      "Epoch 995 Batch  590/718   train_loss = 2.333\n",
      "Epoch 995 Batch  690/718   train_loss = 2.370\n",
      "Epoch 996 Batch   72/718   train_loss = 2.426\n",
      "Epoch 996 Batch  172/718   train_loss = 2.379\n",
      "Epoch 996 Batch  272/718   train_loss = 2.382\n",
      "Epoch 996 Batch  372/718   train_loss = 2.363\n",
      "Epoch 996 Batch  472/718   train_loss = 2.387\n",
      "Epoch 996 Batch  572/718   train_loss = 2.408\n",
      "Epoch 996 Batch  672/718   train_loss = 2.300\n",
      "Epoch 997 Batch   54/718   train_loss = 2.224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 997 Batch  154/718   train_loss = 2.451\n",
      "Epoch 997 Batch  254/718   train_loss = 2.253\n",
      "Epoch 997 Batch  354/718   train_loss = 2.357\n",
      "Epoch 997 Batch  454/718   train_loss = 2.231\n",
      "Epoch 997 Batch  554/718   train_loss = 2.449\n",
      "Epoch 997 Batch  654/718   train_loss = 2.411\n",
      "Epoch 998 Batch   36/718   train_loss = 2.314\n",
      "Epoch 998 Batch  136/718   train_loss = 2.331\n",
      "Epoch 998 Batch  236/718   train_loss = 2.274\n",
      "Epoch 998 Batch  336/718   train_loss = 2.384\n",
      "Epoch 998 Batch  436/718   train_loss = 2.344\n",
      "Epoch 998 Batch  536/718   train_loss = 2.357\n",
      "Epoch 998 Batch  636/718   train_loss = 2.391\n",
      "Epoch 999 Batch   18/718   train_loss = 2.354\n",
      "Epoch 999 Batch  118/718   train_loss = 2.269\n",
      "Epoch 999 Batch  218/718   train_loss = 2.401\n",
      "Epoch 999 Batch  318/718   train_loss = 2.321\n",
      "Epoch 999 Batch  418/718   train_loss = 2.454\n",
      "Epoch 999 Batch  518/718   train_loss = 2.381\n",
      "Epoch 999 Batch  618/718   train_loss = 2.302\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n",
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "seq_length, load_dir = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    inputs = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    return inputs, initial_state, final_state, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return int_to_vocab[np.argmax(probabilities)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "homer, i want you to have a special dinner.\n",
      "\n",
      "i can't believe you were a geezer-pleaser of fifty-two of my...\n",
      "not a chance. but the only thing i can do is monitor the garage and a half horse. i mean, i think i have a fever. i get out.\n",
      "\n",
      "i got a little problem, but i don't know what to do.\n",
      "i don't know...\n",
      "\n",
      "oh, i can't believe i have a problem.\n",
      "i think i\n"
     ]
    }
   ],
   "source": [
    "gen_length = 100\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'homer'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The netwrok generated good scentences (but far from perfect). To improve the results larger batch size must be used, but that needs very powerful gpu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
